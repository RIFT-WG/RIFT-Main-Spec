<?xml version="1.0" encoding="US-ASCII"?>
<!-- This template is for creating an Internet Draft using xml2rfc, which is available here: http://xml.resource.org. -->
<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
<!-- One method to get references from the online citation libraries.
There has to be one entity for each item to be referenced.
An alternate method (rfc include) is described in the references. -->
<!ENTITY RFC1982 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.1982.xml">
<!ENTITY RFC5304 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5304.xml">
<!ENTITY RFC5310 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5310.xml">
<!ENTITY RFC4271 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.4271.xml">
        <!--
<!ENTITY RFC4655 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.4655.xml">
-->
<!ENTITY RFC5301 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5301.xml">
<!ENTITY RFC5306 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5306.xml">
<!ENTITY RFC5308 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5308.xml">
<!ENTITY RFC5309 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5309.xml">
<!ENTITY RFC5120 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5120.xml">
<!ENTITY RFC7602 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7602.xml">
<!ENTITY RFC7938 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7938.xml">
        <!--
<!ENTITY RFC7855 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7855.xml">
        -->
<!ENTITY RFC2328 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2328.xml">
<!ENTITY RFC5303 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5303.xml">

<!ENTITY RFC0826 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.0826.xml">
<!ENTITY RFC2131 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2131.xml">
<!ENTITY RFC8415 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8415.xml">
<!ENTITY RFC3626 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.3626.xml">
<!ENTITY RFC2365 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.2365.xml">
<!ENTITY RFC4291 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.4291.xml">
<!ENTITY RFC4861 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.4861.xml">
<!ENTITY RFC4862 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.4862.xml">
<!ENTITY RFC5082 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5082.xml">
<!ENTITY RFC5549 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5549.xml">
<!ENTITY RFC5881 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5881.xml">
<!ENTITY RFC5709 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5709.xml">
<!ENTITY RFC5905 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.5905.xml">
        <!ENTITY RFC6518 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.6518.xml">

        <!ENTITY RFC7752 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7752.xml">
<!ENTITY RFC7987 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.7987.xml">
<!ENTITY RFC8174 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8174.xml">
<!ENTITY RFC8200 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8200.xml">
        <!ENTITY RFC8202 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8202.xml">

        <!-- SR removed
        <!ENTITY RFC8402 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8402.xml">
        -->

        <!ENTITY RFC8505 SYSTEM "http://xml.resource.org/public/rfc/bibxml/reference.RFC.8505.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
 please see http://xml.resource.org/authoring/README.html. -->
<!-- Below are generally applicable Processing Instructions (PIs) that most I-Ds might want to use.
 (Here they are set differently than their defaults in xml2rfc v1.32) -->
<?rfc strict="yes" ?>
<!-- give errors regarding ID-nits and DTD validation -->
<!-- control the table of contents (ToC) -->
<?rfc toc="yes"?>
<!-- generate a ToC -->
<?rfc tocdepth="4"?>
<!-- the number of levels of subsections in ToC. default: 3 -->
<!-- control references -->
<?rfc symrefs="yes"?>
<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc sortrefs="yes" ?>
<!-- sort the reference entries alphabetically -->
<!-- control vertical white space
 (using these PIs as follows is recommended by the RFC Editor) -->
<?rfc compact="yes" ?>
<!-- do not start each main section on a new page -->
<?rfc subcompact="no" ?>
<!-- keep one blank line between list items -->
<!-- end of list of popular I-D processing instructions -->
<rfc category="std" docName="draft-ietf-rift-rift-10" ipr="trust200902">
    <!-- category values: std, bcp, info, exp, and historic
     ipr values: full3667, noModification3667, noDerivatives3667
     you can add the attributes updates="NNNN" and obsoletes="NNNN"
     they will automatically be output with "(if approved)" -->

    <front>
        <!-- The abbreviated title is used in the page header - it is 
         only necessary if the
         full title is longer than 39 characters -->

        <title abbrev="RIFT">RIFT: Routing in Fat Trees</title>

        <!-- add 'role="editor"' below for the editors if appropriate -->

        <!-- Another author who claims to be an editor -->

         <author fullname="Tony Przygienda" initials="A." surname="Przygienda" role="editor">
			<organization>Juniper</organization>
			<address>
                <postal>
                    <street>1137 Innovation Way
                    </street>
                    <city>Sunnyvale</city>
                    <region>CA
                    </region>
                    <code/>
                    <country>USA
                    </country>
                </postal>
                <phone/>
                <facsimile/>
                <email>prz@juniper.net
                </email>
				<uri/>
			</address>
		</author>


            <author fullname="Alankar Sharma" initials="A"
                surname="Sharma">

                <organization>Comcast</organization>

                <address>
                    <postal>
                        <street>1800 Bishops Gate Blvd</street>

                        <city>Mount Laurel</city>

                        <region>NJ</region>

                        <code>08054</code>

                        <country>US</country>
                    </postal>

                    <email>Alankar_Sharma@comcast.com</email>

                </address>
            </author>

            <author initials="P" surname="Thubert" fullname="Pascal Thubert">
                <organization abbrev="Cisco">Cisco Systems, Inc</organization>
                <address>
                    <postal>
                        <street>Building D</street>
                        <street>45 Allee des Ormes - BP1200 </street>
                        <city>MOUGINS - Sophia Antipolis</city>
                        <code>06254</code>
                        <country>FRANCE</country>
                    </postal>
                    <phone>+33 497 23 26 34</phone>
                    <email>pthubert@cisco.com</email>
                </address>
            </author>

         <author fullname="Bruno Rijsman" initials="Bruno" surname="Rijsman">
            <organization>Individual</organization>
            <address>
                <postal>
                    <street></street>

                    <city></city>

                    <region></region>

                    <code></code>

                    <country></country>
                </postal>

                <email>brunorijsman@gmail.com</email>
            </address>
        </author>

         <author fullname="Dmitry Afanasiev" initials="Dmitry" surname="Afanasiev">
            <organization>Yandex</organization>
            <address>
                <postal>
                    <street></street>

                    <city></city>

                    <region></region>

                    <code></code>

                    <country></country>
                </postal>

                <email>fl0w@yandex-team.ru</email>
            </address>
        </author>

        <date year="2020"/>

        <!-- If the month and year are both specified and are the current ones, xml2rfc will fill
         in the current day for you. If only the current year is specified, xml2rfc will fill
         in the current day and month for you. If the year is not the current one, it is
         necessary to specify at least a month (xml2rfc assumes day="1" if not specified for the
         purpose of calculating the expiry date).  With drafts it is normally sufficient to
         specify just the year. -->

        <!-- Meta-data Declarations -->

        <area>Routing</area>

        <workgroup>RIFT Working Group</workgroup>

        <!-- WG name at the upper left corner of the doc,
         IETF is fine for individual submissions.
         If this element is not present, the default is "Network Working Group",
         which is used by the RFC Editor as a nod to the history of the IETF. -->

        <!-- Keywords will be incorporated into HTML output
         files in a meta tag but they have no effect on text or nroff
         output. If you submit your draft to the RFC Editor, the
         keywords will be used for the search engine. -->

        <abstract>

            <t>This document defines a
                specialized, dynamic routing protocol for
                Clos and fat-tree network topologies optimized towards minimization of
                configuration and operational
                complexity. The protocol
                <list style="symbols">
                    <t>deals with no configuration,
                        fully automated construction of fat-tree topologies based
                        on detection of links,
                    </t>

                    <t>minimizes the amount of routing
                        state held at each level,</t>

                    <t>automatically prunes and load balances
                        topology
                        flooding exchanges over a sufficient subset of links,
                    </t>

                    <t>supports
                        automatic disaggregation of prefixes on link and node failures to
                        prevent black-holing and suboptimal routing,
                    </t>

                    <t>allows traffic steering and
                        re-routing policies,
                    </t>

                    <t>allows loop-free non-ECMP forwarding,
                    </t>

                    <t>automatically re-balances traffic towards the spines based on
                        bandwidth available and finally
                    </t>

                    <t>provides
                        mechanisms to synchronize a limited key-value data-store that
                        can be used after protocol convergence to e.g.
                        bootstrap higher levels of functionality on nodes.
                    </t>
                </list>
            </t>
        </abstract>


    </front>

    <middle>
        <section title="Authors">

            <t>
                This work is a product of a list of individuals which are all to
                be considered major contributors independent of the fact whether
                their name made it to the limited boilerplate author's list or not.
            </t>

            <texttable anchor="authors" style="none" title="RIFT Authors">

    <ttcol></ttcol><ttcol></ttcol><ttcol></ttcol><ttcol></ttcol><ttcol></ttcol>

    <c>Tony Przygienda, Ed.</c><c>|</c><c>Alankar Sharma</c><c>|</c><c>Pascal Thubert</c>
    <c>Juniper Networks</c>    <c>|</c><c>Comcast</c>       <c>|</c><c>Cisco</c>

    <c></c><c></c><c></c><c></c><c></c>
    
    <c>Bruno Rijsman</c>      <c>|</c><c>Ilya Vershkov</c> <c>|</c><c>Dmitry Afanasiev</c>
    <c>Individual</c>         <c>|</c><c>Mellanox</c>      <c>|</c><c>Yandex</c>

    <c></c><c></c><c></c><c></c><c></c>

    <c>Don Fedyk</c>          <c>|</c><c>Alia Atlas</c>    <c>|</c><c>John Drake</c>
    <c>Individual</c>         <c>|</c><c>Individual</c>    <c>|</c><c>Juniper</c>

</texttable>

            </section>

        <section title="Introduction">


<!--<t> ANISOTROPIC protocol could be used to describe RIFT in contrary to 
 uniform information distribution</t>-->

            <t><xref
                    target="CLOS">Clos</xref> and <xref
                    target="FATTREE">Fat-Tree</xref> topologies
                have gained prominence in today's networking, primarily as
                result of
                the paradigm shift towards a centralized data-center based
                architecture that is poised to deliver a majority of
                computation and storage services
                in the  future.
                Today's current routing protocols were geared  towards a
                network with an irregular topology and low degree of connectivity originally
                but given
                they were the only available options, consequently
                several
                attempts to apply those protocols to Clos have been made.
                Most successfully
                <xref
                        target="RFC4271">BGP</xref> <xref
                        target="RFC7938"></xref>
                has been extended to this purpose, not as much due to its inherent
                suitability but rather because the perceived capability to easily
                modify BGP  and the immanent difficulties with
                <xref
                        target="DIJKSTRA">link-state</xref>
                based protocols to optimize topology exchange and converge quickly in
                large scale densely meshed topologies. The incumbent protocols precondition
                normally extensive configuration or provisioning during bring up and
                re-dimensioning.  This tends to be viable only for a set of organizations with
                according networking operation skills and budgets.
                For many IP fabric
                builders a desirable protocol would be one that auto-configures itself
                and deals with failures and mis-configurations with a minimum of human
                intervention only.  Such a solution would allow local IP fabric bandwidth to
                be consumed in a 'standard component' fashion, i.e. provision it much
                faster and operate it at much lower costs than today, much like compute or storage
                is consumed already.
            </t>
            <t>
                In looking at the problem through the lens of data center
                requirements, RIFT addresses challenges in IP fabric routing
                not through an incremental
                modification of either a link-state (distributed computation)
                or distance-vector (diffused computation) but rather a
                mixture of both, colloquially best described as "link-state towards
                the spine" and "distance vector towards the leaves".  In other words, "bottom" levels
                are flooding their link-state information in
                the "northern" direction while each node generates under normal
                conditions a "default route" and floods it in the "southern" direction.
                This type of protocol allows naturally for
                highly desirable aggregation. Alas, such
                aggregation could blackhole
                traffic in cases of misconfiguration or while failures are being
                resolved  or even cause partial network partitioning and this
                has to be addressed by some adequate mechanism.
                The approach RIFT takes
                is described
                in <xref target="disaggregate"/> and is basically
                based on automatic, sufficient disaggregation of prefixes in case
                of link and node failures.</t>

            <t>For the visually oriented reader, <xref target="first-simple"/>
                presents a first level simplified view of the resulting information
                and routes on a RIFT fabric.  The top of the fabric is holding
                in its link-state database the nodes below it and the routes to
                them.  In the second row of the
                database table we indicate that partial information of other nodes in
                the same level is available as well. The details of how this is
                achieved will be postponed for the moment.  When we look at the
                "bottom" of the fabric, the leaves, we see that the topology is
                basically empty and they only hold a load balanced default route
                to the next level under normal conditions.
            </t>

                <t>The balance of this document details a dedicated IP fabric
                    routing protocol, fills in the
                    specification details and ultimately includes resulting
                    security considerations.
            </t>

                <t>
                    <figure align="center" anchor="first-simple"
                        title="RIFT information distribution">
                        <artwork align="center" type="ascii-art"><![CDATA[
.                                  [A,B,C,D]
.                                  [E]
.             +-----+      +-----+
.             |  E  |      |  F  | A/32 @ [C,D]
.             +-+-+-+      +-+-+-+ B/32 @ [C,D]
.               | |          | |   C/32 @ C
.               | |    +-----+ |   D/32 @ D
.               | |    |       |
.               | +------+     |
.               |      | |     |
.       [A,B] +-+---+  | | +---+-+ [A,B]
.       [D]   |  C  +--+ +-+  D  | [C]
.             +-+-+-+      +-+-+-+
.  0/0  @ [E,F] | |          | |   0/0  @ [E,F]
.  A/32 @ A     | |    +-----+ |   A/32 @ A
.  B/32 @ B     | |    |       |   B/32 @ B
.               | +------+     |
.               |      | |     |
.             +-+---+  | | +---+-+
.             |  A  +--+ +-+  B  |
. 0/0 @ [C,D] +-----+      +-----+ 0/0 @ [C,D]
                        ]]>
                        </artwork>
                    </figure>
                    
                </t>


            <section title="Requirements Language">
                <t>The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
                    "SHOULD", "SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in this
                    document are to be interpreted as described in <xref
                        target="RFC8174">RFC 8174</xref>.</t>
            </section>

        </section>

        <section title="Reference Frame">


            <section title="Terminology" toc="default" anchor="glossary">


                <t>
                    This section presents the terminology used in this document.
                    It is assumed that the reader is thoroughly familiar with the
                    terms and concepts used in <xref target="RFC2328">OSPF</xref>
                    and <xref target="ISO10589-Second-Edition">IS-IS</xref>, <xref target="ISO10589"/>
                    as well as the according
                    graph theoretical concepts of shortest path first <xref
                        target="DIJKSTRA">(SPF)</xref> computation and DAGs.
                </t>
                <t>

                    <list style='hanging'>

                        <t hangText="Crossbar:">
                            Physical arrangement of ports in a switching matrix without
                            implying any further scheduling or buffering disciplines.
                        </t>
                        <t hangText="Clos/Fat Tree:">
                            This document uses the terms Clos and Fat Tree interchangeably
                            whereas it always refers to a folded spine-and-leaf topology with possibly multiple
                            Points of Delivery (PoDs) and one or multiple Top of Fabric (ToF) planes. Several modifications such as leaf-2-leaf
                            shortcuts and multiple level shortcuts are possible and described further in
                            the document.
                        </t>

                        <t hangText="Directed Acyclic Graph (DAG):">A finite directed graph with no directed cycles (loops).
                            If links in Clos are considered as either being all directed towards the top or vice versa, each
                            of such two graphs is a DAG.
                        </t>

                        <t hangText="Folded Spine-and-Leaf:">
                            In case Clos fabric input and output stages are analogous, the fabric can be
                            "folded" to build a "superspine" or top which we will call Top of Fabric (ToF)
                            in this document.
                        </t>

                        <t hangText="Level:"> Clos and Fat Tree networks are
                            topologically partially ordered graphs and 'level' denotes the set of nodes at the
                            same height
                            in such a network, where the bottom level (leaf) is the level with lowest
                            value.

                            A node has links to nodes one level down and/or one level up.
                            Under some circumstances, a node may have links to nodes at
                            the same level.

                            As footnote: Clos terminology
                            uses often the concept of "stage" but due to the
                            folded nature of the Fat Tree we do not use it
                            to prevent misunderstandings.</t>

                        <t hangText="Superspine vs. Aggregation and Spine vs. Edge/Leaf:">
                            Traditional level names in  5-stages folded Clos for Level 2, 1 and 0 respectively.  We
                            normalize this language to talk about top-of-fabric (ToF), top-of-pod (ToP)
                            and leaves.
                        </t>

                        <t hangText="Zero Touch Provisioning (ZTP):">
                            Optional RIFT mechanism which allows to derive node levels automatically
                            based on minimum configuration (only ToF property has to be provisioned on according nodes).
                        </t>

                        <t hangText="Point of Delivery (PoD):">A self-contained
                            vertical slice or subset of a Clos or Fat Tree network
                            containing normally only level 0
                            and level 1 nodes.  A node in a PoD communicates with
                            nodes in other PoDs via the Top-of-Fabric. We number PoDs to
                            distinguish them and use PoD #0 to denote "undefined" PoD.
                        </t>

                        <t hangText="Top of PoD (ToP):">
                            The set of nodes that provide intra-PoD communication and have
                            northbound adjacencies outside of the PoD, i.e. are at the
                            "top" of the PoD.
                        </t>

                        <t hangText="Top of Fabric (ToF):">
                            The set of nodes that provide inter-PoD communication and have
                            no northbound adjacencies, i.e. are at the "very top" of the fabric.
                            ToF nodes do not belong
                            to any PoD and are assigned "undefined"
                            PoD value to indicate
                            the equivalent of "any" PoD.
                        </t>

                        <t hangText="Spine:">Any nodes north of leaves and south of top-of-fabric nodes. Multiple
                            layers of spines in a PoD are possible.
                        </t>

                        <t hangText="Leaf:">A node without southbound adjacencies. Its level
                            is 0 (except cases where it is deriving its level via ZTP and
                            is running without
                            LEAF_ONLY which will be explained in <xref target="ZTP"/>).
                        </t>

                        <t hangText="Top-of-fabric Plane or Partition:">In large fabrics top-of-fabric switches may not
                            have enough ports to aggregate all switches south of them and
                            with that, the ToF is 'split' into multiple independent planes.
                            Introduction and <xref target="Planes"/> explains the concept in more detail.
                            A plane is subset of ToF nodes that see each other through
                            south reflection or E-W links.
                        </t>

                        <t hangText="Radix:">A radix of a switch is basically number of
                            switching ports it provides. It's sometimes called
                            fanout as well.</t>

                        <t hangText="North Radix:">Ports cabled northbound to higher level nodes.</t>

                        <t hangText="South Radix:">Ports cabled southbound to lower level nodes.</t>

                        <t hangText="South/Southbound and North/Northbound (Direction):">
                            When describing protocol
                            elements and procedures,
                            we will be
                            using in different situations the directionality
                            of the compass. I.e., 'south' or 'southbound' mean
                            moving
                            towards the bottom of the Clos or Fat Tree network
                            and 'north' and 'northbound' mean moving towards
                            the top of the Clos or Fat Tree network.

                        </t>

                        <t hangText="Northbound Link:">
                            A link to a node one level up or in other words, one
                            level further north.
                        </t>

                        <t hangText="Southbound Link:">
                            A link to a node one level down or in other words, one
                            level further south.
                        </t>

                        <t hangText="East-West Link:">A link between
                            two nodes at the same level. East-West
                            links are normally not part of Clos or
                            "fat-tree" topologies.
                        </t>

                        <t hangText="Leaf shortcuts (L2L):"> East-West links at
                            leaf level
                            will need to be differentiated from East-West links at
                            other levels.
                        </t>

                        <t hangText="Routing on the host (RotH):">Modern data center
                            architecture variant where servers/leaves are multi-homed and
                            consecutively participate in routing.
                        </t>

                        <t hangText="Northbound representation:">Subset of topology
                            information flooded
                            towards higher levels of the fabric.
                        </t>

                        <t hangText="Southbound representation:">Subset of topology
                            information sent
                            towards a lower level.
                        </t>

                        <t hangText="South Reflection:">Often abbreviated just as
                            "reflection" it defines a mechanism where South Node TIEs
                            are "reflected" from the level south back up north to allow
                            nodes in the same level
                            without E-W links to "see" each other's node TIEs.</t>

                        <t hangText="TIE:">This is an acronym for a "Topology
                            Information Element". TIEs are exchanged between RIFT nodes to
                            describe parts of a network such as links and address prefixes,
                            in a fashion similar to ISIS LSPs or OSPF LSAs.
                            A TIE has always a direction and a type. We will talk about
                            North TIEs (sometimes abbreviated as N-TIEs) when talking about
                            TIEs in the
                            northbound representation
                            and South-TIEs (sometimes abbreviated as S-TIEs)
                            for the southbound equivalent. TIEs have different types such
                            as node and prefix TIEs.
                        </t>

                        <t hangText="Node TIE:">This stands as acronym for a
                            "Node Topology Information Element" that contains all
                            adjacencies
                            the node discovered and
                            information about node itself. Node TIE should NOT be confused with
                            a N-TIE since "node" defines the type of TIE rather than
                            its direction.
                        </t>

                        <t hangText="Prefix TIE:">This is an acronym for a "Prefix Topology
                            Information Element" and it contains all prefixes
                            directly attached to
                            this node in case of a North TIE and in case of South TIE the necessary
                            default routes the node advertises
                            southbound.
                        </t>

                        <t hangText="Key Value TIE:">A South TIE that is carrying a set of
                            key value pairs <xref target="DYNAMO"/>.
                            It can be used to distribute information in the southbound
                            direction within
                            the protocol.
                        </t>

                        <t hangText="TIDE:">Topology Information Description Element,
                            equivalent to CSNP in ISIS.</t>

                        <t hangText="TIRE:">Topology
                        Information Request Element, equivalent to PSNP in ISIS. It
                        can both
                        confirm received and request missing TIEs.</t>

                        <t hangText="De-aggregation/Disaggregation:">
                            Process in which a node decides to
                            advertise more specific prefixes Southwards, either positively to
                            attract the corresponding traffic, or negatively to repel it.
                            Disaggregation is performed to prevent black-holing and suboptimal
                            routing to the more specific prefixes.</t>

                        <t hangText="LIE:">This is an acronym for a
                            "Link Information Element",
                            largely equivalent to HELLOs in IGPs and exchanged over
                            all the links between systems running RIFT to form three way adjacencies.
                        </t>

                        <t hangText="Flood Repeater (FR):">A node can designate one or more northbound
                            neighbor nodes to be flood repeaters. The flood repeaters are responsible for
                            flooding northbound TIEs further north. They are
                            similar to MPR in OSLR. The document
                            sometimes calls them flood leaders as well.
                        </t>

                        <t hangText="Bandwidth Adjusted Distance (BAD):">
                            Each RIFT node
                            can calculate the amount of northbound bandwidth
                            available towards a node
                            compared to other nodes at the same level and
                            can modify the
                            route distance accordingly to allow for the lower level to
                            adjust their load balancing towards spines.</t>

                        <t hangText="Overloaded:">Applies to a node advertising
                            `overload` attribute as set. The semantics closely
                            follow the meaning of the same attribute
                            in <xref target="ISO10589-Second-Edition"/>.</t>

                        <t hangText="Interface:">A layer 3 entity over which RIFT
                            control packets are exchanged.
                        </t>
                        <t hangText="Three-Way Adjacency:">RIFT tries to form a unique adjacency
                            over an interface and exchange local configuration and
                            necessary ZTP information. An adjacency is only advertised in node TIEs and
                            used for computations after
                            it achieved three-way state, i.e. both routers reflected each other in
                            LIEs including relevant security information. LIEs before three-way state is
                            reached may carry ZTP related information already.</t>

                        <t hangText="Bi-directional Adjacency:">
                            Bidirectional adjacency is an adjacency where nodes of both sides of the
                            adjacency advertised it in the node TIEs with the correct levels and system
                            IDs. Bi-directionality is used to check in different algorithms whether the link should be
                            included.
                        </t>

                        <t hangText="Neighbor:">Once a three-way adjacency has been
                            formed a neighborship relationship contains the neighbor's
                            properties. Multiple adjacencies can be formed to a remote node
                            via parallel interfaces but such adjacencies are NOT sharing
                            a neighbor structure. Saying "neighbor" is thus equivalent to
                            saying "a three-way adjacency".</t>

                        <t hangText="Cost:">The term signifies the weighted distance between
                            two neighbors.</t>
                        <t hangText="Distance:">Sum of costs (bound by infinite distance)
                            between two nodes.</t>

                        <t hangText="Shortest-Path First (SPF):">A well-known graph algorithm
                            attributed to Dijkstra that establishes a tree of shortest paths
                            from a source to destinations on the graph. We use SPF acronym due to its
                            familiarity as general term for the node reachability calculations
                            RIFT can employ to ultimately calculate routes of which Dijkstra algorithm is one.
                        </t>

                        <t hangText="North SPF (N-SPF):">A reachability calculation that is progressing northbound,
                            as example SPF that is using South Node TIEs only. Normally it progresses a single hop
                            only and installs default routes.
                        </t>

                        <t hangText="South SPF (S-SPF):">A reachability calculation that is progressing southbound,
                            as example SPF that is using North Node TIEs only.
                        </t>

                        <t hangText="Security Envelope">RIFT packets are flooded within an authenticated
                            security envelope that
                            allows to protect the integrity of information a node accepts.
                        </t>

                    </list>
                </t>
            </section>

            <section title="Topology">

                <t>
                    <figure align="center" anchor="pic-topo-three"
                        title="A three level spine-and-leaf topology">
                        <artwork align="center"><![CDATA[
.                +--------+          +--------+          ^ N
.                |ToF   21|          |ToF   22|          |
.Level 2         ++-+--+-++          ++-+--+-++        <-*-> E/W
.                 | |  | |            | |  | |           |
.             P111/2|  |P121          | |  | |         S v
.                 ^ ^  ^ ^            | |  | |
.                 | |  | |            | |  | |
.  +--------------+ |  +-----------+  | |  | +---------------+
.  |                |    |         |  | |  |                 |
. South +-----------------------------+ |  |                 ^
.  |    |           |    |         |    |  |              All TIEs
.  0/0  0/0        0/0   +-----------------------------+     |
.  v    v           v              |    |  |           |     |
.  |    |           +-+    +<-0/0----------+           |     |
.  |    |             |    |       |    |              |     |
.+-+----++ optional +-+----++     ++----+-+           ++-----++
.|       | E/W link |       |     |       |           |       |
.|Spin111+----------+Spin112|     |Spin121|           |Spin122|
.+-+---+-+          ++----+-+     +-+---+-+           ++---+--+
.  |   |             |   South      |   |              |   |
.  |   +---0/0--->-----+ 0/0        |   +----------------+ |
. 0/0                | |  |         |                  | | |
.  |   +---<-0/0-----+ |  v         |   +--------------+ | |
.  v   |               |  |         |   |                | |
.+-+---+-+          +--+--+-+     +-+---+-+          +---+-+-+
.|       |  (L2L)   |       |     |       |  Level 0 |       |
.|Leaf111~~~~~~~~~~~~Leaf112|     |Leaf121|          |Leaf122|
.+-+-----+          +-+---+-+     +--+--+-+          +-+-----+
.  +                  +    \        /   +              +
.  Prefix111   Prefix112    \      /   Prefix121    Prefix122
.                          multi-homed
.                            Prefix
.+---------- Pod 1 ---------+     +---------- Pod 2 ---------+
                        ]]>
                        </artwork>
                    </figure>

                </t>

<t>
    <figure align="center" anchor="partitioned-spine"
        title="Topology with multiple planes">
        <artwork align="center"><![CDATA[
            .+--------+  +--------+  +--------+  +--------+
            .|ToF   A1|  |ToF   B1|  |ToF   B2|  |ToF   A2|
            .++-+-----+  ++-+-----+  ++-+-----+  ++-+-----+
            . | |         | |         | |         | |
            . | |         | |         | +---------------+
            . | |         | |         |           | |   |
            . | |         | +-------------------------+ |
            . | |         |           |           | | | |
            . | +-----------------------+         | | | |
            . |           |           | |         | | | |
            . |           | +---------+ | +---------+ | |
            . |           | |           | |       |   | |
            . | +---------------------------------+   | |
            . | |         | |           | |           | |
            .++-+-----+  ++-+-----+  +--+-+---+  +----+-+-+
            .|Spine111|  |Spine112|  |Spine121|  |Spine122|
            .+-+---+--+  ++----+--+  +-+---+--+  ++---+---+
            .  |   |      |    |       |   |      |   |
            .  |   +--------+  |       |   +--------+ |
            .  |          | |  |       |          | | |
            .  |   -------+ |  |       |   +------+ | |
            .  |   |        |  |       |   |        | |
            .+-+---+-+   +--+--+-+   +-+---+-+  +---+-+-+
            .|Leaf111|   |Leaf112|   |Leaf121|  |Leaf122|
            .+-------+   +-------+   +-------+  +-------+
        ]]>
        </artwork>
    </figure>

</t>

                <t>
                    We will use topology in <xref target="pic-topo-three"/> (called commonly a fat
                    tree/network in modern IP fabric considerations
                    <xref target="VAHDAT08"/>
                    as homonym to the
                    <xref target="FATTREE">original definition of the term</xref>)
                        in all further considerations.
                    This figure depicts
                    a generic "single plane fat-tree" and the concepts explained using
                    three levels
                    apply by induction to further levels and higher degrees
                    of connectivity. Further, this document will deal also
                    with designs that
                    provide only sparser connectivity and  "partitioned spines"
                    as shown in <xref target="partitioned-spine"/>
                    and explained further in <xref target="Planes"/>.
                </t>

            </section>

        </section>

        <!-- based on Hardwick IAB review

        <section anchor="reqs" title="Requirement Considerations">
            <t>
                <xref
                    target="RFC7938"></xref> gives the original set of requirements
                augmented here based upon recent experience in the
                operation of fat-tree
                networks.
            </t>
            <t>
                <list style='format REQ%d: ' >
                    <t>The control protocol should discover the physical
                        links automatically
                        and be able to detect cabling that
                        violates fat-tree topology constraints.
                        It must react accordingly to such mis-cabling attempts,
                        at a minimum
                        preventing adjacencies between nodes from being
                        formed and traffic
                        from being forwarded on those mis-cabled links. E.g.
                        connecting a leaf to a spine at level 2 should be
                        detected and ideally prevented.

                    </t>

                    <t>A node without any configuration beside default values
                        should come up at the correct level
                        in any PoD it is introduced into. Optionally,
                        it must be possible to
                        configure nodes to restrict their participation to
                        the PoD(s) targeted at any level.
                    </t>

                    <t>Optionally, the protocol should allow to provision IP
                        fabrics where the
                        individual
                        switches carry no configuration information and are
                        all deriving their
                        level from a "seed". Observe that this requirement
                        may collide with the desire
                        to detect cabling misconfiguration and with that
                        only one of the requirements
                        can be fully met in a chosen configuration mode.
                    </t>

                    <t>
                        The solution should allow for minimum size routing
                        information base and forwarding
                        tables at leaf level for speed, cost and simplicity
                        reasons. Holding excessive amount of information away
                        from leaf nodes simplifies operation and lowers cost of
                        the underlay and allows to scale and introduce proper multi-homing
                        down to the server level. The routing solution should allow for
                        easy instantiation of multiple routing planes. Coupled
                        with mobility defined in <xref target="mobreq"/>
                        this should allow for "light-weight" overlays
                        on an IP fabric with e.g. native IPv6 mobility support.
                    </t>

                    <t>Very high degree of ECMP must be
                        supported. Maximum ECMP is currently understood as the most
                        efficient
                        routing approach to maximize the throughput of switching
                        fabrics <xref target="MAKSIC2013"/>.
                    </t>

                    <t>Non equal cost anycast must be supported to allow for
                        easy and robust multi-homing of services without regressing to
                        careful balancing of link costs.
                        </t>
                    <t>Traffic engineering should be allowed by modification of
                        prefixes and/or their next-hops.
                    </t>

                    <t>The solution should allow for access to link states of
                        the whole topology
                        to enable efficient support for modern control
                        architectures like <xref
                            target="RFC7855">SPRING</xref> or
                        <xref target="RFC4655">PCE</xref>.
                    </t>

                    <t>The solution should easily accommodate opaque data to
                        be carried throughout the topology to subsets of nodes.
                        This can be used
                        for many purposes, one of them being a key-value
                        store that allows
                        bootstrapping of nodes based right at the time of
                        topology discovery. Another use is distributing
                        MAC to L3 address binding from the leaves up north in
                        case of e.g. DHCP.
                    </t>

                    <t>Nodes should be taken out and introduced into production
                        with minimum
                        wait-times and minimum of "shaking" of the network, i.e.
                        radius of propagation (often called "blast radius")
                        of changed information should be as small as feasible.
                    </t>

                    <t>The protocol should allow for maximum aggregation of carried
                        routing information while at the same time automatically
                        de-aggregating
                        the prefixes to prevent black-holing in case of failures.
                        The de-aggregation
                        should support maximum possible ECMP/N-ECMP remaining
                        after failure.
                    </t>

                    <t>Reducing the scope of communication needed throughout
                        the network on link and state
                        failure, as well as reducing advertisements of
                        repeating or idiomatic information in
                        stable state is highly desirable since it leads to
                        better stability and faster convergence behavior.
                    </t>

                    <t>Under normal, fully converged condition,
                        once a packet is forwarded along a link in a "southbound" direction,
                        it must not take any further "northbound"
                        links (Valley Free Routing).
                        Taking a path
                        through the spine in cases where a shorter
                        path is available is highly undesirable
                        (Bow Tying).
                    </t>

                    <t>Parallel links between same set of
                    nodes must be distinguishable for SPF, failure and
                        traffic engineering
                    purposes. </t>

    <t> The protocol must support interfaces sharing the same address.
        Specifically, it must operate in presence of
    unnumbered links (even parallel ones) and/or links of a single node
    being configured with same addresses.</t>

    <t>It would be desirable to achieve fast re-balancing of flows when links,
    especially towards the spines are lost or provisioned without regressing to
    per flow traffic engineering which introduces significant amount of complexity
    while possibly not being reactive enough to account for short-lived flows.
    </t>

    <t anchor="mobreq">The control plane should be able to unambiguously determine the current
    point of attachment (which port on which leaf node) of a prefix, even in a
    context of fast mobility, e.g., when the prefix is a host address on a
    wireless node that 1) may associate to any of multiple access points (APs)
    that are attached to different ports on a same leaf node or to different
    leaf nodes, and 2) may move and reassociate several times to a different access point
    within a sub-second period.
    </t>

    <t>The protocol must provide
        security mechanisms that allow the operator to restrict nodes,
        especially leaf nodes without proper credentials, from forming a three-way adjacency and
        participating in routing.
    </t>

                </list>
</t>


<t>
            Following list represents non-requirements:

            </t>

<t>
    <list style='format PEND%d: ' >
        <t>Supporting anything but point-to-point links is not necessary.
        </t>


</list>
    </t>

<t>
    Finally, following are the non-requirements:
</t>
<t>
    <list style='format NONREQ%d: ' >
        <t>Broadcast media support is unnecessary. However, miscabling leading
            to multiple nodes on a broadcast segment must be operationally easily
            recognizable and detectable while not taxing
            the protocol excessively.
            </t>

        <t>Purging link state elements is unnecessary given its fragility and complexity and
            today's large memory size on
            even modest switches and routers.
            </t>

        <t>Special support for layer 3 multi-hop adjacencies is not part of
            the protocol specification. Such support
            can be easily provided by using tunneling technologies the same
            way IGPs today are solving the problem.
            </t>

    </list>
</t>

        </section>

        -->

        <section title="RIFT: Routing in Fat Trees">

            <t>
                We present here a
                detailed outline of a protocol optimized for Routing
                in Fat Trees (RIFT) that in most abstract terms has
                many properties of a modified link-state protocol
                <xref target="RFC2328"></xref><xref
                target="ISO10589-Second-Edition"></xref> when
                distributing information northbound and
                distance vector <xref target="RFC4271"></xref> protocol
                when distributing information southbound.
                While this is  an unusual combination,
                it does quite naturally exhibit the desirable properties
                we seek.
            </t>


            <section title="Overview">

                <section title="Properties">
<t>
    The most singular property of RIFT is that it floods flat 
    link-state information northbound only so that each level obtains 
    the full topology of levels south of it. Link-State information is,
    with some exceptions, never flooded East-West or back South again. 
    Exceptions like south reflection is explained in detail in 
    <xref target="posdisaggreg"/> and east-west flooding at ToF level 
    in multi-plane fabrics is outlined in <xref target="Planes"/>. 
    In southbound direction, the protocol operates like a "fully 
    summarizing, unidirectional" path vector protocol or rather a 
    distance vector with implicit split horizon. Routing
    information, normally
    just the default route, propagates one hop south and is 
    're-advertised' by nodes at next lower level.
    However, RIFT uses flooding in the southern direction as well to 
    avoid the overhead of building an update per adjacency. We omit
    describing the East-West direction for the moment.
</t>
<t>

    Those information flow constraints create not only an anisotropic 
    protocol (i.e. the information is not distributed "evenly" or 
    "clumped" but summarized along the N-S gradient) but also a 
    "smooth" information propagation where nodes do not receive the 
    same information from multiple directions at the same time.
    Normally, accepting the same reachability on any link, without 
    understanding its topological significance, forces tie-breaking on 
    some kind of distance metric.
    And such tie-breaking leads ultimately in hop-by-hop forwarding to
    shortest paths only.
    In constrast to that, RIFT, under normal conditions,
    does not need to tie-break same
    reachability information from multiple directions. Its computation
    principles (south forwarding direction is always preferred) leads 
    to valley-free forwarding behavior. And since valley free routing 
    is loop-free, it can use all feasible paths which is another 
    highly desirable property if available bandwidth should be 
    utilized to the maximum extent possible.
</t>
<t>
    To account for the "northern" and the "southern" information split 
    the link state database is partitioned accordingly into "north 
    representation" and "south representation" TIEs.
    In simplest terms the North TIEs contain a link state topology 
    description of lower levels and and South TIEs carry simply 
    default routes towards the level above. This oversimplified view 
    will be refined gradually in following sections while introducing
    protocol procedures and state machines at the same time.
</t>
                </section>
                <section title="Generalized Topology View" anchor="Planes">

<t>
    This section will shed some light on the topologies RIFT addresses, including
    multi plane fabrics and their implications.
    Readers that are only interested in single plane designs, i.e. all 
    top-of-fabric nodes being topologically equal and initially 
    connected to all the switches at the level below them, can skip 
    the rest of <xref target="Planes"/> and resulting 
    <xref target="negdisaggreg"/> as well.
</t>
<t>
    It is quite difficult to visualize multi plane design, which are
    effectively multi-dimensional switching matrices. To cope with that,
    we will introduce a methodology allowing us to depict the 
    connectivity in two-dimensional pictures. Further, we will leverage
    the fact that we are dealing basically with stacked crossbar 
    fabrics where ports align "on top of each other" in a regular 
    fashion.
</t>
<t>
    A word of caution to the reader; at this point it should be 
    observed that the language used to describe Clos variations, 
    especially in multi-plane designs, varies widely between sources. 
    This description follows the terminology introduced in 
    <xref target="glossary"/>. It is unavoidable to have it present
    to be able to follow the rest of this section correctly.
</t>

                <section title="Terminology">
<t>
    This section describes the terminology and acronyms used in the 
    rest of the text.

    <list style='hanging'>
        <t hangText="P:">Denotes the number of PoDs in a topology.</t>
        <t hangText="S:">Denotes the number of ToF nodes in a topology.
                         </t>
        <t hangText="K:">Denotes the number of ports in radix of a 
                         switch pointing north or south. Further,
                         K_LEAF denotes number of ports pointing
                         south, i.e. towards leaves, and K_TOP for
                         number of ports pointing north towards a 
                         higher spine level.
                         To simplify the visual aids, notations and
                         further considerations, K will be mostly
                         set to
                         Radix/2.</t>
        <t hangText="ToF Plane:">Set of ToFs that are aware of each 
                                 other by means of south reflection.
                                 We number planes by capital letters, e.g.
                                 plane A.
                                 </t>
        <t hangText="N:">Denote the number of independent ToF planes 
                         in a topology.</t>
        <t hangText="R:">Denotes a redundancy factor, i.e. number of 
                         connections a spine has towards a ToF plane. 
                         In single plane design K_TOP is equal to R.
                         </t>
        <t hangText="Fallen Leaf:">A fallen leaf in a plane Z is a 
                                   switch that lost all connectivity
                                   northbound to Z.</t>
    </list>
</t>
                </section>
    
                <section title="Clos as Crossed Crossbars">
<t>
    The typical topology for which RIFT is defined is built of P 
    number of PoDs and connected together by S number of ToF nodes. 
    A PoD node has K number of ports (also called Radix). We consider half of
    them (K=Radix/2) as connecting host devices from the south, and
    the other half connecting to interleaved PoD Top-Level switches to the
    north. Ratio K can be chosen differently without loss of generality
    when port speeds differ or the fabric is oversubscribed but K=R/2
    allows for more readable representation whereby there are as many 
    ports facing north as south on any intermediate node. We represent 
    a node hence in a schematic fashion with ports "sticking out" to 
    its north and south rather than by the usual real-world front 
    faceplate designs of the day.
</t>

<t>
    <xref target="leaftopview"/> provides a view of a leaf node as 
    seen from the north, i.e. showing ports that connect northbound. 
    For lack of a better symbol, we have chosen to use the "o" as 
    ASCII visualisation of a single port. In this example, K_LEAF has 
    6 ports. Observe that the number of PoDs is not related to Radix 
    unless the ToF Nodes are constrained to be the same as the PoD 
    nodes in a particular deployment.
</t>  
                    <figure align="center" anchor="leaftopview"
                        title="A Leaf Node, K_LEAF=6">
                        <artwork align="center"><![CDATA[
                        
    Top view                      
     +---+
     |   |
     | o |     e.g., Radix = 12, K_LEAF = 6
     |   |
     | o |   
     |   |      -------------------------
     | o ------- Physical Port (Ethernet) ----+
     |   |      -------------------------     |
     | o |                                    |
     |   |                                    |
     | o |                                    |
     |   |                                    |
     | o |                                    |
     |   |                                    |
     +---+                                    |
     
       ||             ||      ||      ||      ||      ||      ||  
     +----+       +------------------------------------------------+   
     |    |       |                                                |
     +----+       +------------------------------------------------+  
       ||             ||      ||      ||      ||      ||      ||   
           Side views 
     
                        ]]></artwork>
                    </figure>
  
  
          
<t>
    The Radix of a PoD's topnode may be different than that of the
    leaf node. Though, more often than not, a same type of node is 
    used for both, effectively forming a square (K*K).
    In general case, we could have switches with K_TOP southern 
    ports on nodes at the top of the PoD which are not necessarily the 
    same as K_LEAF. For instance, in the representations below, we 
    pick a 6 port K_LEAF and a 8 port K_TOP. In order to form a 
    crossbar, we need K_TOP Leaf Nodes as illustrated in 
    <xref target="PODSview"/>.
</t>  
                        <figure align="center" anchor="PODSview"
                        title="Southern View of a PoD, K_TOP=8">
                        <artwork align="center"><![CDATA[
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o |
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | 
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | 
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | 
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o |  
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o |
     |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+ 
                                        ]]></artwork>
                    </figure>
  
    
<t>
   As further visualized in <xref target="PODNview"/> the K_TOP Leaf Nodes
   are fully interconnected with the K_LEAF PoD-top nodes, providing 
   connectivity that can be represented as a crossbar when "looked at"
    from the
   north. The result is that, in the absence of a failure, a packet
   entering the PoD from the north on any port can be routed to any port
   in the south of the PoD and vice versa. And that is precisely why it makes
    sense to talk about a
    "switching
    matrix".
</t>  
                
           <figure align="center" anchor="PODNview"
                        title="Northern View of a PoD's Spines, K_TOP=8">
                        <artwork align="center"><![CDATA[

                                E<-*->W

    +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+
    |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |
  +--------------------------------------------------------+
  |   o      o      o      o      o      o      o      o   |
  +--------------------------------------------------------+
  +--------------------------------------------------------+
  |   o      o      o      o      o      o      o      o   |
  +--------------------------------------------------------+
  +--------------------------------------------------------+
  |   o      o      o      o      o      o      o      o   |
  +--------------------------------------------------------+
  +--------------------------------------------------------+
  |   o      o      o      o      o      o      o      o   |
  +--------------------------------------------------------+
  +--------------------------------------------------------+
  |   o      o      o      o      o      o      o      o   |<-+
  +--------------------------------------------------------+  |
  +--------------------------------------------------------+  |
  |   o      o      o      o      o      o      o      o   |  |
  +--------------------------------------------------------+  |
    |   |  |   |  |   |  |   |  |   |  |   |  |   |  |   |    |
    +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+    |
               ^                                              |
               |                                              |
               |     ----------        ---------------------  |
               +----- Leaf Node        PoD top Node (Spine) --+
                     ----------        ---------------------
                        ]]></artwork>
                    </figure>
            
<t>Side views of this PoD is illustrated in <xref target="PODSideview"/> and
<xref target="PODSideview2"/>.        
</t>    
                        <figure align="center" anchor="PODSideview"
                        title="Side View of a PoD, K_TOP=8, K_LEAF=6">
                        <artwork align="center"><![CDATA[
                        
                     Connecting to Spine  
                      
     ||      ||      ||      ||      ||      ||      ||      ||
 +----------------------------------------------------------------+   N
 |                    PoD top Node seen sideways                  |   ^
 +----------------------------------------------------------------+   |
     ||      ||      ||      ||      ||      ||      ||      ||       *
   +----+  +----+  +----+  +----+  +----+  +----+  +----+  +----+     |
   |    |  |    |  |    |  |    |  |    |  |    |  |    |  |    |     v
   +----+  +----+  +----+  +----+  +----+  +----+  +----+  +----+     S
     ||      ||      ||      ||      ||      ||      ||      ||
   
                          Connecting to Client nodes  
                       
                                        ]]></artwork>
                    </figure>          
                    
                        <figure align="center" anchor="PODSideview2"
                        title="Other side View of a PoD, K_TOP=8, 
                        K_LEAF=6, 90&deg; turn in E-W Plane">
                        <artwork align="center"><![CDATA[

             Connecting to Spine

    ||      ||      ||      ||      ||      ||
  +----+  +----+  +----+  +----+  +----+  +----+              N
  |    |  |    |  |    |  |    |  |    |  |   PoD top Nodes   ^
  +----+  +----+  +----+  +----+  +----+  +----+              |
    ||      ||      ||      ||      ||      ||                *
+------------------------------------------------+            |
|              Leaf seen sideways                |            v
+------------------------------------------------+            S
    ||      ||      ||      ||      ||      ||

             Connecting to Client nodes
                    
                                        ]]></artwork>
                    </figure>
<t> 
    As next step, let us observe
    that a resulting PoD can be abstracted as a bigger node with
    a number K of K_POD= K_TOP * K_LEAF, and the design can recurse.
</t>
<t>
    It will be critical at this point that, before progressing further,
    the concept and the picture of "crossed crossbars" is clear. 
    Else, the following considerations might be difficult to 
    comprehend.
</t>
<t>
    To continue, the PoDs are interconnected with each other through a
    Top-of-Fabric (ToF) node at the very top or the north edge of the 
    fabric. The resulting ToF is NOT partitioned if, and only if (IIF),
     every PoD top level node (spine) is connected to every ToF Node.
    This topology is also referred to as a single plane configuration
    and is quite popular due to its simplicity.
    In order to reach a 1:1 connectivity ratio between the ToF and the 
    leaves, it results that there are K_TOP ToF nodes, because each
    port of a ToP node connects to a different ToF node, and K_LEAF 
    ToP nodes for the same reason. Consequently, it will take (P * K_LEAF)
    ports on a ToF node to connect to each of the K_LEAF ToP nodes of 
    the P PoDs, as shown in <xref target="nonpart"/>.
</t>  
                
           <figure align="center" anchor="nonpart"
            title="Fabric Spines and TOFs in Single Plane Design, 3 
                   PoDs">
                        <artwork align="center"><![CDATA[
                            
     [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ] <-----+
      |   |   |   |   |   |   |   |        |
   [=================================]     |     -----------
      |   |   |   |   |   |   |   |        +----- Top-of-Fabric
     [ ] [ ] [ ] [ ] [ ] [ ] [ ] [ ]       +----- Node      -------+
                                           |     -----------       |
                                           |                       v
     +-+ +-+ +-+ +-+ +-+ +-+ +-+ +-+ <-----+                      +-+
     | | | | | | | | | | | | | | | |                              | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]                            | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] -------------------------  | |
   [ |o| |o| |o| |o| |o| |o| |o| |o<--- Physical Port (Ethernet)  | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] -------------------------  | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]                            | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]                            | | 
     | | | | | | | | | | | | | | | |                              | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]                            | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]      --------------        | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] <---  PoD top level        | | 
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]       node (Spine)  ---+   | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]      --------------    |   | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]                        |   | |
     | | | | | | | | | | | | | | | |  -+           +-   +-+   v   | |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] |           |  --| |--[ ]--| |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] |   -----   |  --| |--[ ]--| |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] +--- PoD ---+  --| |--[ ]--| |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] |   -----   |  --| |--[ ]--| |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] |           |  --| |--[ ]--| |
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] |           |  --| |--[ ]--| |
     | | | | | | | | | | | | | | | |  -+           +-   +-+       | | 
     +-+ +-+ +-+ +-+ +-+ +-+ +-+ +-+                              +-+
      
                        ]]></artwork>
                    </figure>
  
  
<t>
    The top view can be collapsed into a third dimension where the 
    hidden depth index is representing the PoD number. We can then 
    show one PoD as a class of PoDs and hence save one dimension in 
    our representation. 
    The Spine Node expands in the depth and the vertical dimensions, 
    whereas the PoD top level Nodes are constrained, in horizontal 
    dimension.
    A port in the 2-D representation represents effectively the class 
    of all the ports at the same position in all the PoDs that are 
    projected in its position along the depth axis. This is shown in 
    <xref target="colnonpart"/>.
</t> 
          
           <figure align="center" anchor="colnonpart"
            title="Collapsed Northern View of a Fabric for Any Number 
                   of PoDs">
                        <artwork align="center"><![CDATA[
         / / / / / / / / / / / / / / / /
        / / / / / / / / / / / / / / / / 
       / / / / / / / / / / / / / / / /   
      / / / / / / / / / / / / / / / /   ]              
     +-+ +-+ +-+ +-+ +-+ +-+ +-+ +-+   ]]
     | | | | | | | | | | | | | | | |  ]   ---------------------------
   [ |o| |o| |o| |o| |o| |o| |o| |o| ] <-- PoD top level node (Spine)
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]    ---------------------------
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]]]]
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]]]     ^^
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]]     //  PoDs
   [ |o| |o| |o| |o| |o| |o| |o| |o| ]     // (in depth)
     | |/| |/| |/| |/| |/| |/| |/| |/     //
     +-+ +-+ +-+/+-+/+-+ +-+ +-+ +-+     //
              ^
              |     ----------------
              +----- Top-of-Fabric Node
                    ----------------
                        ]]></artwork>
                    </figure>
<t>
    As simple as single plane deployment is it introduces a limit due to
    the bound on the available radix of the ToF nodes that has to be
    at least P * K_LEAF. Nevertheless, we will see that a distinct 
    advantage of a connected or non-partitioned Top-of-Fabric is that 
    all failures can be resolved by simple, non-transitive, positive 
    disaggregation (i.e. nodes advertising more specific prefixes with 
    the default to the level below them that is however not propagated
    further down the fabric) as described in <xref target="posdisaggreg"/>
    . In other words; non-partitioned ToF nodes can always reach nodes
    below or withdraw the routes from PoDs they cannot reach 
    unambiguously. And with this, positive disaggregation can heal all 
    failures and still allow all the ToF nodes to see each other via
    south reflection. Disaggregation will be explained in further detail in
    <xref target="disaggregate"/>.
</t>

<t>
    In order to scale beyond the "single plane limit", the 
    Top-of-Fabric can be partitioned by a N number of identically 
    wired planes where N is an integer divider of K_LEAF.
    The 1:1 ratio and the desired symmetry are still served, this time 
    with (K_TOP * N) ToF nodes, each of (P * K_LEAF / N) ports.
    N=1 represents a non-partitioned Spine and N=K_LEAF is a maximally 
    partitioned Spine.
    Further, if R is any integer divisor of K_LEAF, then N=K_LEAF/R is a
    feasible number of planes and R a redundancy factor.
    If proves convenient for deployments to use a radix for the leaf 
    nodes that is a power of 2 so they can pick a number of planes 
    that is a lower power of 2.
    The example in <xref target="partitionedin2"/> splits the Spine in 
    2 planes with a redundancy factor R=3, meaning that there are 3 
    non-intersecting paths between any leaf node and any ToF node. 
    A ToF node must have, in this case, at least 3*P ports, and be 
    directly connected to 3 of the 6 PoD-ToP nodes (spines) in each PoD.
</t>      

  <figure align="center" anchor="partitionedin2"
            title="Northern View of a Multi-Plane ToF Level, K_LEAF=6, 
                   N=2">
                        <artwork align="center"><![CDATA[
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+
     
   Plane 1  
  ----------- . ------------ . ------------ . ------------ . -------- 
   Plane 2
   
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | 
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+  
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+      
              ^
              |
              |      ----------------
              +----- Top-of-Fabric node
                     "across" depth
                     ----------------
                        ]]></artwork>
                    </figure>
     
<t>
    At the extreme end of the spectrum it is even possible to fully 
    partition the spine with N = K_LEAF and R=1, while maintaining 
    connectivity between each leaf node and each Top-of-Fabric node.
    In that case the ToF node connects to a single Port per PoD, so it
    appears as a single port in the projected view represented in
    <xref target="maxpartitioned"/>.
    The number of ports required on the Spine Node is more or equal
    to P, the number of PoDs.
</t>

  <figure align="center" anchor="maxpartitioned"
            title="Northern View of a Maximally Partitioned ToF Level, 
                   R=1">
                        <artwork align="center"><![CDATA[
   Plane 1  
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+  -+
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |
  ----------- . ------------------- . ------------ . -------- |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |
  ----------- . ------------ . ---- . ------------ . -------- |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |
  ----------- . ------------ . ------------------- . -------- +<-+
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |  |
  ----------- . ------------ . ------------ . ---- . -------- |  |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |  |
  ----------- . ------------ . ------------ . --------------- |  |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+   |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o |  | o | | |  |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |--|   |-+ |  |
     +---+  +---+  +---+  +---+  +---+  +---+  +---+  +---+  -+  |
   Plane 6    ^                                                  |
              |                                                  |
              |     ----------------           -------------     |
              +-----  ToF       Node           Class of PoDs  ---+
                    ----------------           -------------
                        ]]></artwork>
                    </figure>

</section> <!-- "Generalized Topology View"  -->

                </section>
<section title="Fallen Leaf Problem" anchor="Fallen">

<t>
    As mentioned earlier, RIFT exhibits an anisotropic behaviour 
    tailored for fabrics with a North / South orientation and a high 
    level of interleaving paths. A non-partitioned fabric makes a 
    total loss of connectivity between a Top-of-Fabric node at the 
    north and a leaf node at the south a very rare but yet possible 
    occasion that is fully healed by positive disaggregation as 
    described in <xref target="posdisaggreg"/>.
    In large fabrics or fabrics built from switches with low radix, 
    the ToF ends often being partitioned in planes which makes the
    occurrence of having a given leaf being only reachable from a 
    subset of the ToF nodes more likely to happen. This makes some 
    further considerations necessary.
</t>

<t>  
    We define a "Fallen Leaf" as a leaf that can be reached by only a 
    subset, but not all, of Top-of-Fabric nodes due to missing 
    connectivity. If R is the redundancy factor, then it takes at 
    least R breakages to reach a "Fallen Leaf" situation.
</t>

<t>  
    In a maximally partitioned fabric, the redundancy factor is R= 1, 
    so any breakage in the fabric may cause one or more fallen leaves.
    However, not all cases require disaggregation. The following cases 
    do not require particular action in such scenario:
        <list>
            <t>If a southern link on a leaf node goes down, then 
               connectivity to any node attached to the leaf is lost. 
               There is no need to disaggregate since the connectivity 
               is lost from all spine nodes to the leaf nodes in the 
               same fashion.
            </t>
         
            <t>If a southern link on a leaf node goes down, then 
               connectivity through that leaf is lost for all nodes. 
               There is no need to disaggregate since the connectivity 
               to this leaf is lost for all spine nodes in a same 
               fashion.
            </t>

            <t>If a ToF Node goes down, then northern traffic towards 
               it is routed via alternate ToF nodes in the same plane 
               and there is no need to disaggregate routes.
            </t>
        </list>
</t>

<t>
    In a general manner, the mechanism of non-transitive positive 
    disaggregation is sufficient when the disaggregating ToF nodes 
    collectively connect to all the ToP nodes in the broken plane. 
    This happens in the following case:
  
        <list>
            <t>If the breakage is the last northern link from a ToP 
               node to a ToF node going down, then the fallen leaf 
               problem affects only The ToF node, and the connectivity 
               to all the nodes in the PoD is lost from that ToF node. 
               This can be observed by other ToF nodes within the 
               plane where the ToP node is located and positively 
               disaggregated within that plane.
            </t>
        </list>
  
    On the other hand, there is a need to disaggregate the routes to 
    Fallen Leaves in a transitive fashion, all the way to the other 
    leaves in the following cases:
         
        <list style="symbols">
            <t>If the breakage is the last northern link from a leaf
               node within a plane (there is only one such link in a 
               maximally partitioned fabric) that goes down, then 
               connectivity to all unicast prefixes attached to the 
               leaf node is lost within the plane where the link is
               located. 
         
               Southern Reflection by a leaf node, e.g., between ToP
               nodes, if the PoD has only 2 levels, happens in between 
               planes, allowing the ToP nodes to detect the problem 
               within the PoD where it occurs and positively 
               disaggregate.

               The breakage can be observed by the ToF nodes in the 
               same plane through the North flooding of TIEs from the 
               ToP nodes. The ToF nodes however need to be aware of 
               all the affected prefixes for the negative, possibly 
               transitive disaggregation to be fully effective (i.e. 
               a node advertising in control plane that it cannot
               reach a certain more specific prefix than default 
               whereas such disaggregation must in extreme condition 
               propagate further down southbound).
         
               The problem can also be observed by the ToF nodes in 
               the other planes through the flooding of North TIEs 
               from the affected leaf nodes, together with non-node
               North TIEs which indicate the affected prefixes.
               To be effective in that case, the positive 
               disaggregation must reach down to the nodes that make 
               the plane selection, which are typically the ingress 
               leaf nodes. The information is not useful for
               routing in the intermediate levels.
            </t>

            <t>If the breakage is a ToP node in a maximally 
               partitioned fabric - in which case it is the only ToP 
               node serving the plane in that PoD - goes down, 
               then the connectivity to all the nodes in the PoD is
               lost within the plane where the ToP node is located. 
               Consequently, all leaves of the PoD fall in this plane.
  
               Since the Southern Reflection between the ToF nodes 
               happens only within a plane, ToF nodes in other planes 
               cannot discover fallen leaves in a different plane.
               They also cannot determine beyond their local plane 
               whether a leaf node that was initially reachable has
               become unreachable. 
         
               As the breakage can be observed by the ToF 
               nodes in the plane where the breakage happened, the 
               ToF nodes in the plane need to be aware of all the 
               affected prefixes for the negative disaggregation to be 
               fully effective.

               The problem can also be observed by the ToF nodes in 
               the other planes through the flooding of North TIEs 
               from the affected leaf nodes, if there
               are only 3 levels and the ToP nodes are directly 
               connected to the leaf nodes, and then again it can
               only be effective it is propagated transitively to the 
               leaf, and useless above that level.
            </t>
        </list>   
 </t>

<t>
    For the sake of easy comprehension let us roll the abstractions 
    back into a simple example and observe that in
    <xref target="partitioned-spine"/> the loss of link Spine 122 to 
    Leaf 122 will make Leaf 122 a fallen leaf for Top-of-Fabric plane 
    B. 
    Worse, if the cabling was never present in first place, plane B 
    will not even be able to know that such a fallen leaf exists. 
    Hence partitioning without further treatment results in two grave 
    problems:
        <list style="symbols">
            <t>Leaf 111 trying to route to Leaf 122 MUST choose Spine 
               111 in plane A as its next hop since plane B will 
               inevitably blackhole the packet when forwarding using 
               default routes or do excessive bow tying. This 
               information must be in its routing table.
            </t>
            <t>Any kind of "flooding" or distance vector trying to 
               deal with the problem by distributing host routes will 
               be able to converge only using paths through leaves.
                The flooding of information on Leaf 122 would have to go
                up to Top-of-Fabric A and then "loopback" over other 
                leaves to ToF B leading in extreme cases to traffic for
                Leaf 122 when presented to plane B taking an "inverted
                fabric" path where leaves start to serve as TOFs, at least
                for the duration of a protocol's convergence.
            </t>
        </list>
</t>

                </section>
                
                <section title="Discovering Fallen Leaves" 
                                anchor="discFallen">
<t> 
    As illustrated later, and without further proof, the way to
    deal with fallen leaves in multi-plane designs, when aggregation is
    used, is that RIFT requires all the ToF nodes to share the same 
    north topology database. This happens naturally in single plane design
    by the means of northbound flooding and south reflection but needs additional
    considerations in multi-plane fabrics.
    To satisfy this RIFT, in multi-plane designs, relies at the ToF 
    level on ring interconnection of switches in multiple planes.
    Other solutions are possible but they either need more cabling or 
    end up having much longer flooding paths and/or single points of
    failure.
</t>
<t>
    In detail, by reserving two ports on each Top-of-Fabric node
    it is possible to connect them together by interplane 
    bi-directional rings as illustrated in <xref target="interspine"/>.
    The rings will be used to exchange full north topology information
    between planes. All ToFs having same north topology allows by the means of
    transitive, negative disaggregation described in 
    <xref target="negdisaggreg"/> to efficiently fix any possible 
    fallen leaf scenario. 
    Somewhat as a side-effect, the exchange of information fulfills the
    ask to present full view of the fabric topology at the 
    Top-of-Fabric level, without the need to collate it from multiple
    points by additional complexity of technologies like 
    <xref target="RFC7752"/>.
</t>

  <figure align="center" anchor="interspine"
            title="Connecting Top-of-Fabric Nodes Across Planes by 
                   Rings">
                        <artwork align="center"><![CDATA[
  
       +---+  +---+  +---+  +---+  +---+  +---+  +--------+
       |   |  |   |  |   |  |   |  |   |  |   |  |        |
       |      |      |      |      |      |      |        |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o | |    | Plane A
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
      |      |      |      |      |      |      |         |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o | |    | Plane B
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
       |      |      |      |      |      |      |        |
                           ...                            |
       |      |      |      |      |      |      |        |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
   | | o |  | o |  | o |  | o |  | o |  | o |  | o | |    | Plane X
   +-|   |--|   |--|   |--|   |--|   |--|   |--|   |-+    |
     +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+  +-o-+      |
       |      |      |      |      |      |      |        |
       |   |  |   |  |   |  |   |  |   |  |   |  |        |
       +---+  +---+  +---+  +---+  +---+  +---+  +--------+
Rings    1      2      3      4      5      6      7

                        ]]></artwork>
                    </figure>


                </section>

               <section title="Addressing the Fallen Leaves Problem" 
                               anchor="FixFallen">

<t>
    One consequence of the "Fallen Leaf" problem is that some prefixes
    attached to the fallen leaf become unreachable from some of the 
    ToF nodes.
    RIFT proposes two methods to address this issue, the positive and 
    the negative disaggregation. Both methods flood South TIEs to 
    advertise the impacted prefix(es).
</t>    
<t>
    When used for the operation of disaggregation, a positive South 
    TIE, as usual, indicates reachability to a prefix of given length 
    and all addresses subsumed by it.
    In contrast, a negative route advertisement indicates that the 
    origin cannot route to the advertised prefix.
</t>    
<t>
    The positive disaggregation is originated by a router that can 
    still reach the advertised prefix, and the operation is not 
    transitive. In other words, the receiver does not generate its own
    flooding south as a consequence of receiving positive 
    disaggregation advertisements from a higher level node.
    The effect of a positive disaggregation is that the traffic to 
    the impacted prefix will follow the longest match and will be 
    limited to the northbound routers that advertised the more 
    specific route.
</t>    
<t>
    In contrast, the negative disaggregation can be transitive, and is
    propagated south when all the possible routes have been advertised
    as negative exceptions.
    A negative route advertisement is only actionable when the 
    negative prefix is aggregated by a positive route advertisement 
    for a shorter prefix.
    In such case, the negative advertisement "punches out a hole" in
    the positive route in the routing table, making the positive prefix
    reachable through
    the originator with the special consideration of the negative 
    prefix removing certain next hop neighbors.
</t>
<t>
    When the ToF is not partitioned, the collective southern flooding 
    of the positive disaggregation by the ToF nodes that can still 
    reach the impacted prefix is in general enough to cover all the 
    switches at the next level south, typically the ToP nodes. 
    If all those switches are aware of the disaggregation, they 
    collectively create a ceiling that intercepts all the traffic 
    north and forwards it to the ToF nodes that advertised the more
    specific route.
    In that case, the positive disaggregation alone is sufficient to 
    solve the fallen leaf problem.
</t>
<t>
    On the other hand, when the fabric is partitioned in planes, the 
    positive disaggregation from ToF nodes in different planes do not 
    reach the ToP switches in the affected plane and cannot solve the 
    fallen leaves problem.
    In other words, a breakage in a plane can only be solved in that 
    plane.
    Also, the selection of the plane for a packet typically occurs at 
    the leaf level and the disaggregation must be transitive and reach 
    all the leaves. In that case, the negative disaggregation is 
    necessary.
    The details on the RIFT approach to deal with fallen leaves in an
    optimal way are specified in <xref target="negdisaggreg"/>.
</t>
  

                </section>
  
            </section>

            <section title="Specification">

                <t>
                    This section specifies the protocol in a normative fashion by either
                    prescriptive procedures or behavior defined by Finite State Machines (FSM).
                </t>

<t>Some FSM figures are provided as <xref target="DOT"/> description due to
    limitations of ASCII art.</t>

<t>"On Entry" actions on FSM state are performed every time and right before the according
    state is entered, i.e. after any transitions from previous state.</t>

<t>"On Exit" actions are performed every time and immediately when a state is
    exited, i.e. before any transitions towards target state are performed.</t>

<t>Any attempt to transition from a state towards another on reception of
    an event where no action is specified MUST be considered an unrecoverable
    error.</t>

<t>The FSMs and procedures are normative in the sense that an implementation MUST implement
    them either literally or an implementation
    MUST exhibit externally observable
    behavior that is identical to the execution of the specified
    FSMs.</t>

<t>Where a FSM representation is inconvenient, i.e. the amount of procedures and kept state
    exceeds the amount of transitions, we defer to a more procedural description on
    data structures.</t>

                <section title="Transport">

                    <t>All packet formats are defined in <xref target="thrift">Thrift</xref>
                        models in <xref target="schema"/>.</t>


                    <t>The serialized model is carried in an envelope within
                    a UDP frame that provides security and allows validation/modification of
                    several important fields without de-serialization for performance and
                    security reasons.</t>


                </section>

                <section title="Link (Neighbor) Discovery (LIE Exchange)" anchor="LIE">

                      <t>RIFT LIE exchange auto-discovers neighbors, negotiates ZTP parameters and discovers
                          miscablings.
                          It uses a three-way handshake mechanism
                    which is a cleaned up version of <xref target="RFC5303"></xref>.
                    Observe that for easier comprehension
                        the terminology of one/two and three-way states does
                    NOT align with OSPF or ISIS FSMs albeit they use roughly same mechanisms.

                        The formation progresses under normal conditions
                        from one-way to two-way and then three-way state at which
                        point it is ready to exchange TIEs per <xref target="ties"/>.
                    </t>

<t>LIE
    exchange happens over well-known administratively
    locally scoped and configured or otherwise well-known
    IPv4 multicast address <xref target="RFC2365"/>
    and/or link-local multicast scope <xref target="RFC4291"/>
    for IPv6 <xref target="RFC8200"/> using a configured or otherwise
    a well-known destination UDP port defined in <xref target="constants"/>.
    LIEs SHOULD be sent with an IPv4 Time to Live (TTL) / IPv6 Hop Limit (HL) of 1
    to prevent RIFT information
    reaching beyond a single L3 next-hop in the topology.
    LIEs SHOULD be sent with network control precedence.
</t>


                    <t>
    Originating port
    of the LIE has no further significance other than identifying the origination point.
    LIEs are
    exchanged over all links running RIFT.
                    </t>
                    <t>
    An implementation MAY listen and send LIEs on IPv4 and/or
    IPv6 multicast addresses.  A node MUST NOT originate LIEs
    on an address family if it does not process received LIEs on that
    family. LIEs on same link are considered
    part of the same negotiation independent on the address family
    they arrive on. Observe further
    that the LIE source address may not identify
    the peer uniquely in unnumbered or link-local address cases so
    the response transmission MUST occur over the same interface the LIEs have been
    received on. A node MAY use any of the adjacency's source addresses it
    saw in
    LIEs on the specific interface during adjacency formation to send TIEs.
    That implies that an implementation MUST be ready to accept TIEs on
    all addresses it used as source of LIE frames.
</t>

<t>A three-way adjacency over any address family implies support
    for IPv4 forwarding if the `v4_forwarding_capable` flag is set to true and a node
    can use <xref target="RFC5549"/> type of forwarding in such a situation.
    It is expected that the whole fabric
    supports the same type of forwarding of address families on all the links.
    Operation of a fabric where only
    some of the links are supporting forwarding on an address family and
    others do not is outside the scope of this specification.
    </t>

<t>The protocol does NOT support selective disabling of
    address families, disabling v4 forwarding capability or any local address changes
    in three-way state, i.e. if a link has entered three-way
    IPv4 and/or IPv6 with a
    neighbor on an adjacency and it wants to stop supporting one of
    the families or change
    any of its local addresses or stop v4 forwarding, it has to
    tear down and rebuild the adjacency. It also has to remove any information
    it stored about the adjacency such as LIE source addresses seen.
    </t>

                    <t>
                        Unless
                        ZTP as described in <xref target="ZTP"/>
                        is used,
                        each node is
                        provisioned with the level at which it
                        is operating. It MAY be also provisioned
                        with its PoD. If any of those values is
                        undefined, then accordingly
                        a default level
                        and/or an "undefined" PoD are assumed.
                        This means that leaves
                        do not need to be configured at all if initial
                        configuration values are all left at "undefined" value.
                        Nodes above
                        ToP MUST remain at "any" PoD value which has
                        the same value as "undefined" PoD.
                        This information is propagated
                        in the LIEs
                        exchanged.
                    </t>

<t>Further definitions of leaf flags are found in
    <xref target="ZTP"/> given they have implications in terms
    of level and adjacency forming here.
    </t>
                    <t>A node tries to form a three-way adjacency
                        if and only if
                    </t>

                    <t><list style="numbers">
                        <t>the node is in the same PoD or either the node or
                            the neighbor
                            advertises "undefined/any" PoD membership (PoD# = 0) AND</t>
                                                <t>the neighboring node is
                                                    running the same MAJOR
                            schema version AND</t>
                        <t anchor="samepod">the neighbor is not member
                            of some PoD while the node
                            has a northbound adjacency already joining another
                            PoD AND</t>
                        <t>the neighboring node uses a valid System ID AND</t>
                        <t>the neighboring node uses a different System ID than the node
                            itself</t>
                        <t>the advertised MTUs match on both sides AND</t>
                        <t>both nodes advertise defined level values AND</t>
                        <t anchor="topmustHAL">[<list style="empty">
                            <t anchor="mustHAL">i) the node is at level 0 and has
                                no three way adjacencies already
                                to nodes at
                                Highest Adjacency Three-Way level (HAT as
                                defined later in <xref target="ZTPTerminology"/>) with level different than the adjacent
                                node OR
                            </t>
                            <t>ii) the node is not at level 0 and
                                the neighboring node is at level 0 OR</t>
                            <t >iii) both nodes are at level 0 AND both indicate
                                support for
                                <xref target="leaf2leaf"/> OR</t>
                            <t>iv) neither node is at level 0 and the
                                neighboring node is at most one level away
                            </t>
                        </list>].
                        </t>

                    </list>
                    </t>

                    <t>The rules checking PoD numbering MAY be optionally disregarded
                        by a node if PoD detection is undesirable or has to be
                        ignored. This will not affect the correctness of the protocol
                        except preventing detection of certain miscabling cases.</t>

                    <t>A node configured with "undefined" PoD membership MUST,
                        after building first northbound three way adjacencies to a node
                        being in
                        a defined PoD, advertise that PoD
                        as part of its LIEs. In case that adjacency is lost,
                        from all available northbound
                        three way adjacencies the node with the highest System ID
                        and defined PoD is chosen. That way the northmost defined
                        PoD value (normally the ToP nodes) can
                        diffuse southbound towards the leaves "forcing"
                        the PoD value on any node with "undefined" PoD.
                        </t>

                    <t>LIEs arriving with IPv4 Time to Live (TTL) / IPv6 Hop Limit (HL) larger than 1 MUST be ignored.
                    </t>

                    <t>A node SHOULD NOT send out LIEs without defined level
                        in the header but in certain scenarios it may
                        be beneficial for trouble-shooting purposes.</t>

<section title="LIE FSM">

<t>
        This section specifies the precise, normative LIE FSM and
   can be omitted unless the reader is pursuing an implementation of
   the protocol.
</t>
<t>Initial state is `OneWay`.
</t>

<t>Event `MultipleNeighbors` occurs normally when more than two nodes see each
    other on the same link or a remote node is quickly reconfigured or rebooted
    without regressing to `OneWay` first. Each
    occurrence of the event SHOULD generate a clear, according notification to help
    operational deployments.</t>

<t>The machine sends LIEs on several transitions to accelerate adjacency
    bring-up without waiting for the timer tic.</t>

<figure align="center" title="LIE FSM">
<artwork align="left"><![CDATA[
            Enter
              |
              V
        +-----------+
        | OneWay    |<----+
        |           |     | HALChanged [StoreHAL]
        | Entry:    |     | HALSChanged [StoreHALS]
        | [CleanUp] |     | HATChanged [StoreHAT]
        |           |     | HoldTimerExpired [-]
        |           |     | InstanceNameMismatch [-]
        |           |     | LevelChanged [UpdateLevel, PUSH SendLie]
        |           |     | LieReceived [ProcessLIE]
        |           |     | MTUMismatch [-]
        |           |     | NeighborAddressAdded [-]
        |           |     | NeighborChangedAddress [-]
        |           |     | NeighborChangedLevel [-]
        |           |     | NeighborChangedMinorFields [-]
        |           |     | NeighborDroppedReflection [-]
        |           |     | PODMismatch [-]
        |           |     | SendLIE [SendLIE]
        |           |     | TimerTick [PUSH SendLIE]
        |           |     | UnacceptableHeader
        |           |     | UpdateZTPOffer [SendOfferToZTPFSM]
        |           |-----+
        |           |
        |           |<--------------------- (ThreeWay)
        |           |--------------------->
        |           | ValidReflection [-]
        |           |
        |           |---------------------> (Multiple
        |           | MultipleNeighbors      Neighbors
        +-----------+  [StartMulNeighTimer]  Wait)
            ^    |                 
            |    |                 
            |    | NewNeighbor [PUSH SendLIE]
            |    V
           (TwoWay)
]]></artwork></figure>
<figure align="center" title="LIE FSM (continued)">
<artwork align="left"><![CDATA[
           (OneWay)
            |    ^
            |    | HoldTimeExpired [-]
            |    | InstanceNameMismatch [-]
            |    | LevelChanged [StoreLevel]
            |    | MTUMismatch [-]
            |    | NeighborChangedAddress [-]
            |    | NeighborChangedLevel [-]
            |    | PODMismatch [-]
            |    | UnacceptableHeader [-]
            V    |
        +-----------+
        | TwoWay    |<----+
        |           |     | HALChanged [StoreHAL]
        |           |     | HALSChanged [StoreHALS]
        |           |     | HATChanged [StoreHAT]
        |           |     | LevelChanged [StoreLevel]
        |           |     | LIERcvd [ProcessLIE]
        |           |     | SendLIE [SendLIE]
        |           |     | TimerTick [PUSH SendLIE, 
        |           |     |            IF HoldTimer expired
        |           |     |            PUSH HoldTimerExpired]
        |           |     | UpdateZTPOffer [SendOfferToZTPFSM]
        |           |-----+
        |           |
        |           |<----------------------
        |           |----------------------> (Multiple
        |           | NewNeighbor             Neighbors
        |           |   [StartMulNeighTimer]  Wait)
        |           | MultipleNeighbors
        +-----------+  [StartMulNeighTimer]
            ^    |
            |    | ValidReflection [-]
            |    V
          (ThreeWay)
]]></artwork></figure>
<figure align="center" title="LIE FSM (continued)">
<artwork align="left"><![CDATA[
                   (TwoWay)    (OneWay)
                    ^    |        ^
                    |    |        | HoldTimerExpired [-]
                    |    |        | InstanceNameMismatch [-]
                    |    |        | LevelChanged [UpdateLevel]
                    |    |        | MTUMismatch [-]
                    |    |        | NeighborChangedAddress [-]
                    |    |        | NeighborChangedLevel [-]
NeighborDropped-    |    |        | PODMismatch [-]
     Reflection [-] |    |        | UnacceptableHeader [-]
                    |    V        |
                +-----------+     |
                | ThreeWay  |-----+
                |           |
                |           |<----+
                |           |     | HALChanged [StoreHAL]
                |           |     | HALSChanged [StoreHALS]
                |           |     | HATChanged [StoreHAT]
                |           |     | LieReceived [ProcessLIE]
                |           |     | SendLIE [SendLIE]
                |           |     | TimerTick [PUSH SendLie, 
                |           |     |            IF HoldTimer expired
                |           |     |            PUSH HoldTimerExpired]
                |           |     | UpdateZTPOffer [SendOfferToZTPFSM]
                |           |     | ValidReflection [-]
                |           |-----+
                |           |----------------------> (Multiple
                |           | MultipleNeighbors       Neighbors
                +-----------+   [StartMulNeighTimer]  Wait)
]]></artwork></figure>
<figure align="center" title="LIE FSM (continued)">
<artwork align="left"><![CDATA[
       (TwoWay) (ThreeWay)
            |     |
            V     V
        +------------+
        | Multiple   |<----+
        | Neighbors  |     | HALChanged [StoreHAL]
        | Wait       |     | HALSChanged [StoreHALS]
        |            |     | HATChanged [StoreHAT]
        |            |     | MultipleNeighbors
        |            |     |   [StartMultipleNeighborsTimer]
        |            |     | TimerTick [IF MulNeighTimer expired
        |            |     |            PUSH MultipleNeighborsDone]
        |            |     | UpdateZTPOffer [SendOfferToZTP]
        |            |-----+
        |            |
        |            |<---------------------------
        |            |---------------------------> (OneWay)
        |            | LevelChanged [StoreLevel]
        +------------+ MultipleNeighborsDone [-]
]]></artwork></figure>

<!-- generated output -->

 <t>Events</t>
<t><list style="symbols">
	<t>TimerTick:
		one second timer tic</t>
	<t>LevelChanged:
		node's level has been changed by ZTP or configuration</t>
	<t>HALChanged:
		best HAL computed by ZTP has changed</t>
	<t>HATChanged:
		HAT computed by ZTP has changed</t>
	<t>HALSChanged:
		set of HAL offering systems computed by ZTP has changed</t>
	<t>LieRcvd:
		received LIE</t>
	<t>NewNeighbor:
		new neighbor parsed</t>
	<t>ValidReflection:
		received own reflection from neighbor</t>
	<t>NeighborDroppedReflection:
		lost previous own reflection from neighbor</t>
	<t>NeighborChangedLevel:
		neighbor changed advertised level</t>
	<t>NeighborChangedAddress:
		neighbor changed IP address</t>
	<t>UnacceptableHeader:
		unacceptable header seen</t>
	<t>MTUMismatch:
		MTU mismatched</t>
	<t>InstanceNameMismatch:
		Instance mismatched</t>
	<t>PODMismatch:
		Unacceptable PoD seen</t>
	<t>HoldtimeExpired:
		adjacency hold down expired</t>
	<t>MultipleNeighbors:
		more than one neighbor seen on interface</t>
	<t>MultipleNeighborsDone:
		cooldown for multiple neighbors expired</t>
	<t>SendLie:
		send a LIE out</t>
	<t>UpdateZTPOffer:
		update this node's ZTP offer</t>

</list></t>

<t>Actions</t>
<t><list>
	<t>on MultipleNeighbors in OneWay finishes in MultipleNeighborsWait:
		start multiple neighbors timer as 4 * DEFAULT_LIE_HOLDTIME
		</t>
	<t>on NeighborDroppedReflection in ThreeWay finishes in TwoWay:
		no action
		</t>
	<t>on NeighborDroppedReflection in OneWay finishes in OneWay:
		no action
		</t>
	<t>on PODMismatch in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on NewNeighbor in TwoWay finishes in MultipleNeighborsWait:
		PUSH SendLie event
		</t>
	<t>on LieRcvd in OneWay finishes in OneWay:
		PROCESS_LIE
		</t>
	<t>on UnacceptableHeader in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on UpdateZTPOffer in TwoWay finishes in TwoWay:
		send offer to ZTP FSM
		</t>
	<t>on NeighborChangedAddress in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on HALChanged in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		store new HAL
		</t>
	<t>on NeighborChangedAddress in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on MultipleNeighbors in TwoWay finishes in MultipleNeighborsWait:
		start multiple neighbors timer as 4 * DEFAULT_LIE_HOLDTIME
		</t>
	<t>on LevelChanged in ThreeWay finishes in OneWay:
		update level with event value
		</t>
	<t>on LieRcvd in ThreeWay finishes in ThreeWay:
		PROCESS_LIE
		</t>
	<t>on ValidReflection in OneWay finishes in ThreeWay:
		no action
		</t>
	<t>on NeighborChangedLevel in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on MultipleNeighbors in ThreeWay finishes in MultipleNeighborsWait:
		start multiple neighbors timer as 4 * DEFAULT_LIE_HOLDTIME
		</t>
	<t>on InstanceNameMismatch in OneWay finishes in OneWay:
		no action
		</t>
	<t>on NewNeighbor in OneWay finishes in TwoWay:
		PUSH SendLie event
		</t>
	<t>on UpdateZTPOffer in OneWay finishes in OneWay:
		send offer to ZTP FSM
		</t>
	<t>on UpdateZTPOffer in ThreeWay finishes in ThreeWay:
		send offer to ZTP FSM
		</t>
	<t>on MTUMismatch in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on TimerTick in OneWay finishes in OneWay:
		PUSH SendLie event
		</t>
	<t>on SendLie in TwoWay finishes in TwoWay:
		SEND_LIE
		</t>
	<t>on ValidReflection in ThreeWay finishes in ThreeWay:
		no action
		</t>
	<t>on InstanceNameMismatch in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on HoldtimeExpired in OneWay finishes in OneWay:
		no action
		</t>
	<t>on TimerTick in ThreeWay finishes in ThreeWay:
		PUSH SendLie event, if holdtime expired PUSH HoldtimeExpired event
		</t>
	<t>on HALChanged in TwoWay finishes in TwoWay:
		store new HAL
		</t>
	<t>on HoldtimeExpired in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on HALSChanged in TwoWay finishes in TwoWay:
		store HALS
		</t>
	<t>on HALSChanged in ThreeWay finishes in ThreeWay:
		store HALS
		</t>
	<t>on ValidReflection in TwoWay finishes in ThreeWay:
		no action
		</t>
	<t>on MultipleNeighborsDone in MultipleNeighborsWait finishes in OneWay:
		no action
		</t>
	<t>on NeighborAddressAdded in OneWay finishes in OneWay:
		no action
		</t>
	<t>on TimerTick in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		decrement MultipleNeighbors timer, if expired PUSH MultipleNeighborsDone
		</t>
	<t>on MTUMismatch in OneWay finishes in OneWay:
		no action
		</t>
	<t>on MultipleNeighbors in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		start multiple neighbors timer as 4 * DEFAULT_LIE_HOLDTIME
		</t>
	<t>on LieRcvd in TwoWay finishes in TwoWay:
		PROCESS_LIE
		</t>
	<t>on HATChanged in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		store HAT
		</t>
	<t>on HoldtimeExpired in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on NeighborChangedLevel in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on LevelChanged in OneWay finishes in OneWay:
		update level with event value, PUSH SendLie event
		</t>
	<t>on SendLie in OneWay finishes in OneWay:
		SEND_LIE
		</t>
	<t>on HATChanged in OneWay finishes in OneWay:
		store HAT
		</t>
	<t>on LevelChanged in TwoWay finishes in TwoWay:
		update level with event value
		</t>
	<t>on HATChanged in TwoWay finishes in TwoWay:
		store HAT
		</t>
	<t>on PODMismatch in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on LevelChanged in MultipleNeighborsWait finishes in OneWay:
		update level with event value
		</t>
	<t>on UnacceptableHeader in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on NeighborChangedLevel in OneWay finishes in OneWay:
		no action
		</t>
	<t>on InstanceNameMismatch in ThreeWay finishes in OneWay:
		no action
		</t>
	<t>on HATChanged in ThreeWay finishes in ThreeWay:
		store HAT
		</t>
	<t>on HALChanged in OneWay finishes in OneWay:
		store new HAL
		</t>
	<t>on UnacceptableHeader in OneWay finishes in OneWay:
		no action
		</t>
	<t>on HALChanged in ThreeWay finishes in ThreeWay:
		store new HAL
		</t>
	<t>on UpdateZTPOffer in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		send offer to ZTP FSM
		</t>
	<t>on NeighborChangedMinorFields in OneWay finishes in OneWay:
		no action
		</t>
	<t>on NeighborChangedAddress in OneWay finishes in OneWay:
		no action
		</t>
	<t>on MTUMismatch in TwoWay finishes in OneWay:
		no action
		</t>
	<t>on PODMismatch in OneWay finishes in OneWay:
		no action
		</t>
	<t>on SendLie in ThreeWay finishes in ThreeWay:
		SEND_LIE
		</t>
	<t>on TimerTick in TwoWay finishes in TwoWay:
		PUSH SendLie event, if holdtime expired PUSH HoldtimeExpired event
		</t>
	<t>on HALSChanged in OneWay finishes in OneWay:
		store HALS
		</t>
	<t>on HALSChanged in MultipleNeighborsWait finishes in MultipleNeighborsWait:
		store HALS
		</t>
	<t>on Entry into OneWay:
		CLEANUP
		</t>

</list></t>



<t>
    Following words are used for well known procedures:

    <list style="numbers">
        <t>PUSH Event: pushes an event to be executed by the FSM upon exit of this action</t>
        <t>CLEANUP: neighbor MUST be reset to unknown</t>
        <t>SEND_LIE: create a new LIE packet
            <list style="numbers">
                <t>reflecting the neighbor if known and valid and</t>
                <t>setting the necessary `not_a_ztp_offer` variable if level was derived from last
                    known neighbor on this interface and</t>
                <t>setting `you_are_not_flood_repeater` to computed value</t>
            </list>
        </t>
        <t>PROCESS_LIE:
            <list style="numbers">
                <t>if lie has wrong major version OR our own system ID or invalid system ID then CLEANUP
                    else
                </t>
                <t>if lie has non matching MTUs then
                    CLEANUP,
                    PUSH UpdateZTPOffer,
                    PUSH MTUMismatch
                    else
                </t>
                <t>if PoD rules do not allow adjacency forming then
                    CLEANUP,
                    PUSH PODMismatch,
                    PUSH MTUMismatch
                    else
                </t>
                <t>if lie has undefined level OR my level is undefined OR
                    this node is leaf and remote level lower than HAT OR
                    (lie's level is not leaf AND its difference is more than one from my level) then
                    CLEANUP,
                    PUSH UpdateZTPOffer,
                    PUSH UnacceptableHeader
                    else
                </t>
                <t>PUSH UpdateZTPOffer,
                    construct temporary new neighbor structure with values from lie,
                    if no current neighbor exists then set neighbor to new neighbor, PUSH NewNeighbor event, CHECK_THREE_WAY else
                    <list style="numbers">
                        <t>if current neighbor system ID differs from lie's system ID then PUSH MultipleNeighbors
                            else
                        </t>
                        <t>if current neighbor stored level differs from lie's level then PUSH NeighborChangedLevel
                            else
                        </t>
                        <t>if current neighbor stored IPv4/v6 address differs from lie's address then PUSH NeighborChangedAddress
                            else
                        </t>

                        <t>if any of neighbor's flood address port, name, local linkid changed then PUSH NeighborChangedMinorFields and</t>
                        <t>CHECK_THREE_WAY</t>

                    </list>
                </t>
            </list>
        </t>
        <t>CHECK_THREE_WAY: if current state is one-way do nothing else
            <list style="numbers">
                <t>if lie packet does not contain neighbor then if current state is three-way then PUSH  NeighborDroppedReflection else</t>
                <t>if packet reflects this system's ID and local port and state is three-way then PUSH event ValidReflection else PUSH event MultipleNeighbors</t>
            </list>
        </t>
    </list>

</t>


    <!-- end generated output -->


            </section>

                </section>

                <section title="Topology Exchange (TIE Exchange)" anchor="ties">

                    <section title="Topology Information Elements">
                        <t>Topology and reachability information in RIFT is
                            conveyed by the means of TIEs which have good
                            amount of commonalities with LSAs in OSPF.

</t>
                        <t>The TIE exchange
                            mechanism uses
                            the port indicated by each node in the LIE
                            exchange and the interface on which the adjacency has been
                            formed as destination. It SHOULD use TTL of 1 as well and
                            set inter-network control precedence on according packets.
                            </t>

                            <t>TIEs
                            contain sequence numbers, lifetimes and a type.
                            Each type has ample identifying number space
                            and information
                            is spread across possibly many TIEs of a certain
                            type by the means of a hash function that a node
                            or deployment can individually determine. One extreme
                            design choice is a prefix per TIE which leads to
                            more BGP-like behavior where small increments
                            are only advertised on route changes
                            vs. deploying with dense prefix packing into few TIEs
                            leading to more traditional IGP trade-off with fewer
                            TIEs. An implementation may even rehash prefix to TIE
                                mapping at any time
                            at the cost of significant amount of re-advertisements
                            of TIEs. </t>


                        <t>More information about the TIE structure can be
                            found in the schema in <xref target="schema"/>.
                            </t>

                        </section>

                    <section title="South- and Northbound Representation">
                            <t>A central concept of RIFT is that each node represents
                                itself differently depending on the direction in
                                which it is advertising information.

                                More precisely,
                                a spine node represents two different databases
                                over its adjacencies
                                depending whether it advertises TIEs to the
                                north or to the south/sideways.

                                We call those differing TIE databases
                                either south- or
                                northbound (South TIEs and North TIEs)
                                depending on the direction of distribution.
                           </t>

<t> The North TIEs hold all of the node's adjacencies and local
    prefixes while the  South TIEs hold only all of the node's adjacencies, the
    default prefix with necessary disaggregated prefixes and local prefixes.
    We will explain this in detail further in <xref target="disaggregate"/>.
</t>

<t>The TIE types are mostly symmetric in both directions and <xref target="tie-types"/>
    provides a quick reference to main TIE types including direction
    and their function.
    </t>

<texttable anchor="tie-types"
    title="TIE Types"
    style="all">

    <ttcol>TIE-Type</ttcol><ttcol>Content</ttcol>

    <c>Node North TIE</c>         <c>node properties and adjacencies</c>
    <c>Node South TIE</c>        <c>same content as node North TIE</c>
    <c>Prefix North TIE</c>             <c>contains nodes' directly reachable prefixes</c>
    <c>Prefix South TIE</c>            <c>contains originated defaults and directly reachable prefixes</c>
    <c>Positive Disaggregation South TIE</c> <c>contains disaggregated prefixes</c>
    <c>Negative Disaggregation South TIE</c> <c>contains special, negatively disaggregated prefixes to support multi-plane designs</c>
    <c>External Prefix North TIE</c>    <c>contains external prefixes</c>
<c>Key-Value North TIE</c>            <c>contains nodes northbound KVs</c>
<c>Key-Value South TIE</c>          <c>contains nodes southbound KVs</c>
    </texttable>

<t>As an example illustrating a databases holding both
    representations, consider the
    topology in <xref target="pic-topo-three"/> with the optional
    link between spine 111 and spine 112 (so that the flooding on an
    East-West link can be shown). This example assumes unnumbered
    interfaces.  First, here are the TIEs generated by some
    nodes. For simplicity, the key value elements which may be included
    in their South TIEs or North TIEs are not shown.</t>

<figure align="center" anchor="ties-topo-three"
    title="example TIES generated in a 2 level spine-and-leaf topology">
    <artwork align="left"><![CDATA[

        ToF 21 South TIEs:
        Node South TIE:
          NodeElement(level=2, neighbors((Spine 111, level 1, cost 1),
          (Spine 112, level 1, cost 1), (Spine 121, level 1, cost 1),
          (Spine 122, level 1, cost 1)))
        Prefix South TIE:
          SouthPrefixesElement(prefixes(0/0, cost 1), (::/0, cost 1))

        Spine 111 South TIEs:
        Node South TIE:
          NodeElement(level=1, neighbors((ToF 21, level 2, cost 1,
                      links(...)),
          (ToF 22, level 2, cost 1, links(...)),
          (Spine 112, level 1, cost 1, links(...)),
          (Leaf111, level 0, cost 1, links(...)),
          (Leaf112, level 0, cost 1, links(...))))
        Prefix South TIE:
          SouthPrefixesElement(prefixes(0/0, cost 1), (::/0, cost 1))

        Spine 111 North TIEs:
        Node North TIE:
          NodeElement(level=1,
          neighbors((ToF 21, level 2, cost 1, links(...)),
          (ToF 22, level 2, cost 1, links(...)),
          (Spine 112, level 1, cost 1, links(...)),
          (Leaf111, level 0, cost 1, links(...)),
          (Leaf112, level 0, cost 1, links(...))))
        Prefix North TIE:
          NorthPrefixesElement(prefixes(Spine 111.loopback)

        Spine 121 South TIEs:
        Node South TIE:
          NodeElement(level=1, neighbors((ToF 21,level 2,cost 1),
          (ToF 22, level 2, cost 1), (Leaf121, level 0, cost 1),
          (Leaf122, level 0, cost 1)))
        Prefix South TIE:
          SouthPrefixesElement(prefixes(0/0, cost 1), (::/0, cost 1))

        Spine 121 North TIEs:
        Node North TIE:
          NodeElement(level=1,
          neighbors((ToF 21, level 2, cost 1, links(...)),
          (ToF 22, level 2, cost 1, links(...)),
          (Leaf121, level 0, cost 1, links(...)),
          (Leaf122, level 0, cost 1, links(...))))
        Prefix North TIE:
          NorthPrefixesElement(prefixes(Spine 121.loopback)

        Leaf112 North TIEs:
        Node North TIE:
          NodeElement(level=0,
          neighbors((Spine 111, level 1, cost 1, links(...)),
          (Spine 112, level 1, cost 1, links(...))))
        Prefix North TIE:
          NorthPrefixesElement(prefixes(Leaf112.loopback, Prefix112,
          Prefix_MH))
    ]]>
    </artwork>
</figure>

<t>
    It may be here not necessarily obvious why the node South TIEs contain all the
    adjacencies of the according node.
    This will be necessary for algorithms given in <xref target="mpr"/> and <xref target="bwb"/>.
</t>
</section>

<section title="Flooding">
<t>
The  mechanism used to distribute TIEs is the well-known (albeit
modified in several
respects to take advantage of fat tree topology) flooding mechanism used by
today's link-state protocols. Although flooding is initially more demanding to implement
    it avoids many
problems with update style used in
diffused computation
such as distance vector protocols.
Since flooding tends to present an unscalable burden in large, densely meshed
topologies (fat trees being unfortunately such a topology)
we provide as solution a close to optimal global flood reduction and load balancing
optimization in <xref target="mpr"/>.
</t>

<t>
As described before, TIEs themselves are transported over UDP with the
ports indicated in the LIE
    exchanges and using the destination address

    on which the LIE adjacency
    has been formed. For unnumbered IPv4 interfaces same considerations
    apply as in equivalent OSPF case.</t>

    <section title="Normative Flooding Procedures" anchor="floodproc">

        <t>On reception of a TIE with an undefined level value in the packet header
    the node SHOULD issue a warning and indiscriminately discard the packet.</t>

        <t>This section specifies the precise, normative flooding mechanism and can be omitted unless the
            reader is pursuing an implementation of the protocol.
        </t>

                <t>
                    Flooding Procedures are described in terms of a flooding state of an adjacency
                    and resulting operations on it driven by packet arrivals. The FSM itself has basically just a
                    single state and is not well suited to represent the behavior. An implementation MUST
                    behave on the wire in the same way as the provided normative procedures of this paragraph.
                </t>

                <t>
                    RIFT does not specify any kind of flood rate limiting since such specifications
                    always assume particular points in available technology speeds and feeds and
                    those points are shifting at faster and faster rate (speed of light holding for
                    the moment). The encoded packets provide hints to react accordingly to
                    losses or overruns.

                </t>

                <t>Flooding of all according topology exchange elements SHOULD be performed at
                    highest feasible rate whereas the rate of transmission MUST be throttled by
                    reacting to adequate features of the system such as e.g. queue lengths or
                    congestion indications in the protocol packets.
                </t>

                <t>
                    A node SHOULD NOT send out any topology information elements if the adjacancy is not in a
                    "three-way" state. No further tightening of this rule is possible due to possible link
                    buffering and re-ordering of LIEs and TIEs/TIDEs/TIREs.
                </t>

                <t>
                    A node MUST drop any received  TIEs/TIDEs/TIREs unless it is in three-way state.
                </t>

                <t>
                    TIDEs and TIREs MUST NOT be re-flooded the way
                    TIEs of other nodes are are MUST be always generated by the node itself and cross only
                    to the neighboring node.
                </t>

                <section title="FloodState Structure per Adjacency">

                    <t>The structure contains conceptually the following elements.
                        The word collection
                        or queue indicates a set of elements that can be iterated:
                        <list style="hanging">
                            <t hangText="TIES_TX:">Collection containing all the TIEs to
                                transmit on the adjacency.</t>
                            <t hangText="TIES_ACK:">Collection containing all the TIEs that
                                have to be acknowledged on the adjacency.</t>
                            <t hangText="TIES_REQ:">Collection containing all the TIE headers
                                that have to be requested on the adjacency.</t>
                            <t hangText="TIES_RTX:">Collection containing all TIEs that need
                                retransmission with the according time to retransmit.</t>
                        </list>

                    </t>

                    <t>Following words are used for well known procedures operating on this structure:

                        <list style="hanging">
                            <t hangText="TIE">Describes either a full RIFT TIE or accordingly
                                just the `TIEHeader` or `TIEID`. The according meaning is
                                unambiguously contained in the context of the algorithm. </t>
                            <t hangText="is_flood_reduced(TIE):">returns whether a TIE can be
                                flood reduced or not.</t>

                            <t hangText="is_tide_entry_filtered(TIE):">returns whether a header
                                should be propagated in TIDE according to flooding scopes.</t>
                            <t hangText="is_request_filtered(TIE):">returns whether a TIE request
                                should be propagated to neighbor or not according to flooding scopes.</t>
                            <t hangText="is_flood_filtered(TIE):">returns whether a TIE requested be
                                flooded to neighbor or not according to flooding scopes.</t>
                            <t hangText="try_to_transmit_tie(TIE):">
                                <list style="letters">
                                    <t>if not is_flood_filtered(TIE) then
                                        <list style="numbers">
                                            <t>remove TIE from TIES_RTX if present </t>
                                            <t>if TIE" with same key on TIES_ACK then
                                                <list style="format %c.">
                                                    <t>if TIE" same or newer than TIE do nothing else </t>
                                                    <t>remove TIE" from TIES_ACK and add TIE to TIES_TX</t>
                                                </list>
                                            </t>
                                            <t>else insert TIE into TIES_TX</t>
                                        </list>
                                    </t>
                                </list>
                            </t>
                            <t hangText="ack_tie(TIE):">remove TIE from all collections and then
                                insert TIE into TIES_ACK.</t>
                            <t hangText="tie_been_acked(TIE):">remove TIE from all collections.</t>
                            <t hangText="remove_from_all_queues(TIE):">same as `tie_been_acked`.</t>
                            <t hangText="request_tie(TIE):">if not is_request_filtered(TIE) then
                                remove_from_all_queues(TIE) and add to TIES_REQ.</t>
                            <t hangText="move_to_rtx_list(TIE):">remove TIE from TIES_TX and then
                                add to TIES_RTX using TIE retransmission interval.</t>
                            <t hangText="clear_requests(TIEs):">remove all TIEs from TIES_REQ.</t>
                            <t hangText="bump_own_tie(TIE):">for self-originated TIE
                                originate an empty or re-generate with
                                version number higher then the one in TIE.</t>
                        </list>
                    </t>

                    <t>
                        The collection SHOULD be served with following priorities if the system
                        cannot process all the collections in real time:
                        <list>
                            <t>Elements on TIES_ACK should be processed with highest priority</t>
                            <t>TIES_TX</t>
                            <t>TIES_REQ and TIES_RTX</t>
                        </list>
                    </t>
                </section>


                <section title="TIDEs">

                    <t>
                        `TIEID` and `TIEHeader` space forms a strict total order (modulo
                        incomparable
                        sequence numbers in the very unlikely event that
                        can occur if a TIE is "stuck" in a part of a network while the originator
                        reboots and reissues TIEs many times
                        to the point its sequence# rolls over and forms incomparable distance
                        to the "stuck" copy) which implies that
                        a comparison relation is possible between two elements. With that it is
                        implicitly
                        possible to compare TIEs, TIEHeaders and TIEIDs to each other whereas the
                        shortest
                        viable key is always implied.
                    </t>

                    <t>When generating and sending TIDEs an implementation SHOULD ensure that
                        enough bandwidth is left to send elements of Floodstate structure.
                    </t>

                    <section title="TIDE Generation">
                        <t>As given by timer constant, periodically generate TIDEs by:

                            <list>
                                <t>NEXT_TIDE_ID: ID of next TIE to be sent in TIDE.</t>
                                <t>TIDE_START: Begin of TIDE packet range.</t>
                            </list>

                            <list style="letters">
                                <t>NEXT_TIDE_ID = MIN_TIEID</t>
                                <t>while NEXT_TIDE_ID not equal to MAX_TIEID do
                                    <list style="numbers">
                                        <t>TIDE_START = NEXT_TIDE_ID</t>
                                        <t>HEADERS = At most TIRDEs_PER_PKT headers in TIEDB
                                            starting
                                            at NEXT_TIDE_ID or higher that SHOULD be filtered by
                                            is_tide_entry_filtered and MUST either have a lifetime left > 0
                                            or have no content
                                        </t>
                                        <t>if HEADERS is empty then START = MIN_TIEID else START =
                                            first
                                            element in HEADERS
                                        </t>
                                        <t>if HEADERS' size less than TIRDEs_PER_PKT then END =
                                            MAX_TIEID
                                            else END = last element in HEADERS
                                        </t>
                                        <t>send sorted HEADERS as TIDE setting START and END as
                                            its range
                                        </t>
                                        <t>NEXT_TIDE_ID = END</t>
                                    </list>
                                </t>
                            </list>
                        </t>
                            <t>The constant `TIRDEs_PER_PKT` SHOULD be generated and used by the
                                implementation to
                                limit the amount of TIE headers per TIDE so the sent TIDE PDU
                                does not exceed interface MTU.
                            </t>
                            <t>
                                TIDE PDUs SHOULD be spaced on sending to prevent packet drops.
                            </t>

                    </section>

                    <section title="TIDE Processing">

                        <t>On reception of TIDEs the following processing is performed:


                        <list>
                            <t>TXKEYS: Collection of TIE Headers to be send after processing of the packet</t>
                            <t>REQKEYS: Collection of TIEIDs to be requested after processing of the packet</t>
                            <t>CLEARKEYS: Collection of TIEIDs to be removed from flood state queues</t>
                            <t>LASTPROCESSED: Last processed TIEID in TIDE</t>
                            <t>DBTIE: TIE in the LSDB if found</t>
                        </list>

                        <list style="letters">

                            <t>LASTPROCESSED = TIDE.start_range</t>
                            <t>for every HEADER in TIDE do
                                <list style="numbers">
                                    <t>DBTIE = find HEADER in current LSDB </t>
                                    <t>if HEADER &lt; LASTPROCESSED then report error and reset adjacency and return</t>
                                    <t>put all TIEs in LSDB where (TIE.HEADER &gt; LASTPROCESSED and
                                        TIE.HEADER &lt; HEADER) into TXKEYS</t>
                                    <t>LASTPROCESSED = HEADER</t>
                                    <t>if DBTIE not found then
                                        <list  style="format %I) ">
                                            <t>if originator is this node then bump_own_tie</t>
                                            <t>else put HEADER into REQKEYS</t>
                                        </list>
                                    </t>
                                    <t>if DBTIE.HEADER &lt; HEADER then
                                        <list  style="format %I) ">
                                            <t>if originator is this node then bump_own_tie
                                                else
                                                <list  style="format %i. ">
                                                    <t>if this is a North TIE header from a northbound neighbor then
                                                            override DBTIE in LSDB with HEADER</t>
                                                    <t>else put HEADER into REQKEYS</t>
                                                </list>
                                            </t>
                                        </list>
                                    </t>
                                    <t>if DBTIE.HEADER &gt; HEADER then put DBTIE.HEADER into TXKEYS </t>
                                    <t>if DBTIE.HEADER = HEADER then
                                        <list   style="format %I) ">
                                            <t>if DBTIE has content already then put DBTIE.HEADER into CLEARKEYS</t>
                                            <t>else put HEADER into REQKEYS</t>
                                        </list>
                                    </t>
                                </list>
                            </t>
                            <t>put all TIEs in LSDB where (TIE.HEADER > LASTPROCESSED and
                                TIE.HEADER &le; TIDE.end_range) into TXKEYS </t>
                            <t>for all TIEs in TXKEYS try_to_transmit_tie(TIE) </t>
                            <t>for all TIEs in REQKEYS request_tie(TIE) </t>
                            <t>for all TIEs in CLEARKEYS remove_from_all_queues(TIE)</t>
                        </list>
                        </t>
                    </section>
                </section>

                <section title="TIREs">
                    <section title="TIRE Generation">
                        <t>
                    There is not much to say here. Elements from both TIES_REQ and TIES_ACK
                            MUST be collected
                        and sent out as fast as feasible as TIREs. When sending TIREs
                            with elements from TIES_REQ the `lifetime` field MUST be set to 0
                            to force reflooding from the neighbor even if the TIEs seem to be
                            same.
                        </t>
                    </section>

                    <section title="TIRE Processing">

                        <t>On reception of TIREs the following processing is performed:

                            <list>
                                <t>TXKEYS: Collection of TIE Headers to be send after processing of the packet</t>
                                <t>REQKEYS: Collection of TIEIDs to be requested after processing of the packet</t>
                                <t>ACKKEYS: Collection of TIEIDs that have been acked</t>
                                <t>DBTIE: TIE in the LSDB if found</t>
                            </list>

                            <list style="letters">

                                <t>for every HEADER in TIRE do
                                    <list style="numbers">
                                        <t>DBTIE = find HEADER in current LSDB </t>
                                        <t>if DBTIE not found then do nothing </t>
                                        <t>if DBTIE.HEADER &lt; HEADER then put HEADER into REQKEYS </t>
                                        <t>if DBTIE.HEADER &gt; HEADER then put DBTIE.HEADER into TXKEYS </t>
                                        <t>if DBTIE.HEADER = HEADER then put DBTIE.HEADER into ACKKEYS</t>
                                    </list>
                                </t>

                                <t>for all TIEs in TXKEYS try_to_transmit_tie(TIE) </t>
                                <t>for all TIEs in REQKEYS request_tie(TIE) </t>
                                <t>for all TIEs in ACKKEYS tie_been_acked(TIE) </t>
                            </list>
                        </t>

                    </section>
                </section>

                <section title="TIEs Processing on Flood State Adjacency">

                    <t>On reception of TIEs the following processing is performed:

                        <list>
                            <t>ACKTIE: TIE to acknowledge</t>
                            <t>TXTIE: TIE to transmit</t>
                            <t>DBTIE: TIE in the LSDB if found</t>
                        </list>

                        <list style="letters">
                            <t>DBTIE = find TIE in current LSDB </t>
                            <t>if DBTIE not found then
                                <list style="numbers">
                                    <t>if originator is this node then bump_own_tie with
                                        a short remaining lifetime</t>
                                    <t>else insert TIE into LSDB and ACKTIE = TIE</t>
                                </list>
                                else
                                <list style="numbers">
                                    <t>if DBTIE.HEADER = TIE.HEADER then
                                        <list style="format %i. ">
                                            <t>if DBTIE has content already then ACKTIE = TIE </t>
                                            <t>else process like the "DBTIE.HEADER &lt; TIE.HEADER" case</t>
                                        </list>
                                    </t>
                                    <t>if DBTIE.HEADER &lt; TIE.HEADER then
                                        <list style="format %i. ">
                                            <t>if originator is this node then bump_own_tie</t>
                                            <t>else insert TIE into LSDB and ACKTIE = TIE</t>
                                        </list>
                                    </t>
                                    <t>if DBTIE.HEADER &gt; TIE.HEADER then
                                        <list style="format %i. ">
                                            <t>if DBTIE has content already then TXTIE = DBTIE</t>
                                            <t>else ACKTIE = DBTIE </t>
                                        </list>
                                    </t>
                                </list>

                            </t>
                            <t>if TXTIE is set then try_to_transmit_tie(TXTIE) </t>
                            <t>if ACKTIE is set then ack_tie(TIE)</t>
                        </list>

                    </t>

                </section>

                <section title="TIEs Processing When LSDB Received Newer Version on Other Adjacencies">
                    <t>
                    The Link State Database can be considered to be a switchboard that does not need any
                    flooding procedures but can be given new versions of
                    TIEs by a peer. Consecutively, a peer receives from the LSDB newer versions of TIEs
                    received by other peers and processes them (without any filtering) just like
                    receiving TIEs from its remote peer. This publisher model can be implemented in
                    many ways.
                    </t>
                </section>

                <section title="Sending TIEs">
                    <t>
                        On a periodic basis all TIEs with lifetime left > 0 MUST be sent out
                        on the adjacency, removed from TIES_TX list and requeued onto TIES_RTX list.
                    </t>
                </section>
            </section>

</section>

<section title="TIE Flooding Scopes" anchor="tiescopes">


<t>In a somewhat analogous fashion to link-local, area and domain flooding scopes,
RIFT defines several complex "flooding scopes" depending on the direction and type of TIE
propagated.</t>

<t>Every North TIE is flooded northbound, providing a node at a given level with the
    complete topology of
    the Clos or Fat Tree network that is reachable southwards of it, including all specific prefixes.
    This means that a packet
    received from a node at the same or lower level whose destination is covered
    by one of those specific
    prefixes will be routed directly towards the node advertising that prefix
    rather than sending
    the packet to a node at a higher level.</t>

<t>A node's Node South TIEs, consisting of all node's adjacencies and
prefix South TIEs limited to those related to default IP prefix and disaggregated prefixes,
    are flooded southbound in order to allow
the nodes one level down to see connectivity of the higher level as well
as reachability to the rest of the fabric.  In
order to allow an E-W disconnected node in
a given level to receive the South TIEs of other nodes at its level, every *NODE*
South TIE is "reflected" northbound to level from which it was
received. It should be noted that East-West links are included in
South TIE flooding (except at ToF level);
those TIEs need to be flooded to satisfy algorithms in <xref target="calculate"/>.
In that way nodes at same level can learn about each other
without a lower level, e.g. in case of leaf level.
The precise, normative flooding scopes are given in <xref target="tie-tire-tide-scopes"/>.
Those rules govern
as well what SHOULD be included in TIDEs on the adjacency.
Again, East-West flooding scopes are
identical to South flooding scopes except in case of ToF East-West links (rings) which
    are basically performing northbound flooding.
</t>

<t>Node South TIE "south reflection" allows to support positive disaggregation on failures describes
    in <xref target="disaggregate"/> and flooding reduction in <xref target="mpr"/>.
</t>

<texttable anchor="tie-tire-tide-scopes"
    title="Normative Flooding Scopes"
    style="all">

    <ttcol>Type / Direction</ttcol> <ttcol>South</ttcol><ttcol>North</ttcol><ttcol>East-West</ttcol>

    <c>node South TIE</c>     <c>flood if level of originator is equal to this node</c>
    <c>flood if level of originator is higher than this node</c><c>flood only if this node
    is not ToF</c>

    <c>non-node South TIE</c>    <c>flood self-originated only</c>
    <c>flood only if neighbor is originator of TIE</c>
    <c>flood only if self-originated and this node is not ToF</c>

    <c>all North TIEs</c>      <c>never flood</c>       <c>flood always</c>
        <c>flood only if this node is ToF</c>

    <c>TIDE</c>           <c>include at least all non-self
                        originated North TIE headers and
                        self-originated South TIE headers and
                        node South TIEs of nodes at same
                        level</c>       <c>include at least all node South TIEs and
                                        all South TIEs originated by peer and
                                        all North TIEs</c>   <c>if this node is ToF then
                                               include all North TIEs, otherwise only self-originated
                                                TIEs</c>

    <c>TIRE as Request</c>           <c>request all North TIEs and all peer's
                            self-originated TIEs and
                            all node South TIEs</c>
                                            <c>request all South TIEs</c>
    <c>if this node is ToF then apply North scope rules, otherwise South scope rules</c>

    <c>TIRE as Ack</c><c>Ack all received TIEs</c>
                                    <c>Ack all received TIEs</c>
                                            <c>Ack all received TIEs</c>
                                            
</texttable>

    <t>
        If the TIDE includes additional TIE
        headers beside the ones specified, the receiving neighbor must apply
        according filter to the received TIDE strictly and MUST NOT request
        the extra TIE headers that were not allowed by the flooding scope rules in
        its direction.
    </t>

<t>As an example to illustrate these rules, consider using
    the topology in <xref target="pic-topo-three"/>, with the
    optional link between spine 111 and spine 112, and the
    associated TIEs given in <xref
    target="ties-topo-three"/>. The flooding from particular
    nodes of the TIEs is given in <xref
    target="flooding-topo-three"/>.</t>

<texttable anchor="flooding-topo-three"
    title="Flooding some TIEs from example topology"
    style="full">

    <ttcol>Router floods to</ttcol> <ttcol>Neighbor</ttcol><ttcol>TIEs</ttcol>

    <c>Leaf111</c> <c>Spine 112</c> <c>Leaf111 North TIEs, Spine 111 node South TIE</c>
    <c>Leaf111</c> <c>Spine 111</c> <c>Leaf111 North TIEs, Spine 112 node South TIE</c>
    <c></c>  <c></c>  <c></c>
    <c>Spine 111</c> <c>Leaf111</c> <c>Spine 111 South TIEs  </c>
    <c>Spine 111</c> <c>Leaf112</c> <c>Spine 111 South TIEs </c>
    <c>Spine 111</c> <c>Spine 112</c> <c>Spine 111 South TIEs</c>

    <c>Spine 111</c> <c>ToF 21</c> <c>Spine 111 North TIEs,
        Leaf111 North TIEs, Leaf112 North TIEs, ToF 22 node South TIE </c>

    <c>Spine 111</c> <c>ToF 22</c> <c>Spine 111 North TIEs,
        Leaf111 North TIEs, Leaf112 North TIEs, ToF 21 node South TIE </c>
    <c></c>  <c></c>  <c></c>

    <c>... </c><c>...</c><c>...</c>
    <c>ToF 21</c> <c>Spine 111</c> <c>ToF 21 South TIEs</c>
    <c>ToF 21</c> <c>Spine 112</c> <c>ToF 21 South TIEs</c>
    <c>ToF 21</c> <c>Spine 121</c> <c>ToF 21 South TIEs</c>
    <c>ToF 21</c> <c>Spine 122</c> <c>ToF 21 South TIEs</c>

    <c>... </c><c>...</c><c>...</c>

</texttable>


    <!--

     

                                    <t>Flooding northbound floods all TIEs EXCEPT
                                        the South TIEs of nodes at
                                        the same or lower
                                        levels.

                                        Flooding North TIEs from lower levels
                                        provides all necessary
                                        information to the nodes at higher
                                        levels. Flooding South TIEs from the
                                        higher level (based on those rules
                                        it will only the next higher one)
                                        allows a disconnected spine to see the
                                        South TIEs of other members of its level
                                        given the level below it will reflect
                                        its South TIEs. Flooding
                                        East-West TIEs from the same level is
                                        necessary in case the
                                        upper level is disconnected from certain
                                        nodes in a level.

                                        leaves do not need to follow this rule
                                        and can freely flood
                                        TIEs of other leaves northbound.</t>

                                    <t >
                                        Southbound links are where the really
                                        interesting changes
                                        happen since here the link-state becomes
                                        de-facto a
                                        "one-hop distance vector" protocol. A
                                        spine node starts to
                                        send on this link different TIEs than
                                        it uses on
                                        north-
                                        or Eastbound links, namely its South TIEs.
                                        They form an independent
                                        database that represents ONLY the
                                        node's neighbors
                                        and a default IP prefix. Node's South TIEs
                                        MUST NEVER be flooded
                                        northbound and MUST be simply dropped
                                        on reception on a
                                        southbound link if they do not come
                                        from the node's own
                                        level, i.e. have been reflected by a
                                        lower level.
                                    </t>


     -->

                        </section>


                    <section title="'Flood Only Node TIEs' Bit" anchor="onlynodeties">
                        <t>
                            RIFT includes an optional ECN mechanism to prevent "flooding inrush"
                            on restart or bring-up with many southbound neighbors.
                            A node MAY set on its
                            LIEs the according bit to indicate to the neighbor that it should
                            temporarily flood node TIEs only to it. It SHOULD only set it
                            in the southbound direction. The receiving node SHOULD
                            accommodate the request to lessen the flooding load on the affected
                            node if south of the sender and SHOULD ignore the bit if
                            northbound.
                            </t>

                        <t>Obviously this mechanism is most useful in southbound
                            direction. The distribution of node TIEs guarantees correct
                            behavior of algorithms like disaggregation or default route
                            origination.
                            Furthermore though, the use of this bit presents an inherent trade-off
                            between processing load and convergence speed since suppressing
                            flooding of northbound prefixes from neighbors will lead to
                            blackholes.
                        </t>
                    </section>

                        <section
                            title="Initial and Periodic Database Synchronization">
                            <t>The initial exchange of RIFT is modeled after
                                ISIS with TIDE being equivalent to CSNP and
                                TIRE playing the role of PSNP. The content of
                                TIDEs and TIREs is governed
                                by <xref target="tie-tire-tide-scopes"/>.
                                </t>


                            </section>

                        <section
                            title="Purging and Roll-Overs">

<t>
    When a node exits the network, if "unpurged", residual stale TIEs may exist in the network until their lifetimes expire
    (which in case of RIFT is by default a rather long period
    to prevent ongoing re-origination of TIEs in very large topologies).
    RIFT does however not have a "purging mechanism" in the traditional sense based on
    sending specialized "purge" packets.
    In other routing protocols such mechanism has proven
 to be complex and fragile based on many years of experience. RIFT simply issues a new, empty version of the TIE
    with a short
    lifetime and relies on each node to age out and delete such TIE copy independently.

                            Abundant amounts of memory are
                                available
                                today even on low-end platforms and hence keeping those relatively short-lived
                                extra copies for a while
                                is acceptable. The
                                information will age out and in the meantime all computations
                                will deliver correct results if a node
                                leaves the network due
                                to the new information distributed by its adjacent
                                nodes breaking bi-directional connectivity checks in
                                different computations.
                                </t>
                            <t>Once a RIFT node issues a TIE with an ID, it SHOULD
                                preserve the ID as long as feasible (also when
                                the protocol restarts), even if the TIE
                                looses
                                all content. The re-advertisement of empty TIE
                                fulfills the purpose of purging any information
                                advertised in previous versions. The originator
                                is free to not re-originate the according empty TIE
                                again or originate an empty TIE with relatively
                                short lifetime to prevent large number of long-lived
                                empty
                                stubs polluting the network.
                                Each node
                                MUST timeout and clean up the according empty TIEs
                                independently.
                                </t>
                            <t>Upon restart a node MUST, as any link-state
                                implementation, be prepared to receive
                                TIEs with its own system ID and supersede them
                                with equivalent, newly generated, empty TIEs with
                                a higher sequence number. As above, the lifetime
                                can be relatively short since it only needs to
                                exceed the necessary propagation and processing
                                delay by all the nodes that are within the
                                TIE's flooding scope.
                                </t>
                            <t>TIE sequence numbers are rolled over using the method
                                described in
                                <xref target="arithmetic"/>.
                                First sequence number of any spontaneously
                                 originated TIE (i.e. not originated
                                to override a detected older copy in
                                the network) MUST be
                                a reasonably unpredictable random number in the
                                interval [0, 2^10-1] which will prevent otherwise
                                identical TIE headers to remain "stuck" in the network
                                with content different from TIE originated after reboot.</t>
                            </section>

<section title="Southbound Default Route Origination" anchor="defaultrouterules">

    <t>Under certain conditions nodes issue a default route in their South Prefix TIEs with
        costs as computed in <xref target="varydefault"/>.</t>

    <t>A node X that
        <list style='numbers' >

       <t> is NOT overloaded AND</t>
       <t>has southbound or East-West adjacencies</t>
       </list>


        originates in its south prefix TIE such a default
        route IIF

 <list style='numbers' >
        <t>all other nodes at X's' level are overloaded OR</t>
        <t>all other nodes at X's' level have NO northbound
                adjacencies
                OR</t>
        <t>X has computed reachability to a default
                route during N-SPF.</t>
</list>

        </t>

    <t>The term "all other nodes at X's' level" describes obviously
        just the nodes at the same level in the PoD with a viable lower level
        (otherwise the node South TIEs cannot be reflected and the nodes in e.g.
        PoD 1 and PoD 2 are "invisible" to each other).
</t>
    
    <t>A node originating a southbound
        default route MUST install a default discard route
        if it did not compute a default route during N-SPF.
    </t>



</section>






    <section
       title="Northbound TIE Flooding Reduction" anchor="mpr">
    <t>
    Section 1.4 of the <xref target="RFC3626">Optimized Link State Routing 
    Protocol</xref> (OLSR) introduces the concept of a "multipoint relay" (MPR)
    that minimize the overhead of flooding messages in the network by reducing
    redundant retransmissions in the same region.
    </t>
    <t>
    A similar technique is applied to RIFT to control northbound flooding.
    Important observations first:
</t>

<t>
<list style="numbers">
    <t>a node MUST flood self-originated North TIEs to all the reachable nodes at
    the level above which we call the node's "parents";
    </t>
    <t>
    it is typically not necessary that all parents reflood the North TIEs to achieve
    a complete flooding of all the reachable nodes two levels above which we
    choose to call the node's "grandparents"; 
    </t>
    <t>
    to control the volume of its flooding two hops North and yet keep it robust
    enough, it is advantageous for a node to select a subset of its parents as
    "Flood Repeaters" (FRs), which combined together deliver two or more copies
    of its flooding to all of its parents, i.e. the originating node's
    grandparents;
    </t>
    <t>
        nodes at the same level do NOT have to agree on
        a specific algorithm to select the FRs,
        but overall load balancing should be achieved so that different
        nodes at the same level should tend to select different parents as FRs;
    </t>
    <t>
    there are usually many solutions to the problem of finding a set of FRs for
    a given node; the problem of finding the minimal set is (similar to) a
    NP-Complete problem and a globally optimal set may not be the minimal one if
    load-balancing with other nodes is an important consideration;
    </t>

    <t>
    it is expected that there will be often sets of equivalent nodes at a
    level L, defined as having a common set of parents at L+1.
    Applying this observation at both L and L+1, an algorithm may attempt to
    split the larger problem in a sum of smaller separate problems;
    </t>
    <t>
    it is another expectation that there will be from time to time a broken
    link between a parent and a grandparent, and in that case the parent is
    probably a poor FR due to its lower reliability.
    An algorithm may attempt to eliminate parents with
    broken northbound adjacencies first in order to reduce the number of FRs.
    Albeit it could be argued that relying on higher fanout FRs will slow
    flooding due to higher replication load reliability of FR's links seems to be
    a more pressing concern.
    </t>
    </list>
    </t>
    <t>
    In a fully connected Clos Network, this means that a node selects one
    arbitrary parent as FR and then a second one for redundancy. The computation
    can be kept relatively simple and completely distributed without any need
    for synchronization amongst nodes. In a "PoD" structure, where the Level L+2
    is partitioned in silos of equivalent grandparents that are only reachable
    from respective parents, this means treating each silo as a fully connected
    Clos Network and solve the problem within the silo.
    </t>
    <t>
    In terms of signaling, a node has enough information to select its set of
    FRs; this information is derived from the node's parents' Node South TIEs, which
    indicate the parent's reachable northbound adjacencies to its own parents,
    i.e. the node's grandparents.
    A node may send a LIE to a northbound neighbor with the optional boolean field
    `you_are_flood_repeater` set to false, to indicate that the northbound neighbor is
    not a flood repeater for the node that sent the LIE. In that case the northbound 
    neighbor SHOULD NOT reflood northbound TIEs received from the node that sent the LIE.
    If the `you_are_flood_repeater` is absent or if `you_are_flood_repeater` is set to true,
    then the northbound neighbor is a flood repeater for the node that sent the LIE and MUST
    reflood northbound TIEs received from that node.
     </t>
    <t>This specification proposes a simple default algorithm that SHOULD be
    implemented and used by default on every RIFT node.
        <list style="symbols">
        <t>let |NA(Node) be the set of Northbound adjacencies of node Node and CN(Node) be the cardinality of |NA(Node);</t>
        <t>let |SA(Node) be the set of Southbound adjacencies of node Node and CS(Node) be the cardinality of |SA(Node);</t>
        <t>let |P(Node) be the set of node Node's parents; </t>
        <t>let |G(Node) be the set of node Node's grandparents. Observe that |G(Node) = |P(|P(Node));</t>
        <t>let N be the child node at level L computing a set of FR;</t>
        <t>let P be a node at level L+1 and a parent node of N, i.e. bi-directionally reachable over adjacency A(N, P);</t>
        <t>let G be a grandparent node of N, reachable transitively via a parent P over adjacencies ADJ(N, P) and ADJ(P, G). Observe
            that N does not have enough information to check bidirectional reachability of A(P, G);</t>
        <t>let R be a redundancy constant integer; a value of 2 or higher for R is RECOMMENDED;</t>
        <t>let S be a similarity constant integer; a value in range 0 .. 2 for S is RECOMMENDED, the value of 1 SHOULD be used.
            Two cardinalities are considered as equivalent if their absolute difference is less than or equal to S, i.e.
            <!-- a ~ b => | a div (S+1) | == | b div (S+1) |. -->
            |a-b|&le;S.
            </t>
        <t>let RND be a 64-bit random number generated by the system once on startup.</t>
        </list>
    </t>
    
    <t>    
    The algorithm consists of the following steps:
    <list style="numbers">
    <t>Derive a 64-bits number by XOR'ing 'N's system ID with RND.
    </t>
    <t>
    Derive a 16-bits pseudo-random unsigned integer PR(N) from the resulting 64-bits
    number
    by splitting it in 16-bits-long words W1, W2, W3, W4 (where W1 are the least significant 16 
    bits of the 64-bits number, and W4 are the most significant 16 bits) and then XOR'ing the
    circularly shifted resulting words together:
    <list style="letters">
        <t>
    (W1&lt;&lt;1) xor (W2&lt;&lt;2) xor (W3&lt;&lt;3) xor (W4&lt;&lt;4);
            <vspace/><vspace/>where &lt;&lt; is the circular shift operator.</t>
    </list></t>
    <t>
    Sort the parents by decreasing number of northbound adjacencies
    (using decreasing system id of the parent as tie-breaker):
    sort |P(N) by decreasing CN(P), for all P in |P(N), as ordered array |A(N)
    </t>
    <t>
    Partition |A(N) in subarrays |A_k(N) of parents with equivalent cardinality
    of northbound adjacencies (in other words with equivalent number of
    grandparents they can reach):

    <list style="letters">
        <t>set k=0; // k is the ID of the subarrray</t>
        <t>set i=0; </t>
        <t>while i &lt; CN(N) do
            <list style="format %i)">
                <t>set j=i;  </t>

                <t>while i &lt; CN(N) and CN(|A(N)[j]) - CN(|A(N)[i]) &le; S
                    <list style="format %c.">
                        <t>place |A(N)[i] in |A_k(N) // abstract action, maybe noop </t>
                        <t>set i=i+1;</t>
                    </list>
                </t>
                <t>/* At this point j is the index in |A(N) of the first member of
                |A_k(N) and (i-j) is C_k(N) defined as the cardinality of |A_k(N) */
                    <vspace/><vspace/>
                    set k=k+1;
                </t>
            </list>
        </t>
    </list>

     <vspace/>
        <vspace/>
    /* At this point k is the total number of subarrays, initialized for the
    shuffling operation below */
    </t>
    <t>
    shuffle individually each subarrays |A_k(N) of cardinality C_k(N) within
    |A(N) using the Durstenfeld variation of Fisher-Yates algorithm that depends on N's System ID: 
    <list style="letters">
        <t>while k &gt; 0 do
            <list style="format %i)">
                <t>for i from C_k(N)-1 to 1 decrementing by 1 do
                    <list style="format %c.">
                        <t>set j to PR(N) modulo i;</t>
                        <t>exchange |A_k[j] and |A_k[i];</t>
                    </list>
                </t>
                <t>set k=k-1;</t>
            </list>
        </t>
    </list>
    </t>
    <t>
    For each grandparent G, initialize a counter c(G) with the number of its south-bound adjacencies
    to elected flood repeaters (which is initially zero):
    <list style="letters">
    <t>for each G in |G(N) set c(G) = 0;</t>
    </list></t>
    <t>
    Finally keep as FRs only parents that are needed to maintain the number of
    adjacencies between the FRs and any grandparent G equal or above the
    redundancy constant R:
    <list style="letters">
        <t>for each P in reshuffled |A(N);
            <list style="format %i)">
                <t>if there exists an adjacency ADJ(P, G) in |NA(P) such that c(G) &lt; R then
                    <list style="format %c.">
                        <t>place P in FR set;</t>
                        <t>for all adjacencies ADJ(P, G') in |NA(P) increment c(G')</t>
                    </list>
                </t>
            </list>
        </t>
        <t>If any c(G) is still &lt; R, it was not possible to elect
           a set of FRs that covers all grandparents with redundancy R</t>
    </list>
    </t>
    </list>
    </t>

    <t>Additional rules for flooding reduction:
        <list style="numbers">

            <t>The algorithm MUST be re-evaluated by a node on every change of local
                adjacencies or reception of a parent South TIE with changed adjacencies.
                A node MAY apply a hysteresis to prevent excessive amount of
                computation during periods of network instability just like in case
                of reachability computation.</t>

            <t>
                Upon a change of the flood repeater set, a node SHOULD send out LIEs
                that grant flood repeater status to newly promoted nodes before it sends LIEs
                that revoke the status to the nodes that have been newly demoted.
                This is done to prevent transient behavior where the full coverage of
                grandparents is not guaranteed. Such a condition is sometimes unavoidable
                in case of lost LIEs but it will
                correct itself though at possible transient hit in flooding propagation speeds.
            </t>

            <t>A node MUST always flood its self-originated TIEs.
            </t>

            <t>A node receiving a TIE originated by
                a node for which it is not a flood
                repeater SHOULD NOT reflood such TIEs to its neighbors
                except for rules in <xref target="rule2"/>.
            </t>
            <t>The indication of flood reduction capability
                MUST be carried in the node TIEs and MAY be used to optimize the
                algorithm to account for nodes that will flood regardless.
            </t>
            <t anchor="rule2">A node generates TIDEs as usual but when
                receiving TIREs or TIDEs resulting in requests for a
                TIE of which the newest received copy came on
                an adjacency where the node was not flood repeater it
                SHOULD ignore such requests on first and only first
                request. Normally, the nodes that received
                the TIEs as flooding repeaters should satisfy the requesting node
                and with that no
                further TIREs for such TIEs will be
                generated. Otherwise, the next set
                of TIDEs and TIREs MUST lead to flooding
                independent of the
                flood repeater status. This solves a very difficult
                incast problem on nodes restarting with a very wide fanout, especially
                northbound. To retrieve the full database they often
                end up processing many in-rushing copies whereas this
                approach load-balances the incoming database
                between adjacent nodes and flood repeaters should
                guarantee that two copies are sent by different nodes
                to ensure against any losses.
            </t>

            <!-- belongs into PGP draft

            <t>Obviously flooding reduction does NOT apply to
                self originated TIEs and since
                all policy-guided information consists of
                self-originated TIEs those are unaffected.
            </t>

            -->

        </list>
    </t>

 </section>

    <section title="Special Considerations">

        <t>
            First, due to the distributed, asynchronous nature of
            ZTP, it can create temporary convergence anomalies where nodes at higher levels
            of the fabric temporarily see themselves lower than they belong. Since flooding
            can begin before ZTP is "finished" and in fact must do so given there is no global
            termination criteria, information may end up in wrong layers. A special clause
            when changing level takes care of that.
        </t>

        <t>
            More difficult is a condition where a node (e.g. a leaf) floods a TIE north towards its
            grandparent, then its parent reboots, in fact partitioning the grandparent from
            leaf directly and then the leaf itself reboots. That can leave the
            grandparent holding the "primary copy" of the leaf's TIE. Normally this condition
            is resolved easily by the leaf re-originating its TIE with a higher sequence
            number than it sees in northbound TIEs, here however, when the parent comes back it won't
            be able to obtain leaf's North TIE from the grandparent easily and with that the leaf
            may not issue the TIE with a higher sequence number that can reach the granparent for a long
            time. Flooding
            procedures are extended to deal with the problem by the means of special clauses
            that override the database of a lower level with headers of newer TIEs seen in
            TIDEs coming from the north.
        </t>

    </section>

</section> <!-- flooding -->


<section title="Reachability Computation" anchor="calculate">

    <t>A node has three possible sources of relevant information for reachability computation.
        A node knows
        the full topology south of it from the received North Node TIEs or alternately
        north of it from the South Node TIEs.  A node has the
        set of prefixes with their associated distances and bandwidths from
        corresponding prefix TIEs.</t>

    <t>To compute prefix reachability, a node runs conceptually a northbound
        and a southbound
        SPF.
        We call that N-SPF and S-SPF denoting the direction in which the computation
        front is progressing.
    </t>

    <t>Since neither computation can "loop", it is
        possible to compute non-equal-cost or even
        <xref target="EPPSTEIN">k-shortest paths</xref>
        and "saturate" the fabric
        to the extent desired but we use simple, familiar SPF algorithms and
        concepts here as example due to their prevalence in today's routing.
    </t>

    <section anchor="nspf" title="Northbound SPF">

        <t> N-SPF MUST use exclusively northbound and East-West adjacencies in the computing
            node's node North TIEs (since if the node is a leaf it may not have
            generated a node South TIE)
            when starting SPF. Observe that N-SPF is really just
            a one hop variety since Node South TIEs are not re-flooded southbound
            beyond
            a single level (or East-West) and
            with that the computation cannot progress beyond adjacent nodes.
            </t>

<t>Once progressing, we are using the next higher level's node South TIEs to
    find according adjacencies to verify backlink connectivity.
    Just as in case of IS-IS or OSPF, two unidirectional links MUST be
    associated
    together to confirm bidirectional connectivity. Particular care
    MUST be paid that the Node TIEs
    do not only contain the correct system IDs but matching levels as well.
</t>


        <t>Default route found when crossing an E-W link SHOULD be used IIF

<list style="numbers">
             <t>the node itself does NOT have any northbound adjacencies AND</t>
            <t>the adjacent node has one or more northbound adjacencies</t>
            </list>

            This rule forms
            a "one-hop default route split-horizon" and prevents looping
            over default routes
            while allowing for "one-hop protection" of nodes that lost
            all northbound
            adjacencies except at Top-of-Fabric where the links are used
            exclusively to flood topology information in multi-plane designs.

</t>
        <t>Other south prefixes found when crossing E-W link MAY be used IIF
            <list style="numbers">

            <t>no
            north neighbors are advertising same or supersuming non-default
                prefix AND </t>
            <t>the node does not originate a non-default supersuming prefix
                itself.</t>


            </list>


    i.e. the
    E-W link can be used as a gateway of last resort for a specific prefix only.
    Using south prefixes across E-W link can be beneficial e.g.
    on automatic de-aggregation
    in pathological fabric partitioning scenarios.

            </t>


        <t>
            A detailed example can be found in <xref target="onastickexample"/>.

        </t>


    </section>


    <section anchor="sspf" title="Southbound SPF">

        <t> S-SPF MUST use exclusively the
            southbound adjacencies in the node South TIEs,
            i.e. progresses towards nodes at lower levels. Observe that
            E-W adjacencies are NEVER used in the computation. This enforces the
            requirement that a packet traversing in a southbound direction must
            never change its direction.</t>
        <t>S-SPF MUST use northbound adjacencies in node North TIEs to verify backlink
            connectivity by checking for presence of the link beside correct SystemID and
            level. </t>

    </section>

    <section anchor="ringspf" title="East-West Forwarding Within a non-ToF Level">

        <t>Using south prefixes over horizontal links MAY occur
            if the N-SPF includes East-West adjacencies in computation.
            It can
            protect against pathological fabric partitioning cases that
            leave only paths to destinations that would necessitate multiple
            changes of forwarding direction between north and south.
            </t>
    </section>

        <section anchor="tofew" title="East-West Links Within ToF Level">

            <t>E-W ToF links behave in terms of flooding scopes defined in
                <xref target="tiescopes"/> like northbound links and MUST be used exclusively for control plane
                information flooding. Even though a ToF node could be tempted
                to use those links during southbound SPF and carry traffic over them this
                MUST NOT be attempted since it may lead in, e.g. anycast cases to routing loops.
                An implementation MAY try to resolve the looping problem by following on the ring strictly
                tie-broken
                shortest-paths only but the details are outside this specification. And even then,
                the problem of proper capacity provisioning of such links when they become traffic-bearing in
                case of failures is vexing.</t>
    </section>

</section>

<section title=	"Automatic Disaggregation on Link &amp; Node Failures"
    anchor="disaggregate">

<section title=	"Positive, Non-transitive Disaggregation" anchor="posdisaggreg">

    <t>Under normal circumstances, node's South TIEs contain
        just the adjacencies and a default route.

        However, if a node detects that its default IP
        prefix covers one or more prefixes that are reachable
        through it but not through one or
        more other nodes at the same level, then it MUST
        explicitly advertise those prefixes in an
        South TIE.  Otherwise, some percentage of the northbound
        traffic for those prefixes would
        be sent to nodes without according reachability,
        causing it to be black-holed.
        Even when not black-holing, the resulting forwarding
        could
        'backhaul' packets through the higher level spines,
        clearly an undesirable condition affecting
        the blocking probabilities of the fabric.

    </t>
    <t>We refer to the process of advertising additional prefixes southbound
        as 'positive de-aggregation' or 'positive dis-aggregation'. Such dis-aggregation
        is non-transitive, i.e. its' effects are always contained to a single level of
        the fabric only. Naturally, multiple node or link failures can lead to several
        independent instances of positive dis-aggregation necessary to prevent
        looping or bow-tying the fabric.
    </t>

    <t>
        A node determines the set of prefixes needing de-aggregation
        using the following steps:

        <list style="numbers">

            <t>A DAG computation in the southern
                direction is performed first, i.e. the
                North TIEs are used to find all of prefixes
                it can reach and the set of next-hops in
                the lower level
                for each of them.
                Such a computation can be
                easily performed on a fat tree by
                e.g. setting all link costs in the
                southern direction to 1 and all
                northern directions to infinity.  We
                term set of those prefixes |R, and for each prefix, r,
                in |R, we define
                its set of next-hops to be |H(r).
            </t>

            <t> The node uses reflected South TIEs to find all nodes
                at the same level in the same PoD and the set of southbound
                adjacencies
                for each.  The set of nodes at the same level is termed |N and for each
                node, n, in |N, we define
                its set of southbound adjacencies to be |A(n).
            </t>

            <t>For a given r, if the intersection
                of |H(r) and |A(n), for any n, is null
                then that prefix r must be
                explicitly advertised by the node
                in an South TIE.

                <!-- The set of reachable prefixes
                 advertised in North TIEs for which the set
                 of possible next-hops is disjoint with
                 any of the sets of adjacencies reachable by
                 the other nodes are the disaggregated
                 prefixes.  More formally, the set
                 consists of all r in |R such that |H
                 of r is disjoint to |A for any N. -->
            </t>

            <t>Identical set of de-aggregated prefixes is flooded on each of the
                node's southbound
                adjacencies.  In accordance with the normal flooding rules for an South TIE,
                a node at the lower level that
                receives this South TIE SHOULD NOT propagate it south-bound or
                reflect the disaggregated prefixes
                back over its adjacencies to nodes at the level from which
                it was received.
            </t>

        </list>

    </t>

    <t>To summarize the above in simplest terms: if a node detects that its
        default route encompasses
        prefixes for which one of the other nodes in its level has no
        possible next-hops in the level below,
        it has to disaggregate it to prevent black-holing or suboptimal
        routing through such nodes. Hence
        a node X needs to determine if it can
        reach a different set of south neighbors than other nodes at the
        same level, which are connected to it via at least one common
        south
        neighbor.  If it can, then prefix disaggregation may be required.
        If it can't, then no prefix disaggregation is needed.
        An example of disaggregation is provided in
        <xref target="fabriccut"/>.
    </t>

    <t>A possible
        algorithm is described last:</t>
    <t>
        <list style="numbers">
            <t>Create partial_neighbors = (empty), a set of neighbors with
                partial connectivity to the node X's level from X's perspective.
                Each entry in the set is a south neighbor of X and a list of nodes
                of X.level that can't reach that neighbor.</t>

            <t>A node X determines its set of southbound neighbors
                X.south_neighbors.</t>

            <t>For each South TIE originated from a node Y that X has which is
                at X.level, if Y.south_neighbors is not the same as
                X.south_neighbors but the nodes share at least one
                southern neighbor, for each neighbor N in X.south_neighbors but
                not in Y.south_neighbors, add (N, (Y)) to partial_neighbors if N
                isn't there or add Y to the list for N.</t>

            <t>If partial_neighbors is empty, then node X does not
                disaggregate any prefixes.  If node X is advertising disaggregated
                prefixes in its South TIE, X SHOULD remove them and re-advertise its
                according
                South TIEs.</t>
        </list></t>

    <t>A node X computes reachability to all nodes below it
        based upon the received North TIEs first.  This
        results in a set of routes, each categorized by (prefix,
        path_distance, next-hop-set).  Alternately, for clarity in the
        following procedure, these can be organized by next-hop-set as (
        (next-hops), {(prefix, path_distance)}).  If partial_neighbors isn't
        empty, then the following procedure describes how to identify
        prefixes to disaggregate.</t>


    <!--
     <t>It is worth to observe here that
     this procedure only disaggregates prefixes when there
     is a same-level node with no connectivity to any of the next-hop south
     neighbors.  This obviously ignores concerns about load-balancing; one
     could also decide to advertise a disaggregated prefixes whenever a
     same-level node lacks connectivity to at least one next-hop.  To do
     that, the algorithm would have to advertise the aggregate link bandwidth across
     all of a node's next-hops.  Then the receiving node could accumulate
     the disaggregated prefixes and merge those with the same path_distance
     but do load-balancing among its next-hops based upon the bandwidth
     indicated.  This has a trade-off of adding more flooding - prefixes
     would be disaggregated based on a single failure instead of when
     connectivity is lost - but should give better load-balancing.  Of
     course, instead of aggregate link bandwidth, one could use link count,
     assuming all links in the fabric have the same bandwidth.</t>

     -->

    <figure align="center" anchor="algo-disaggregated-prefixes"
        title="Computation of Disaggregated Prefixes">
        <artwork align="left"><![CDATA[

            disaggregated_prefixes = { empty }
            nodes_same_level = { empty }
            for each South TIE
              if (South TIE.level == X.level and
                  X shares at least one S-neighbor with X)
                add South TIE.originator to nodes_same_level
                end if
              end for

            for each next-hop-set NHS
              isolated_nodes = nodes_same_level
              for each NH in NHS
                if NH in partial_neighbors
                  isolated_nodes =
                    intersection(isolated_nodes,
                                 partial_neighbors[NH].nodes)
                  end if
                end for

              if isolated_nodes is not empty
                for each prefix using NHS
                  add (prefix, distance) to disaggregated_prefixes
                  end for
                end if
              end for

            copy disaggregated_prefixes to X's South TIE
            if X's South TIE is different
              schedule South TIE for flooding
              end if
        ]]>
        </artwork>
    </figure>


    <t>Each disaggregated prefix is sent with the according path_distance.
        This allows a node to send the same South TIE to each south neighbor.
        The south neighbor which is connected to that prefix will thus have a
        shorter path.</t>


    <t>Finally, to summarize the less obvious points partially omitted in the
        algorithms to keep them more tractable:
        <list style="numbers">
            <t>all neighbor relationships MUST perform backlink checks.
                </t>
            <t>overload bits
            as introduced in <xref target="overload"/> have to
            be respected during the computation.
            </t>

            <t>all the lower level nodes are flooded the same disaggregated
                prefixes since we don't want to build an South TIE per node and
                complicate things unnecessarily. The lower level node
                that can compute a southbound route to the prefix
                will prefer it to the disaggregated route anyway based on
                route preference rules.</t>
            <t>positively disaggregated prefixes
                do NOT have to propagate to lower levels. With that the
                disturbance in terms of new flooding is contained to a single
                level experiencing failures.</t>
            <t>disaggregated Prefix South TIEs are not "reflected" by the
                lower level, i.e.
                nodes within same level do NOT need to be aware which node
                computed the need for disaggregation.
            </t>
            <t> The fabric is still
                supporting maximum load balancing properties while not trying
                to send traffic northbound unless
                necessary. </t>
        </list>
    </t>

    <t>In case positive disaggregation is triggered and due to the very stable but
        un-synchronized nature of the algorithm the nodes may issue the necessary
        disaggregated
        prefixes at different points in time. This can lead for a short
        time to an "incast" behavior where the first advertising router based on the
        nature of longest prefix match will attract all the traffic. An implementation
        MAY hence choose different strategies to address this behavior if needed.

    </t>

    <t>
        To close this section it is worth to observe that in a single plane ToF
        this disaggregation prevents blackholing up to (K_LEAF * P) link failures in terms of
        <xref target="Planes"/> or in other terms, it takes at minimum that many
        link failures to partition the ToF into multiple planes.
    </t>

</section> <!-- positive disaggregation -->


<section title=	"Negative, Transitive Disaggregation for Fallen Leaves" anchor="negdisaggreg">
    <t>As explained in <xref target="Fallen"/> failures in multi-plane Top-of-Fabric or
        more than (K_LEAF * P) links failing in single plane design can generate fallen leaves.
        Such scenario cannot be addressed by positive disaggregation
        only and needs a further mechanism.
        </t>

<section title="Cabling of Multiple Top-of-Fabric Planes">


    <t>Let us return in this section to designs with multiple planes as shown in
    <xref target="partitioned-spine"/>. <xref target="partition-one-area-cabling"/> highlights
    how the ToF is cabled in case of two planes
        by the means of dual-rings to distribute all the North TIEs within both planes.
        For people familiar with traditional link-state routing protocols ToF level
        can be considered equivalent to area 0 in OSPF or level-2 in ISIS which need to
        be "connected" as well for the protocol to operate correctly.

    </t>



<t>
    <figure align="center" anchor="partition-one-area-cabling"
        title="Topologically connected planes">
        <artwork align="center"><![CDATA[
.     ++==========++          ++==========++
.     II          II          II          II
.+----++--+  +----++--+  +----++--+  +----++--+
.|ToF   A1|  |ToF   B1|  |ToF   B2|  |ToF   A2|
.++-+-++--+  ++-+-++--+  ++-+-++--+  ++-+-++--+
. | | II      | | II      | | II      | | II
. | | ++==========++      | | ++==========++
. | |         | |         | |         | |
.
. ~~~ Highlighted ToF of the previous multi-plane figure ~~
        ]]>
        </artwork>
    </figure>

</t>


<!-- describe the negative disaggregation computation [i.e. all nodes 
    not on SPF & having some kind of southbound from other nodes in TDB ] -->
    <t>
    As described in <xref target="Fallen"/> failures in multi-plane fabrics
        can lead to blackholes which normal positive disaggregation cannot fix.
    The mechanism of negative, transitive disaggregation incorporated in RIFT
        provides the according solution.
    </t>


</section>




    <section title="Transitive Advertisement of Negative Disaggregates">

    <!-- describe the negative disaggregation transitive behavior, i.e.
        suppression until one northbound provided a negative -->
    <t>
    A ToF node that discovers that it cannot reach a fallen leaf disaggregates
    all the prefixes of such leaves.

        It uses for that purpose negative prefix South TIEs that are, as usual,
        flooded southwards with the
        scope defined in <xref target="tiescopes"/>.

    </t><t>
    Transitively, a node explicitly loses connectivity to a prefix when
        none of its children advertises it
    and when the prefix is negatively disaggregated by all of its parents. When
    that happens, the node originates the negative prefix further down south.
        Since the mechanism applies recursively south
    the negative prefix may propagate transitively all the way down to the leaf. This is necessary
        since leaves connected to multiple planes by means of disjoint paths may have to choose
        the correct plane already at the very bottom of the fabric to make sure that they
        don't send traffic towards another leaf using a plane where it is "fallen" at which in
        point a blackhole is unavoidable.
    </t><t>
    When the connectivity is restored, a node that disaggregated a prefix
    withdraws the negative disaggregation by the usual mechanism of
        re-advertising TIEs omitting the negative prefix.
    </t> 

</section>

    <section title="Computation of Negative Disaggregates">

        <t>
          The document omitted so far the description of the computation necessary to generate
            the correct set of negative prefixes. Negative prefixes can in fact be advertised due
            to two different triggers. We describe them consecutively.


        </t>

        <t>The first origination reason is a computation that uses all the node North TIEs to build
        the set of all reachable nodes by reachability computation over the complete graph
            and including ToF links. The
        computation uses the node itself as root. This is compared with the result of the normal
            southbound SPF as described in <xref target="sspf"/>. The difference are the fallen
            leaves and all their attached prefixes are advertised as negative prefixes southbound
            if the node does not see the prefix being reachable within southbound SPF.
        </t>

        <t>
            The second mechanism hinges on the understanding how the negative prefixes are used
            within the computation as described in <xref target="algo-attach-s-tie-prefixes"/>.
            When attaching the negative prefixes at certain point in time the negative prefix
            may find itself with all the viable nodes from the shorter match nexthop being
            pruned. In other words, all its northbound neighbors provided a negative prefix
            advertisement. This is the trigger to advertise this negative prefix transitively
            south and normally caused by the node being in a plane where the prefix
            belongs to a fabric leaf that has "fallen" in this plane. Obviously, when one of
            the northbound switches withdraws its negative advertisement, the node has to
            withdraw its transitively provided negative prefix as well.
        </t>

    </section>

</section>

</section> <!-- disaggregation -->




<section anchor="sec_attaching_prefixes" title="Attaching Prefixes">

    <t>After SPF is run, it is necessary to attach the resulting reachability information
        in form of prefixes.
        For S-SPF, prefixes from an North TIE are attached to the originating node with
        that node's next-hop set and a distance equal to the prefix's cost
        plus the node's minimized path distance.  The RIFT route database, a
        set of (prefix, prefix-type, attributes, path_distance, next-hop set), accumulates
        these results.</t>

    <t>In case of N-SPF prefixes from each South TIE need to also be added to the RIFT
    route database.  The N-SPF is really just a stub so the
    computing node needs simply to determine, for each prefix in an South TIE
    that originated from adjacent node, what next-hops to use to reach
    that node.  Since there may be parallel links, the next-hops to
    use can be a set; presence of the computing node in the associated
    Node South TIE is sufficient to verify that at least one link has
    bidirectional connectivity.  The set of minimum cost next-hops
    from the computing node X to the originating adjacent node is determined. </t>

    <t>Each prefix has its cost adjusted before being added into the
    RIFT route database.  The cost of the prefix is set to the cost
    received plus the cost of the minimum distance next-hop to that
    neighbor while taking into account its attributes such as mobility
    per <xref target="mobility"/>.  Then each prefix can be added into the RIFT route
    database with the next_hop_set; ties are broken based upon
    type first and then distance and further on `PrefixAttributes` and only the best
        combination is used for forwarding.
    RIFT route preferences are normalized
    by the according <xref target="thrift">Thrift</xref> model type.</t>

    <t>An example implementation for node X follows:

     <figure align="center" anchor="algo-attach-s-tie-prefixes"
            title="Adding Routes from South TIE Positive and Negative Prefixes">
      <artwork align="left"><![CDATA[

  for each South TIE
     if South TIE.level > X.level
        next_hop_set = set of minimum cost links to the
            South TIE.originator
        next_hop_cost = minimum cost link to
            South TIE.originator
        end if
     for each prefix P in the South TIE
        P.cost = P.cost + next_hop_cost
        if P not in route_database:
          add (P, P.cost, P.type,
               P.attributes, next_hop_set) to route_database
          end if
        if (P in route_database):
          if route_database[P].cost > P.cost or
                route_database[P].type > P.type:
            update route_database[P] with (P, P.type, P.cost,
                                           P.attributes,
                                           next_hop_set)
          else if route_database[P].cost == P.cost and
                route_database[P].type == P.type:
            update route_database[P] with (P, P.type,
                                           P.cost, P.attributes,
               merge(next_hop_set, route_database[P].next_hop_set))
          else
            // Not preferred route so ignore
            end if
          end if
        end for
     end for
 ]]>
     </artwork>
    </figure>

    </t>

    <t>
        After the positive prefixes are attached and tie-broken, negative prefixes are attached and
        used in case of northbound computation, ideally from the shortest length to the longest. 
        The nexthop adjacencies for a negative prefix are inherited from the longest positive prefix that
        aggregates it, and subsequently adjacencies to nodes that advertised negative for
        this prefix are removed.
        <!--
        As an example, if a
        hypothetical RIFT routing table contains A.1/16 @ [A,B], A.1.1/24 @ [C,D] it will
        on reception of negative A.1.1.1/32
        from D
        include the entry A.1.1.1/32 @ [C] resulting from computation inheriting A.1.1/24
        nexthops (C and D) and pruning all
        the nodes that advertised this negative prefix (which is D in this case).
        -->
    </t>
    
    <t>The rule of inheritance MUST be maintained when the nexthop list for a prefix is
        modified, as the
       modification may affect the entries for matching negative prefixes of immediate
        longer prefix length.
       For instance, if a nexthop is added, then by inheritance it must be added to all
        the negative routes
       of immediate longer prefixes length unless it is pruned due to a negative
        advertisement for the same
       next hop. Similarly, if a nexthop is deleted for a given prefix, then it is
        deleted for all the
       immediately aggregated negative routes. This will recurse in the case of
        nested negative prefix
       aggregations.
    </t>
    
    <t>
       The rule of inheritance must also be maintained when a new prefix of intermediate length is inserted, 
       or when the immediately aggregating prefix is deleted from the routing table, making an even shorter
       aggregating prefix the one from which the negative routes now inherit their adjacencies. As the 
       aggregating prefix changes, all the negative routes must be recomputed, and then again the process 
       may recurse in case of nested negative prefix aggregations. 
    </t>

    <t>
        Although these operations can be computationally expensive, the overall
        load on devices in the network is low because these computations are not run
        very often, as positive route advertisements are always preferred over negative ones.
        This prevents recursion in most cases because positive reachability information never
        inherits next hops.</t>

    <t>To make the negative disaggregation less abstract and provide an example let us consider a
        ToP node T1 with 4 ToF parents S1..S4 as
    represented in <xref target="negdis1"/>:
    </t>
    
    <figure align="center" anchor="negdis1"
        title="A ToP node with 4 parents">
        <artwork align="center"><![CDATA[

                +----+    +----+    +----+    +----+          N
                | S1 |    | S1 |    | S1 |    | S1 |          ^
                +----+    +----+    +----+    +----+       W< + >E
                 |         |         |         |              v            
                 |+--------+         |         |              S 
                 ||+-----------------+         | 
                 |||+----------------+---------+   
                 ||||       
                +----+   
                | T1 |
                +----+            
        ]]>
        </artwork>
    </figure>
    
    <t>If all ToF nodes can reach all the prefixes in the network; with
    RIFT, they will normally advertise a default route south.
    An abstract Routing Information Base (RIB), more commonly known as a routing table,
        stores all types of maintained routes
    including the negative ones and "tie-breaks" for the best one,
        whereas an abstract Forwarding table (FIB)
    retains only the ultimately computed "positive" routing instructions.
    In T1, those tables would look as illustrated in <xref target="rib1"/>:
    </t>
    
        <figure align="center" anchor="rib1"
        title="Abstract RIB">
        <artwork align="center"><![CDATA[
        
                +---------+   
                | Default |
                +---------+    
                     |           
                     |     +--------+   
                     +---> | Via S1 |
                     |     +--------+   
                     |           
                     |     +--------+   
                     +---> | Via S2 |
                     |     +--------+   
                     |           
                     |     +--------+   
                     +---> | Via S3 |
                     |     +---------+   
                     |           
                     |     +--------+   
                     +---> | Via S4 |
                           +--------+   
                
        ]]>
        </artwork>
    </figure>
    
    <t>
    In case T1 receives a negative advertisement for prefix 2001:db8::/32 from
    S1 a negative route is stored in the RIB (indicated by a ~ sign), while the
    more specific routes to the complementing ToF nodes are installed in FIB.
    RIB and FIB in T1 now look as illustrated in <xref target="rib1.1"/> and
    <xref target="fib1.1"/>, respectively:
    </t>
    
            <figure align="center" anchor="rib1.1"
        title="Abstract RIB after negative 2001:db8::/32 from S1">
        <artwork align="center"><![CDATA[
        
 +---------+                 +-----------------+         
 | Default | <-------------- | ~2001:db8::/32  |                  
 +---------+                 +-----------------+  
      |                               |        
      |     +--------+                |     +--------+   
      +---> | Via S1 |                +---> | Via S1 | 
      |     +--------+                      +--------+ 
      |           
      |     +--------+   
      +---> | Via S2 |
      |     +--------+   
      |           
      |     +--------+   
      +---> | Via S3 |
      |     +---------+   
      |           
      |     +--------+   
      +---> | Via S4 |
            +--------+   
 
 ]]>
        </artwork>
    </figure>
     <t>
     The negative 2001:db8::/32 prefix entry inherits from ::/0, so the positive more specific
     routes are the complements to S1 in the set of next-hops for the default
     route. That entry is composed of S2, S3, and S4, or, in other words, it uses
         all entries
         the the default route with a "hole punched" for S1 into them.
     These are the next hops that are still available to reach 2001:db8::/32,
     now that S1 advertised that it will not forward 2001:db8::/32 anymore.
         Ultimately, those resulting next-hops are installed in
     FIB for the more specific route to 2001:db8::/32 as illustrated below:
     </t>
    <figure align="center" anchor="fib1.1"
        title="Abstract FIB after negative 2001:db8::/32 from S1">
        <artwork align="center"><![CDATA[
        
 +---------+                  +---------------+         
 | Default |                  | 2001:db8::/32 |                  
 +---------+                  +---------------+  
      |                               |        
      |     +--------+                |
      +---> | Via S1 |                |
      |     +--------+                |
      |                               |
      |     +--------+                |     +--------+   
      +---> | Via S2 |                +---> | Via S2 | 
      |     +--------+                |     +--------+ 
      |                               |
      |     +--------+                |     +--------+   
      +---> | Via S3 |                +---> | Via S3 | 
      |     +--------+                |     +--------+ 
      |                               |
      |     +--------+                |     +--------+   
      +---> | Via S4 |                +---> | Via S4 | 
            +--------+                      +--------+ 
                
        ]]>
        </artwork>
    </figure>
    
    
    
    <t>
    To illustrate matters further let us consider T1 receiving a negative advertisement
        for prefix 2001:db8:1::/48
    from S2, which is stored in RIB again.  After the update, the RIB in T1 is
        illustrated in <xref target="rib1.2"/>:
    </t>
    
            <figure align="center" anchor="rib1.2"
        title="Abstract RIB after negative 2001:db8:1::/48 from S2">
        <artwork align="center"><![CDATA[
        
 +---------+        +----------------+         +------------------+   
 | Default | <----- | ~2001:db8::/32 | <------ | ~2001:db8:1::/48 |             
 +---------+        +----------------+         +------------------+
      |                     |                           |       
      |     +--------+      |     +--------+            |      
      +---> | Via S1 |      +---> | Via S1 |            |
      |     +--------+            +--------+            |
      |                                                 |
      |     +--------+                                  |     +--------+
      +---> | Via S2 |                                  +---> | Via S2 |
      |     +--------+                                        +--------+
      |           
      |     +--------+   
      +---> | Via S3 |
      |     +---------+   
      |           
      |     +--------+   
      +---> | Via S4 |
            +--------+   
 
 ]]>
        </artwork>
    </figure>
    <t>
    Negative 2001:db8:1::/48 inherits from 2001:db8::/32 now, so the positive more
    specific routes are the complements to S2 in the set of next hops for
    2001:db8::/32, which are S3 and S4, or, in other words, all entries of
        the parent with the negative holes "punched in" again.
    After the update, the FIB in T1 shows as illustrated
    in <xref target="fib1.2"/>:
    </t>
    <figure align="center" anchor="fib1.2"
        title="Abstract FIB after negative 2001:db8:1::/48 from S2">
        <artwork align="center"><![CDATA[
        
 +---------+         +---------------+         +-----------------+
 | Default |         | 2001:db8::/32 |         | 2001:db8:1::/48 |
 +---------+         +---------------+         +-----------------+
      |                     |                           |    
      |     +--------+      |                           | 
      +---> | Via S1 |      |                           | 
      |     +--------+      |                           | 
      |                     |                           | 
      |     +--------+      |     +--------+            |   
      +---> | Via S2 |      +---> | Via S2 |            | 
      |     +--------+      |     +--------+            | 
      |                     |                           | 
      |     +--------+      |     +--------+            |     +--------+
      +---> | Via S3 |      +---> | Via S3 |            +---> | Via S3 |
      |     +--------+      |     +--------+            |     +--------+
      |                     |                           |
      |     +--------+      |     +--------+            |     +--------+ 
      +---> | Via S4 |      +---> | Via S4 |            +---> | Via S4 | 
            +--------+            +--------+                  +--------+
                
        ]]>
        </artwork>
    </figure>
    
     <t>
     Further, let us say that S3 stops advertising its service as default gateway.
     The entry is removed from RIB as usual. In order to update the FIB, it is
     necessary to eliminate the FIB entry for the default route, as well as all
     the FIB entries that were created for negative routes pointing to the 
     RIB entry being removed (::/0). This is done recursively for 2001:db8::/32
     and then for, 2001:db8:1::/48. The related FIB entries via S3 are removed,
     as illustrated in <xref target="fib1.3"/>.
     </t>
    <figure align="center" anchor="fib1.3"
        title="Abstract FIB after loss of S3">
        <artwork align="center"><![CDATA[
        
 +---------+         +---------------+         +-----------------+
 | Default |         | 2001:db8::/32 |         | 2001:db8:1::/48 |
 +---------+         +---------------+         +-----------------+
      |                     |                           |    
      |     +--------+      |                           | 
      +---> | Via S1 |      |                           | 
      |     +--------+      |                           | 
      |                     |                           | 
      |     +--------+      |     +--------+            |   
      +---> | Via S2 |      +---> | Via S2 |            | 
      |     +--------+      |     +--------+            | 
      |                     |                           | 
      |                     |                           | 
      |                     |                           | 
      |                     |                           | 
      |                     |                           | 
      |     +--------+      |     +--------+            |     +--------+ 
      +---> | Via S4 |      +---> | Via S4 |            +---> | Via S4 | 
            +--------+            +--------+                  +--------+
                
        ]]>
        </artwork>
    </figure>
    <t>
    Say that at that time, S4 would also disaggregate prefix 2001:db8:1::/48.
    This would mean that the FIB entry for 2001:db8:1::/48 becomes a discard
    route, and that would be the signal for T1 to disaggregate prefix
    2001:db8:1::/48 negatively in a transitive fashion with its own children.
    </t>
    
     <t>
    Finally, let us look at the case where S3 becomes available again as a default gateway, and a negative
    advertisement is received from S4 about prefix 2001:db8:2::/48 as opposed to
    2001:db8:1::/48.
    Again, a negative route is stored in the RIB, and the more specific route
    to the complementing ToF nodes are installed in FIB. 
    Since 2001:db8:2::/48 inherits from 2001:db8::/32, the positive FIB routes
    are chosen by removing S4 from S2, S3, S4. The abstract FIB in T1 now shows
    as illustrated in <xref target="fib1.4"/>:
     </t>
    <figure align="center" anchor="fib1.4"
        title="Abstract FIB after negative 2001:db8:2::/48 from S4">
        <artwork align="center"><![CDATA[
        
                                               +-----------------+
                                               | 2001:db8:2::/48 |
                                               +-----------------+
                                                       |
 +---------+       +---------------+    +-----------------+
 | Default |       | 2001:db8::/32 |    | 2001:db8:1::/48 |
 +---------+       +---------------+    +-----------------+
      |                    |                    |      | 
      |     +--------+     |                    |      |     +--------+
      +---> | Via S1 |     |                    |      +---> | Via S2 | 
      |     +--------+     |                    |      |     +--------+ 
      |                    |                    |      | 
      |     +--------+     |     +--------+     |      |     +--------+ 
      +---> | Via S2 |     +---> | Via S2 |     |      +---> | Via S3 | 
      |     +--------+     |     +--------+     |            +--------+
      |                    |                    |       
      |     +--------+     |     +--------+     |     +--------+
      +---> | Via S3 |     +---> | Via S3 |     +---> | Via S3 |
      |     +--------+     |     +--------+     |     +--------+
      |                    |                    |
      |     +--------+     |     +--------+     |     +--------+ 
      +---> | Via S4 |     +---> | Via S4 |     +---> | Via S4 | 
            +--------+            +--------+          +--------+
            
        ]]>
        </artwork>
    </figure>
    
    
</section><!-- attaching prefixes -->



<section title="Optional Zero Touch Provisioning (ZTP)" anchor="ZTP">

    <t>
        Each RIFT node can operate in zero touch
        provisioning (ZTP)
        mode, i.e. it has no configuration (unless it is a Top-of-Fabric
        at the top of the topology
        or the must operate in the topology
        as leaf and/or support leaf-2-leaf procedures)
        and it will fully configure itself after being
        attached to the
        topology. Configured nodes and nodes operating in ZTP can be
        mixed and will form a valid topology if achievable.
        </t>

    <t>The derivation of the level of each node happens based on offers
        received from its neighbors whereas each node (with possibly exceptions
        of configured leaves) tries to attach at the highest possible point in
        the fabric. This guarantees that even if the diffusion front reaches
        a node from "below" faster than from "above", it will greedily abandon
        already negotiated level derived from nodes topologically below it and
        properly peers with nodes above.
        </t>

    <t>The fabric is very consciously numbered from the top to allow for PoDs
        of different heights and minimize number of provisioning necessary,
        in this case just a TOP_OF_FABRIC flag on every node at the top of the fabric.
        </t>

    <t>
        This section describes the necessary concepts and procedures for ZTP
        operation.
    </t>

    <section title="Terminology" anchor="ZTPTerminology">

        <t>The interdependencies between the different flags and the configured
            level can be somewhat vexing at first and it may take multiple reads of
            the glossary to comprehend them.
            </t>

        <t>

            <list style='hanging'>

                <t hangText="Automatic Level Derivation:">Procedures which
                    allow nodes without level configured to derive it
                    automatically. Only applied if CONFIGURED_LEVEL is
                    undefined.</t>

                <t hangText="UNDEFINED_LEVEL:">A "null" value that
                    indicates that the level has not been determined and has
                    not been configured. Schemas normally indicate that
                    by a missing optional value without an
                    available defined default.</t>

                <t hangText="LEAF_ONLY:">An optional configuration
                    flag that can
                    be configured on a node to make sure it never leaves the
                    "bottom of the hierarchy". TOP_OF_FABRIC flag and
                    CONFIGURED_LEVEL
                    cannot be defined at the same time as this flag.
                    It implies
                    CONFIGURED_LEVEL value of 0.
                </t>

                <t hangText="TOP_OF_FABRIC flag:">Configuration flag that MUST be
                    provided to
                    all Top-of-Fabric nodes. LEAF_FLAG and CONFIGURED_LEVEL
                    cannot be defined at the same time as this flag.
                    It implies
                    a CONFIGURED_LEVEL value. In fact, it is basically a
                    shortcut for configuring same level at all Top-of-Fabric
                    nodes which is unavoidable since an initial 'seed' is
                    needed for
                    other ZTP nodes to derive their level in the topology. The flag
                    plays an important role in fabrics with multiple planes to
                    enable successful <xref target="negdisaggreg">negative disaggregation</xref>.
                </t>

                <t hangText="CONFIGURED_LEVEL:">A level value
                    provided manually. When this is defined (i.e. it is not
                    an UNDEFINED_LEVEL)
                    the node is
                    not participating in ZTP. TOP_OF_FABRIC flag
                    is ignored when this value is defined. LEAF_ONLY
                    can be set only if this value is undefined or set to 0.</t>

                <t hangText="DERIVED_LEVEL:">Level value computed via
                    automatic level derivation when
                    CONFIGURED_LEVEL is equal to
                    UNDEFINED_LEVEL.
                </t>

                <t hangText="LEAF_2_LEAF:">An optional flag that
                    can
                    be configured on a node to make sure it supports procedures
                    defined in
                    <xref target="leaf2leaf"/>. In a strict sense it is a
                    capability that implies LEAF_ONLY and the according
                    restrictions. TOP_OF_FABRIC flag is ignored
                    when set at the same time
                    as this flag.</t>

                <t hangText="LEVEL_VALUE:">In ZTP case the original
                    definition of
                    "level" in <xref target="glossary"/> is
                    both extended and relaxed. First, level is defined
                    now as LEVEL_VALUE and is the first defined value of
                    CONFIGURED_LEVEL followed by DERIVED_LEVEL. Second,
                    it is possible for nodes to be more
                    than one level apart to form adjacencies if any of the
                    nodes is at least LEAF_ONLY.</t>

                <t hangText="Valid Offered Level (VOL):">A neighbor's level
                    received
                    on a valid LIE (i.e. passing all checks for adjacency
                    formation while disregarding all clauses involving level
                    values)
                   persisting for the duration of the holdtime interval on the
                   LIE. Observe that offers from nodes offering level value
                   of 0 do not constitute VOLs (since no valid DERIVED_LEVEL
                   can be obtained from those and consequently `not_a_ztp_offer`
                   MUST be ignored). Offers from LIEs with
                   `not_a_ztp_offer` being true are not VOLs either. If a node
                   maintains parallel adjacencies to the neighbor, VOL on each
                   adjacency is considered as equivalent, i.e. the newest VOL
                   from any such adjacency updates the VOL received from the
                   same node.
                </t>

                <t hangText="Highest Available Level (HAL):">Highest defined
                    level value seen from all VOLs received.
                </t>

                <t hangText="Highest Available Level Systems (HALS):">Set of
                    nodes offering
                HAL VOLs.
                </t>

                <t hangText="Highest Adjacency Three Way (HAT):">Highest
                    neighbor level of all the formed three way adjacencies
                    for the node.</t>

            </list>
        </t>
        
    </section> <!-- ZTPTerminology -->

<section title="Automatic SystemID Selection">

    <t>RIFT nodes require a 64 bit SystemID which SHOULD be derived as
        EUI-64 MA-L derive according to <xref target="EUI64"/>. The
        organizationally governed portion of
        this ID  (24 bits) can be used to generate multiple IDs if required to
        indicate more than one RIFT instance."

        </t>
    <t>
        As matter of operational concern, the router MUST
        ensure that such
        identifier is not changing very frequently (or at least not without
        sending all its TIEs with fairly short lifetimes) since otherwise the
        network may be left with large amounts of stale TIEs in other nodes
        (though this is not necessarily a serious problem if the procedures
        described
        in <xref target="security"/> are implemented).
    </t>

</section>

<section title="Generic Fabric Example">

<t>ZTP forces us to think about miscabled or unusually cabled fabric and
how such a topology can be forced into a "lattice" structure which
a fabric
represents (with further restrictions). Let us consider a necessary and
sufficient physical cabling in
<xref target="pic-ztp-generic"/>. We assume all nodes being in
the same PoD.</t>

<figure align="center" anchor="pic-ztp-generic"
    title="Generic ZTP Cabling Considerations">
    <artwork align="center"><![CDATA[
.        +---+
.        | A |                      s   = TOP_OF_FABRIC
.        | s |                      l   = LEAF_ONLY
.        ++-++                      l2l = LEAF_2_LEAF
.         | |
.      +--+ +--+
.      |       |
.   +--++     ++--+
.   | E |     | F |
.   |   +-+   |   +-----------+
.   ++--+ |   ++-++           |
.    |    |    | |            |
.    | +-------+ |            |
.    | |  |      |            |
.    | |  +----+ |            |
.    | |       | |            |
.   ++-++     ++-++           |
.   | I +-----+ J |           |
.   |   |     |   +-+         |
.   ++-++     +--++ |         |
.    | |         |  |         |
.    +---------+ |  +------+  |
.      |       | |         |  |
.      +-----------------+ |  |
.              | |       | |  |
.             ++-++     ++-++ |
.             | X +-----+ Y +-+
.             |l2l|     | l |
.             +---+     +---+
    ]]>
    </artwork>
</figure>

<t>First, we must anchor the "top" of the cabling and that's what
    the TOP_OF_FABRIC flag at node A is for. Then things look smooth until
    we have to decide whether node Y is at the same level as I, J
    (and as consequence, X is south of it) or at
    the same level as X. This is
    unresolvable here until we
    "nail down the bottom" of the topology. To achieve that we choose to
    use in this
    example the leaf flags in X and Y. In case where Y would not have a leaf
    flag it will try to elect highest level offered and end up being
    in same level as I and J.
    </t>

</section>

<section title="Level Determination Procedure" anchor="LDP">
    <t>A node starting up with UNDEFINED_VALUE (i.e. without a
        CONFIGURED_LEVEL or any leaf or TOP_OF_FABRIC flag) MUST follow those
        additional procedures:</t>

    <t>
        <list style="numbers">
            <t>It advertises its LEVEL_VALUE on all LIEs (observe that this
                can be
                UNDEFINED_LEVEL which in terms of the schema is simply an
                omitted optional value).
            </t>
            <t>It computes HAL as numerically highest available level in
                all VOLs.
                </t>
            <t>It chooses then MAX(HAL-1,0) as its DERIVED_LEVEL.
                The node then starts
                to advertise
                this derived level.
            </t>
            <t>A node that lost all adjacencies with HAL value
                MUST  hold
                down computation of new DERIVED_LEVEL for a short period
                of time unless it has no VOLs from southbound adjacencies.
                After the holddown expired, it MUST discard
            all received offers, recompute DERIVED_LEVEL and announce
            it to all neighbors.</t>

            <t>A node MUST reset any adjacency that has changed the level it
                is offering and is in
                three-way state.</t>
            <t>A node that changed its defined level value MUST
                readvertise its own TIEs (since the new `PacketHeader` will
                contain a different level than before). Sequence number of each
                TIE MUST be increased.
                </t>
            <t>After a level has been derived the node MUST set
                the `not_a_ztp_offer` on LIEs towards all systems
                offering a VOL for HAL.
                </t>
            <t>A node that changed its level SHOULD flush from its
                link state database TIEs of all other nodes, otherwise
                stale information may persist on "direction reversal", i.e.
                nodes that seemed south are now north or east-west.
                This will not prevent the correct operation of the
                protocol but could be slightly confusing operationally.</t>
        </list>
    </t>

    <t>A node starting with LEVEL_VALUE being 0 (i.e. it assumes a leaf
        function by being configured with the appropriate flags
        or has a CONFIGURED_LEVEL of 0) MUST follow those
        additional procedures:</t>
    <t>
        <list style="numbers">
            <t>It computes HAT per procedures above but does NOT
                use it to compute DERIVED_LEVEL. HAT is used to limit
                adjacency formation per <xref target="LIE"/>.</t>
        </list>
    </t>
    <t>It MAY also follow modified procedures:
        <list style="numbers">
            <t>It may pick a different strategy to choose VOL, e.g.
                use the VOL value with highest number of VOLs. Such strategies
                are only possible since the node always remains "at the bottom
                of the fabric" while another layer could "invert" the fabric by
                picking its preferred VOL in a different fashion than always
                trying to achieve the highest viable level.
            </t>
            </list>
        </t>

    
</section>

             <section title="ZTP FSM">

                 <t>
        This section specifies the precise, normative ZTP FSM and
   can be omitted unless the reader is pursuing an implementation of
   the protocol.
</t>
                 
<t>Initial state is ComputeBestOffer.
</t>

<figure align="center" title="ZTP FSM FSM">
<artwork align="left"><![CDATA[
  Enter 
    |
    v
+------------------+
| ComputeBestOffer |
|                  |<----+
| Entry:           |     | BetterHAL [LEVEL_COMPUTE]
| [LEVEL_COMPUTE]  |     | BetterHAT [LEVEL_COMPUTE]
|                  |     | ChangeLocalConfiguredLevel [StoreConfigLevel,
|                  |     |                             LEVEL_COMPUTE]
|                  |     | ChangeLocalHierarchyIndications
|                  |     |   [StoreLeafFlags,
|                  |     |    LEVEL_COMPUTE]
|                  |     | LostHAT [LEVEL_COMPUTE]
|                  |     | NeighborOffer [IF NoLevelOffered 
|                  |     |                  THEN REMOVE_OFFER
|                  |     |                  ELSE IF OfferedLevel > Leaf
|                  |     |                    THEN UPDATE_OFFER
|                  |     |                    ELSE REMOVE_OFFER
|                  |     | ShortTic [RemoveExpiredOffers]
|                  |-----+
|                  |
|                  |<---------------------
|                  |---------------------> (UpdatingClients)
|                  | ComputationDone [-]
+------------------+
    ^   |
    |   | LostHAL [IF AnySouthBoundAdjacenciesPresent
    |   |            THEN UpdateHoldDownTimerToNormalValue
    |   |            ELSE FireHoldDownTimerImmediately]
    |   V
(HoldingDown)
]]></artwork></figure>
<figure align="center" title="ZTP FSM FSM (continued)">
<artwork align="left"><![CDATA[
(ComputeBestOffer)
    |   ^
    |   | ChangeLocalConfiguredLevel [StoreConfiguredLevel]
    |   | ChangeLocalHierarchyIndications [StoreLeafFlags]
    |   | HoldDownExpired [PURGE_OFFERS]
    V   |
+------------------+
| HoldingDown      |
|                  |<----+
|                  |     | BetterHAL [-]
|                  |     | BetterHAT [-]
|                  |     | ComputationDone [-]
|                  |     | LostHAL [-]
|                  |     | LostHat [-]
|                  |     | NeighborOffer [IF NoLevelOffered 
|                  |     |                  THEN REMOVE_OFFER
|                  |     |                  ELSE IF OfferedLevel > Leaf
|                  |     |                    THEN UPDATE_OFFER
|                  |     |                    ELSE REMOVE_OFFER
|                  |     | ShortTic [RemoveExpiredOffers,
|                  |     |           IF HoldDownTimer expired
|                  |     |             THEN PUSH HoldDownExpired]
|                  |-----+
+------------------+
    ^
    |
  (UpdatingClients)
]]></artwork></figure>
<figure align="center" title="ZTP FSM FSM (continued)">
<artwork align="left"><![CDATA[
(ComputeBestOffer)
    |   ^
    |   | BetterHAL [-]
    |   | BetterHAT [-]
    |   | LostHAT [-]
    |   | ChangeLocalHierarchyIndications [StoreLeafFlags]
    |   | ChangeLocalConfiguredLevel [StoreConfigLevel]
    V   |
+------------------+
| UpdatingClients  |
|                  |<----+
| Entry:           |     |
| [UpdateAllLIE-   |     | NeighborOffer [IF NoLevelOffered 
|  FSMsWith-       |     |                  THEN REMOVE_OFFER
|  Computation-    |     |                  ELSE IF OfferedLevel > Leaf
|  Results]        |     |                    THEN UPDATE_OFFER
|                  |     |                    ELSE REMOVE_OFFER
|                  |     | ShortTic [RemoveExpiredOffers]
|                  |-----+
+------------------+
    |
    | LostHAL [IF AnySouthBoundAdjacenciesPresent
    |            THEN UpdateHoldDownTimerToNormalValue
    |            ELSE FireHoldDownTimerImmediately]
    V
(HoldingDown)
]]></artwork></figure>



<!-- begin generated output -->

<t>Events</t>
<t><list style="symbols">
	<t>ChangeLocalHierarchyIndications:
		node locally configured with new leaf flags</t>
	<t>ChangeLocalConfiguredLevel:
		node locally configured with a defined level</t>
	<t>NeighborOffer:
		a new neighbor offer with optional level and neighbor state</t>
	<t>BetterHAL:
		better HAL computed internally</t>
	<t>BetterHAT:
		better HAT computed internally</t>
	<t>LostHAL:
		lost last HAL in computation</t>
	<t>LostHAT:
		lost HAT in computation</t>
	<t>ComputationDone:
		computation performed</t>
	<t>HoldDownExpired:
		holddown expired</t>
	<t>ShortTic:
		one second timer tick, to be ignored if transition does not exist</t>

</list></t>

<t>Actions</t>
<t><list>
	<t>on ShortTic in HoldingDown finishes in HoldingDown:
		remove expired offers and if holddown timer expired PUSH_EVENT HoldDownExpired
		</t>
	<t>on ShortTic in ComputeBestOffer finishes in ComputeBestOffer:
		remove expired offers
		</t>
	<t>on HoldDownExpired in HoldingDown finishes in ComputeBestOffer:
		PURGE_OFFERS
		</t>
	<t>on ChangeLocalConfiguredLevel in HoldingDown finishes in ComputeBestOffer:
		store configured level
		</t>
	<t>on ShortTic in UpdatingClients finishes in UpdatingClients:
		remove expired offers
		</t>
	<t>on BetterHAT in ComputeBestOffer finishes in ComputeBestOffer:
		LEVEL_COMPUTE
		</t>
	<t>on BetterHAL in HoldingDown finishes in HoldingDown:
		no action
		</t>
	<t>on ChangeLocalHierarchyIndications in HoldingDown finishes in ComputeBestOffer:
		store leaf flags
		</t>
	<t>on BetterHAT in UpdatingClients finishes in ComputeBestOffer:
		no action
		</t>
	<t>on BetterHAL in UpdatingClients finishes in ComputeBestOffer:
		no action
		</t>
	<t>on ChangeLocalHierarchyIndications in UpdatingClients finishes in ComputeBestOffer:
		store leaf flags
		</t>
	<t>on LostHAL in HoldingDown finishes in HoldingDown:

		</t>
	<t>on LostHAT in ComputeBestOffer finishes in ComputeBestOffer:
		LEVEL_COMPUTE
		</t>
	<t>on LostHAT in HoldingDown finishes in HoldingDown:
		no action
		</t>
	<t>on BetterHAT in HoldingDown finishes in HoldingDown:
		no action
		</t>
	<t>on NeighborOffer in UpdatingClients finishes in UpdatingClients:


                        <list>
                            <t>if no level offered then REMOVE_OFFER</t>
                            <t>else
                                <list>
                                    <t>if offered level > leaf then UPDATE_OFFER</t>
                                    <t>else REMOVE_OFFER</t>
                                </list>
                            </t>
                        </list>
		</t>
	<t>on LostHAL in ComputeBestOffer finishes in HoldingDown:
		if any southbound adjacencies present then update holddown timer to normal duration else fire holddown timer immediately
		</t>
	<t>on LostHAL in UpdatingClients finishes in HoldingDown:
		if any southbound adjacencies present then update holddown timer to normal duration else fire holddown timer immediately
		</t>
	<t>on ComputationDone in ComputeBestOffer finishes in UpdatingClients:
		no action
		</t>
	<t>on LostHAT in UpdatingClients finishes in ComputeBestOffer:
		no action
		</t>
	<t>on ComputationDone in HoldingDown finishes in HoldingDown:

		</t>
	<t>on ChangeLocalConfiguredLevel in ComputeBestOffer finishes in ComputeBestOffer:
		store configured level and LEVEL_COMPUTE
		</t>
	<t>on ChangeLocalConfiguredLevel in UpdatingClients finishes in ComputeBestOffer:
		store configured level
		</t>
	<t>on NeighborOffer in ComputeBestOffer finishes in ComputeBestOffer:


                        <list>
                            <t>if no level offered then REMOVE_OFFER</t>
                            <t>else
                                <list>
                                    <t>if offered level > leaf then UPDATE_OFFER</t>
                                    <t>else REMOVE_OFFER</t>
                                </list>
                            </t>
                        </list>
		</t>
	<t>on NeighborOffer in HoldingDown finishes in HoldingDown:


                        <list>
                            <t>if no level offered then REMOVE_OFFER</t>
                            <t>else
                                <list>
                                    <t>if offered level > leaf then UPDATE_OFFER</t>
                                    <t>else REMOVE_OFFER</t>
                                </list>
                            </t>
                        </list>
		</t>
	<t>on ChangeLocalHierarchyIndications in ComputeBestOffer finishes in ComputeBestOffer:
		store leaf flags and LEVEL_COMPUTE
		</t>
	<t>on BetterHAL in ComputeBestOffer finishes in ComputeBestOffer:
		LEVEL_COMPUTE
		</t>
	<t>on Entry into UpdatingClients:
		update all LIE FSMs with computation results
		</t>
	<t>on Entry into ComputeBestOffer:
		LEVEL_COMPUTE
		</t>

</list></t>



<t>
    Following words are used for well known procedures:

    <list style="numbers">
        <t>PUSH Event: pushes an event to be executed by the FSM upon exit of this action</t>
        <t>COMPARE_OFFERS: checks whether based on current offers and held last results the events
                BetterHAL/LostHAL/BetterHAT/LostHAT are necessary and returns them</t>
        <t>UPDATE_OFFER: store current offer with adjancency holdtime as lifetime and
                COMPARE_OFFERS, then PUSH according events</t>
        <t>LEVEL_COMPUTE: compute best offered or configured level and HAL/HAT, if anything changed
                PUSH ComputationDone</t>
        <t>REMOVE_OFFER: remove the according offer and COMPARE_OFFERS, PUSH according events</t>
        <t>PURGE_OFFERS: REMOVE_OFFER for all held offers, COMPARE OFFERS, PUSH according events</t>

    </list>

</t>


<!-- end generated output -->

                </section>


<section title="Resulting Topologies">
    <t>The procedures defined in <xref target="LDP"/> will lead to the
         RIFT topology and levels depicted in <xref target="pic-ztp-ldped"/>.</t>

    <figure align="center" anchor="pic-ztp-ldped"
        title="Generic ZTP Topology Autoconfigured">
        <artwork align="center"><![CDATA[
.        +---+
.        | As|
.        | 24|
.        ++-++
.         | |
.      +--+ +--+
.      |       |
.   +--++     ++--+
.   | E |     | F |
.   | 23+-+   | 23+-----------+
.   ++--+ |   ++-++           |
.    |    |    | |            |
.    | +-------+ |            |
.    | |  |      |            |
.    | |  +----+ |            |
.    | |       | |            |
.   ++-++     ++-++           |
.   | I +-----+ J |           |
.   | 22|     | 22|           |
.   ++--+     +--++           |
.    |           |            |
.    +---------+ |            |
.              | |            |
.             ++-++     +---+ |
.             | X |     | Y +-+
.             | 0 |     | 0 |
.             +---+     +---+
        ]]>
        </artwork>
    </figure>

<t>
In case we imagine the LEAF_ONLY restriction on Y is removed the outcome
would be very different however and result in <xref target="pic-ztp-ldped-nol"/>.
This demonstrates basically that auto configuration makes miscabling
detection hard and with that can lead to undesirable effects in cases where
leaves are not
"nailed" by the accordingly configured flags and arbitrarily cabled.
</t>

<t>A node MAY analyze the outstanding level offers on its interfaces and
    generate warnings when its internal ruleset flags a possible miscabling.
    As an example, when a node's sees ZTP level offers that differ by more than
    one level from its chosen level (with proper accounting for leaf's being
    at level 0) this can indicate miscabling.

    </t>

<figure align="center" anchor="pic-ztp-ldped-nol"
    title="Generic ZTP Topology Autoconfigured">
    <artwork align="center"><![CDATA[
.        +---+
.        | As|
.        | 24|
.        ++-++
.         | |
.      +--+ +--+
.      |       |
.   +--++     ++--+
.   | E |     | F |
.   | 23+-+   | 23+-------+
.   ++--+ |   ++-++       |
.    |    |    | |        |
.    | +-------+ |        |
.    | |  |      |        |
.    | |  +----+ |        |
.    | |       | |        |
.   ++-++     ++-++     +-+-+
.   | I +-----+ J +-----+ Y |
.   | 22|     | 22|     | 22|
.   ++-++     +--++     ++-++
.    | |         |       | |
.    | +-----------------+ |
.    |           |         |
.    +---------+ |         |
.              | |         |
.             ++-++        |
.             | X +--------+
.             | 0 |
.             +---+

    ]]>
    </artwork>
</figure>


    </section>

</section> <!-- ZTP -->

<section title="Stability Considerations">
    <t>The autoconfiguration mechanism computes a global maximum of levels
        by diffusion. The achieved equilibrium can be disturbed massively by
        all nodes with highest level either leaving or entering the domain (with
        some finer distinctions not explained further).
        It is therefore recommended that each node is multi-homed towards
        nodes with respective HAL offerings. Fortunately,
        this is the natural state of things for
        the topology variants considered in RIFT.
        </t>

    </section>

</section>

        <section title="Further Mechanisms">
            <section title="Overload Bit" anchor="overload">

                <t>The overload Bit MUST be respected in all according
                    reachability computations. A node with overload
                    bit set SHOULD NOT advertise any reachability
                    prefixes southbound except locally hosted ones.
                    A node in overload SHOULD advertise all its locally
                    hosted prefixes north and southbound.
                </t>

                <t>The leaf node SHOULD set the 'overload' bit
                    on its node TIEs, since if the spine nodes were
                    to forward traffic not meant for the local
                    node, the leaf node does not have the topology
                    information to prevent a routing/forwarding
                    loop.

                </t>
            </section>

<section title="Optimized Route Computation on Leaves">

    <t>Since the leaves do see only "one hop away" they do not need to
        run a "proper" SPF.  Instead, they can
        gather the available prefix candidates from their neighbors
        and build the routing table accordingly.

    </t>

    <t>A leaf will have no North TIEs except its own and
        optionally from its East-West
        neighbors.  A leaf will have South TIEs from its neighbors.
        </t>
    <t>Instead of creating a network graph from its North TIEs and
        neighbor's South TIEs and then running
        an SPF, a leaf node can simply compute the minimum cost and
        next_hop_set to each leaf neighbor by examining its local
        adjacencies, determining bi-directionality from the associated
        North TIE, and specifying the neighbor's next_hop_set set and cost
        from the minimum cost local adjacency to that neighbor.</t>

    <t>Then a leaf attaches prefixes as described in <xref
        target="sec_attaching_prefixes"/>.</t>
</section> <!-- <section title="Optimized Route Computation on leaves"> -->

<section title="Mobility" anchor="mobility">
    <t>It is a requirement for RIFT to maintain at the control plane a real time
    status of which prefix is attached to which port of which leaf, even in a
    context of mobility where the point of attachment may change several
    times in a subsecond period of time.
    </t>
    <t>There are two classical approaches to maintain such knowledge in an
    unambiguous fashion:
    <list style="hanging">
        <t hangText="time stamp:">
        With this method, the infrastructure records the precise time at
        which the movement is observed. One key advantage of this technique is
        that it has no dependency on the mobile device. One drawback is that the
        infrastructure must be precisely synchronized to be able to compare time
        stamps as observed by the various points of attachment, e.g., using the
        variation of the Precision Time Protocol (PTP)
        IEEE Std. 1588 <xref target="IEEEstd1588"/>, <xref target="IEEEstd8021AS"/>
            designed for bridged LANs
        IEEE Std. 802.1AS <xref target="IEEEstd8021AS"/>.
        Both the precision of the synchronization protocol and the resolution of
        the time stamp must beat the highest possible roaming time on the fabric.
        Another drawback is that the presence of the mobile device may be
        observed only asynchronously, e.g., after it starts using an IP protocol
        such as ARP <xref target="RFC0826"/>, 
        IPv6 Neighbor Discovery <xref target="RFC4861"/><xref target="RFC4862"/>,
        or DHCP <xref target="RFC2131"/><xref target="RFC8415"/>.
        </t>
        <t hangText="sequence counter:"> With this method, a mobile node
        notifies its point of attachment on arrival with a sequence counter that 
        is incremented upon each movement.  On the positive side, this method does
        not have a dependency on a precise sense of time, since the sequence of
        movements is kept in order by the device. 
        The disadvantage of this approach is the lack of support for protocols
        that may be used by the mobile node to register its presence to the leaf
        node with the capability to provide a sequence counter.
        Well-known issues with wrapping sequence counters must be addressed
        properly, and many forms of sequence counters that vary in both wrapping
        rules and comparison rules. A particular knowledge of the source of the
        sequence counter is required to operate it, and the comparison between
        sequence counters from heterogeneous sources can be hard to impossible.
        </t>
    </list>
    </t>
    <t>
        RIFT supports a hybrid approach contained in an optional `PrefixSequenceType`
        prefix attribute that we call a `monotonic clock` consisting of a timestamp and
        optional sequence number. In case of presence of the attribute:
    <list style="symbols">
        <t>
        The leaf node MAY advertise a time stamp of the latest sighting of a prefix,
        e.g., by snooping IP protocols or the node using the time at which it
        advertised the prefix. RIFT transports the time stamp within the desired
        prefix North TIEs as 802.1AS timestamp.
        </t>
        <t>
        RIFT may interoperate with the <xref target="RFC8505">
        "update to 6LoWPAN Neighbor Discovery"</xref>, which provides a method
        for registering a prefix with a sequence counter called a Transaction ID
        (TID). RIFT transports in such case the TID in its native form.
        </t>

    <t>RIFT also defines an abstract negative clock (ASNC) that
        compares as less than any other clock. By default, the lack of a
        `PrefixSequenceType` in a Prefix North TIE
        is interpreted as  ASNC. We call this also an `undefined` clock.
        </t>
        <t>Any prefix present on the fabric in multiple nodes that has the
        `same` clock is considered as anycast. ASNC is always considered
        smaller than
        any defined clock. </t>
        <t>RIFT implementation assumes by default that all nodes are being synchronized
            to 200 milliseconds
            precision which is easily achievable even in very large fabrics
            using <xref target="RFC5905"/>. An implementation MAY provide a way to reconfigure
            a domain to a different value. We call this variable MAXIMUM_CLOCK_DELTA.
        </t>
	</list>


    </t>

    <section title="Clock Comparison" anchor="clock_compare">
        <t>
            All monotonic clock values are comparable to each other
            using the following rules:
            <list style="numbers">
                <t>ASNC is older than any other value except ASNC AND</t>
                <t>Clock with timestamp differing by more than MAXIMUM_CLOCK_DELTA are
                    comparable by using the timestamps only AND</t>
                <t>Clocks with timestamps differing by less than MAXIMUM_CLOCK_DELTA are
                    comparable by using their TIDs only AND</t>
                <t>An undefined TID is always older than any other TID AND</t>
                <t>TIDs are compared using rules of
                    <xref target="RFC8505"/>.
                        </t>
                </list>
            </t>
        </section>

<section title="Interaction between Time Stamps and Sequence Counters">

    <t>
    For slow movements that occur less frequently than e.g. once per second, the time stamp
    that the RIFT infrastructure captures is enough to determine the freshest
    discovery. If the point of attachment changes faster than the maximum drift of
    the time stamping mechanism (i.e. MAXIMUM_CLOCK_DELTA), then a sequence counter is
    required to add resolution to the freshness evaluation, and it must be sized
    so that the counters stay comparable within the resolution of the time 
    sampling mechanism.  
    </t>
    <t>  
    The sequence counter in <xref target="RFC8505"/> is
    encoded as one octet and wraps around using <xref target="arithmetic"/>.
    </t>
    <t>  
    Within the resolution of MAXIMUM_CLOCK_DELTA the
    sequence counters captured during 2 sequential values of the time stamp SHOULD
    be comparable.
    This means with default values
        that a node may move up to 127 times during a 200 milliseconds period and the
        clocks remain still comparable thus allowing the infrastructure
    to assert the freshest advertisement with no ambiguity.
    </t>
</section> <!-- Interaction between Time Stamps and Sequence Counters -->

<section title="Anycast vs. Unicast">

    <t>
    A unicast prefix can be attached to at most one leaf, whereas an
    anycast prefix may be reachable via more
    than one leaf. 
    </t>

<t>
    If a monotonic clock attribute is provided on the prefix, then the prefix
    with the `newest`
    clock value is strictly preferred. An anycast
    prefix does not carry a clock or all clock attributes MUST be the same under
    the rules of <xref target="clock_compare"/>.
</t>

<t>Observe that it is important that in mobility events the leaf is re-flooding
    as quickly as possible the absence of
    the prefix that moved away.
    </t>

<t>Observe further that without support for <xref target="RFC8505"/>
    movements on the fabric within intervals smaller than 100msec will be seen
    as anycast.</t>


</section> <!-- Anycast vs. Unicast -->

<section title="Overlays and Signaling">
   <t> 
   RIFT is agnostic whether any overlay technology like [MIP, LISP, VxLAN, NVO3]
   and the associated signaling is deployed over it. But it is expected that
   leaf nodes, and possibly Top-of-Fabric nodes can perform the correct
   encapsulation.
   </t>
   <t>
   In the context of mobility, overlays provide a classical solution to 
   avoid injecting mobile prefixes in the fabric and improve the scalability
   of the solution. It makes sense on a data center that already uses overlays
   to consider their applicability to the mobility solution; as an example, a
   mobility protocol such as LISP may inform the ingress leaf of the location of
   the egress leaf in real time.
   </t>   
   <t>
   Another possibility is to consider that mobility as an underlay service and
   support it in RIFT to an extent. The load on the
   fabric augments with the amount of mobility obviously since a move forces
   flooding and computation on all nodes in the scope of the move so tunneling
       from leaf to the Top-of-Fabric may be desired. </t>

    <!-- PRZ: TBD

   <t>
   The overhead and performance limitations can be alleviated with an hybrid
   approach whereby mobile North TIEs related to mobile prefixes are not flooded
   throughout but unicast to a limited set of Spine Nodes, which act as anchor
   points and tunnel the traffic back to the leaf where the mobile prefix is
   attached. This approach hides the mobility to the end nodes by having the
   encapsulation done at the spine or superspine as opposed to the ingress leaf.
   </t>

 <section title="Encapsulation at the Spine">
    <t>The following section describes a variation of the hybrid mode in more
    details, whereby a separate mobility protocol is used as opposed to unicast
    of North TIEs. In that case, the logical steps are the following:
    <list style="numbers">
    <t>the mobile device registers to the visited leaf, e.g., using IPv6 ND
    <xref target="I-D.ietf-6lo-rfc6775-update"/>;
    </t>
    <t>RIFT in the leaf is configured with a mobility protocol and refrains from
    injecting the mobile prefix in the fabric. Instead, the RIFT notifies the
    mobility protocol that handles the signaling;
    </t>
    <t>The mobility protocol informs the anchor point, e.g., duplicated in a
    pair of spine nodes for redundancy, that the mobile device showed up. At
    that point the anchor points may encapsulate traffic to the leaf, which
    decapsulates and forward to the attached mobile node, as illustrated in
    <xref target="mobreg"/>;
    </t>

    <t>
    If a superspine is deployed then it makes sense to flood the superspine so
    all superspine nodes can perform the encapsulation, as illustrated in
    <xref target="mobtunnel"/>. The alternative is either for the anchor points
    to flood the spine so any spine node can perform the encapsulation, or to
    disaggregate the mobile prefix.
    </t>


   </list>
</t>

    -->

   <!-- PRZ: TBD 

<figure align="center" anchor="mobreg"
                    title="Mobility registration">
                    <artwork align="center"><![CDATA[
                +~~~~~~~~~+    +~~~~~~~~~+    +~~~~~~~~~+          
                | Super   |    | Super   |    | Super   |          N 
                | Spine   |    | Spine   |    | Spine   |          ^
    Level 2     +~~~~^~~~~+    +~~~~~^~~~+    +~~~~^~~~~+        W<*>E
                     |  flood         \            |               v
                     +~~~~~~~~~~~~~~~+ \   +~~~~~~~+               S
                  |                   \ | /
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ +++ ~+  +~~~~~~~+  +~~~~~~~+
   | Spine |  | Spine |  | Spine |  | Spine |  | Spine |  | Spine |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~^~~~+  +~~~~~~~+  +~~~~~~~+
                                        |
                                        |
   +~~~~~~~+  +~~~~~~~+  +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~~~~~+
   |       |  |       |  |       |  |   |   |  |       |  |       |
   +~~~~~~~+  +~~~~~~~+  +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~~~~~+
                                        | mobile 
                                        | registration
   +~~~~~~~+  +~~~~~~~+  +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~~~~~+
   | Leaf  |  |       |  | Leaf  |  |   |   |  | Leaf  |  | Leaf  |
   +~~~~~~~+  +~~~~~~~+  +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~~~~~+
                                        |             
                                      mobile 
                                      prefix 

    ]]>
    </artwork>

</figure>

    -->

   <!-- PRZ: TBD 

<figure align="center" anchor="mobtunnel"
                    title="Mobility Tunnel from Superspine">
                    <artwork align="center"><![CDATA[
                +~~~~~~~~~+    +~~~~~~~~~+    +~~~~~~~~~+          
                | Super   |    | +~~~+   |    | Super   |          N 
                | Spine   |    | |   |   |    | Spine   |          ^
    Level 2     +~~~~~~~~~+    +~|~ |v| ~+    +~~~~~~~~~+        W<*>E
                                 |   \ \                           v
                  +~~~~~~~~~~~~~~+    \ \                          S
                  |                    | |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ | | ~+  +~~~~~~~+  +~~~~~~~+
   | Spine |  |   |   |  | Spine |  |  | |  |  | Spine |  | Spine |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ | | ~+  +~~~~~~~+  +~~~~~~~+
                  |                    | |
                  |                    | |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ | | ~+  +~~~~~~~+  +~~~~~~~+
   |       |  |   |   |  |       |  |  | |  |  |       |  |       |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ | | ~+  +~~~~~~~+  +~~~~~~~+
                  |                    | |
                  |                    | |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~ | | ~+  +~~~~~~~+  +~~~~~~~+
   | Leaf  |  |   |   |  | Leaf  |  |  |||  |  | Leaf  |  | Leaf  |
   +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~|~~~+  +~~~~~~~+  +~~~~~~~+
                  |                     v             
                Packet                mobile 
                to                    prefix 

    ]]>
    </artwork>
</figure>


</section>
    -->

</section> <!-- Overlays and Signaling -->

</section> <!-- <section title="Fast Mobility"> -->

<section title="Key/Value Store">

    <section title="Southbound">

        <t>
            The protocol supports a southbound distribution of key-value pairs that
            can be used to e.g. distribute configuration information during topology
            bring-up. The KV South TIEs can arrive from multiple nodes
            and hence need tie-breaking per key. We use the following rules
        </t>
        <t>
            <list style="numbers">
                <t>Only KV TIEs originated by nodes to which the receiver has a bi-directional
                    adjacency are considered.</t>
                <t>Within all such valid KV South TIEs containing the key, the value of the KV South TIE for
                    which the according node South TIE is present, has the
                    highest level and within the same level has
                    highest originating system ID is preferred. If keys in the most preferred
                    TIEs are overlapping, the behavior is undefined.
                </t>

            </list>

        </t>

        <t>Observe that if a node goes down, the node south of it looses adjacencies
            to it and with that the KVs will be disregarded and on tie-break changes
            new KV re-advertised to prevent stale information
            being used by nodes further south. KV information in southbound
            direction is not result of independent computation of every node over same
            set of TIEs but a diffused computation.
        </t>

    </section>

    <section title="Northbound">
        <t>Certain use cases seem to necessitate distribution of essentially
            KV information that is generated in the leaves in the northbound
            direction. Such information is flooded in KV North TIEs.
            Since the originator of northbound KV is preserved during
            northbound flooding, overlapping keys could be used. However,
            to omit further protocol complexity, only the value of the key
            in TIE tie-broken in same fashion as southbound KV TIEs is used.</t>
    </section>
</section>

<section title="Interactions with BFD" anchor="bfd">
    <t>RIFT MAY incorporate <xref target="RFC5881">BFD</xref> to react quickly
        to link failures. In such case following procedures
        are introduced: </t>

    <t>
        <list>

            <t>After RIFT three-way hello adjacency convergence
                a BFD session MAY be formed automatically
                between the RIFT endpoints without further configuration
                using the exchanged discriminators. The capability of the
                remote side to support BFD is carried on the LIEs. </t>
            <t>In case established BFD session goes Down after it was Up, RIFT
                adjacency SHOULD be re-initialized and subsequently started from Init after
                it sees a consecutive BFD Up.</t>
            <t>In case of parallel
                links between nodes each link MAY run its own independent BFD session
                or they may share a session.
            </t>
            <t>In case RIFT changes link identifiers or BFD capability indication
                both the LIE as well as the BFD
                sessions SHOULD be brought down and back up again.</t>
            <t>Multiple RIFT instances MAY choose to share a single BFD session (in
                such case it is undefined what discriminators are used albeit
                RIFT MAY advertise the same link ID for the same interface in
                multiple instances and with that "share" the discriminators).</t>
            <t>BFD TTL follows <xref target="RFC5082"/>.</t>
        </list>
    </t>
</section>


<section title="Fabric Bandwidth Balancing" anchor="bwb">

    <t>A well understood problem in fabrics is that in case of link losses
        it would be ideal to rebalance how much traffic is offered to
        switches in the next level
        based on the ingress and egress bandwidth they have. Current attempts
        rely mostly on specialized traffic engineering via controller or leaves being
        aware of complete topology with according cost and complexity.
    </t>
    <t>RIFT can support a very light weight mechanism that can deal with the problem
        in an approximate way based on the fact that RIFT is loop-free.
    </t>

    <section title="Northbound Direction"
        anchor="varydefault">
        <t>Every RIFT node SHOULD compute the
            amount of northbound bandwidth available through neighbors at higher level
            and modify
            distance received on default route from this neighbor.
            Those different distances SHOULD be used to support weighted
            ECMP forwarding towards higher level when using default route.
            We call such a distance Bandwidth Adjusted Distance
            or BAD. This is best illustrated by a simple example.
        </t>

        <t>
            <figure align="center" anchor="pic-default-modify"
                title="Balancing Bandwidth">
                <artwork align="center"><![CDATA[
                    .   100  x             100 100 MBits
                    .    |   x              |   |
                    .  +-+---+-+          +-+---+-+
                    .  |       |          |       |
                    .  |Spin111|          |Spin112|
                    .  +-+---+++          ++----+++
                    .    |x  ||           ||    ||
                    .    ||  |+---------------+ ||
                    .    ||  +---------------+| ||
                    .    ||               || || ||
                    .    ||               || || ||
                    .   -----All Links 10 MBit-------
                    .    ||               || || ||
                    .    ||               || || ||
                    .    ||  +------------+| || ||
                    .    ||  |+------------+ || ||
                    .    |x  ||              || ||
                    .  +-+---+++          +--++-+++
                    .  |       |          |       |
                    .  |Leaf111|          |Leaf112|
                    .  +-------+          +-------+

                ]]>
                </artwork>
            </figure>
        </t>

        <!--
         More precisely, a node determines all other nodes at the same level
         N_s using the same algorithm as Section 4.2.3.8 while ignoring
         overloaded nodes and computes its own available bandwidth B_s as sum
         of bandwidth on links to its northern neighbors.  In the same fashion
         minimum and maximum available northbound bandwidth for any node in
         N_s is determined as B_min and B_max.  Each node MUST then remap its
         own bandwidth B_s as compared to the range [B_min, B_max] into the
         default route distance in range [`normalized_bw_metric_max`,
         `normalized_bw_metric_min`] in a linear fashion, i.e.  node with
         B_max will advertise default route with `normalized_bw_metric_min`
         and the one with B_min a default route with
         `normalized_bw_metric_max`. In case B_min and B_max cannot be
         determined (e.g.  none of the nodes have any northbound metric), the
         node MUST use `normalized_bw_metric_min`.  In case where other nodes
         have northbound links but the node itself has none it MUST use
         `normalized_bw_metric_max` which amounts to trying to deflect most of
         the northbound traffic to those nodes.

         The range [`normalized_bw_metric_max`, `normalized_bw_metric_min`]
         leaves intentionally enough space to allow for local configuration
         that forces either a lower or higher distance than any automatically
         computed BAD.
         -->

        <t>All links from leaves in <xref target="pic-default-modify"/> are assumed to 10
            MBit/s bandwidth while the uplinks one level further up are assumed to be
            100 MBit/s. Further, in <xref target="pic-default-modify"/> we assume that Leaf111 lost
            one of the parallel links to Spine 111 and with that wants to possibly push more traffic
            onto Spine 112. Leaf 112 has equal bandwidth to Spine 111 and Spine 112 but Spine 111
            lost one of its uplinks.
        </t>

        <t>
            The local modification of the received default route distance from upper level
            is achieved by
            running a relatively simple algorithm where the bandwidth is weighted
            exponentially while the distance on the default route represents a multiplier
            for the bandwidth weight for easy operational adjustments.
        </t>

        <t>On a node L use Node TIEs to
            compute for each non-overloaded northbound neighbor N three values:
            <list>
                <t>L_N_u: as sum of
                    the bandwidth available to N</t>
                <t>N_u: as sum of the uplink bandwidth available on N</t>
                <t>T_N_u: as sum of L_N_u * OVERSUBSCRIPTION_CONSTANT + N_u</t>
            </list>
        </t>

        <t>For all T_N_u determine the according M_N_u as log_2(next_power_2(T_N_u)) and
            determine MAX_M_N_u as maximum value of all M_N_u.
        </t>

        <t>For each advertised default route from a node N modify the advertised distance D
            to BAD = D * (1 + MAX_M_N_u - M_N_u) and use BAD instead of distance D
            to weight balance default forwarding towards N.</t>

        <t>For the example above a simple table of values will help the understanding. We
            assume the default route distance is advertised with D=1 everywhere and
            OVERSUBSCRIPTION_CONSTANT = 1.</t>
        
        <texttable anchor="BADTable"
            title="BAD Computation"
            style="all">
            
            <ttcol>Node</ttcol><ttcol>N</ttcol><ttcol>T_N_u</ttcol><ttcol>M_N_u</ttcol><ttcol>BAD</ttcol>
            <c>Leaf111</c>    <c>Spine 111</c>   <c>110</c>        <c>7</c>           <c>2</c>
            <c>Leaf111</c>    <c>Spine 112</c>   <c>220</c>        <c>8</c>           <c>1</c>
            <c>Leaf112</c>    <c>Spine 111</c>   <c>120</c>        <c>7</c>           <c>2</c>
            <c>Leaf112</c>    <c>Spine 112</c>   <c>220</c>        <c>8</c>           <c>1</c>
        </texttable>
        
        <t>If a calculation produces a result exceeding the range of the type, e.g. bandwidth,
            the result is set to the highest possible value for that type.
        </t>
        
        
        <t>BAD is only computed for default routes. A node MAY compute and use BAD for
            any disaggregated
            prefixes or other RIFT routes. A node MAY use another algorithm than BAD to
        weight northbound traffic based on bandwidth given that the algorithm is
        distributed and un-synchronized and ultimately, its correct behavior does not depend on
        uniformity of balancing algorithms used in the fabric. E.g. it is
        conceivable that leaves could use real time link loads gathered by analytics
            to change the amount of traffic assigned to each default route next hop.</t>
        <t>Observe further that a change in available bandwidth will only affect at maximum
            two levels down in the fabric, i.e. blast radius of bandwidth changes is contained
            no matter its height.</t>
        
    </section>
    
    <section title="Southbound Direction"
        anchor="varysouthbandwidth">

        <t>Due to its loop free properties a node MAY take during S-SPF into account
            the available bandwidth on the nodes in lower levels and modify the amount
            of traffic offered to next level's "southbound" nodes based as what it
            sees is the total achievable maximum flow through those nodes. It is
            worth observing that such computations may work better if standardized
            but does not have to be necessarily. As long the packet keeps on heading
            south it will take one of the available paths and arrive at the intended
            destination.</t>

    </section>
</section>

<section title="Label Binding" anchor="downstreamlabels">

    <t>A node MAY advertise on its LIEs a locally significant, downstream
        assigned, interface specific label. One use of such label
        is a hop-by-hop encapsulation allowing to easily distinguish
        forwarding planes served by a multiplicity of RIFT instances.
    </t>
</section>

<section title="Leaf to Leaf Procedures" anchor="leaf2leaf">
    <t>RIFT can optionally allow special leaf East-West adjacencies under
        additional set of rules. The leaf supporting those
        procedures MUST:

    </t>

    <t>
        <list>
            <t>advertise the
                LEAF_2_LEAF flag in node capabilities AND</t>
            <t>
                set the overload bit on all leaf's node TIEs AND</t>
            <t>flood only node's own north and south TIEs over E-W leaf adjacencies AND </t>
            <t>always use E-W leaf adjacency  in both north as well as south computation AND</t>
            <t>install a discard route for any advertised aggregate in leaf's TIEs AND</t>
            <t>never form southbound adjacencies.</t>
        </list>
    </t>

    <t>This will allow the E-W leaf nodes to exchange traffic strictly for the prefixes
        advertised in each other's north prefix TIEs (since the southbound computation will
        find the reverse direction in the other node's TIE and install its north prefixes).
    </t>
    
</section><!-- leaf-to-leaf -->


<section title="Address Family and Multi Topology Considerations">
    <t>Multi-Topology (MT)<xref target="RFC5120"></xref>
        and Multi-Instance (MI)<xref target="RFC8202"></xref>
        is used today in link-state routing protocols to
        support several domains on the same
        physical topology. RIFT supports this capability by
        carrying transport ports in the LIE protocol
        exchanges.  Multiplexing of LIEs can be achieved by
        either choosing varying multicast addresses or ports
        on the same address.
    </t>

    <t>BFD interactions in <xref target="bfd"/>
        are implementation dependent when multiple RIFT instances run on the
    same link.</t>
</section>

<section title="Reachability of Internal Nodes in the Fabric" anchor="onastick">
    <t>
        RIFT does not precondition that its nodes have reachable addresses albeit
        for operational purposes this is clearly desirable. Under normal operating
        conditions this can be easily achieved by e.g. injecting the node's loopback
        address into North and South Prefix TIEs or other implementation specific
        mechanisms.
    </t>

    <t>
        Things get more interesting in case a node looses all its northbound
        adjacencies but is not at the top of the fabric. That is outside the
        scope of this document and may be covered in a separate document.
    </t>
</section>


<section title="One-Hop Healing of Levels with East-West Links" anchor="healing">

    <t>
        Based on the rules defined in <xref target="calculate"/>,
        <xref target="defaultrouterules"/> and
        given presence of E-W links, RIFT can provide a one-hop protection of nodes that lost
        all their northbound links or in other complex link set failure scenarios except at
        Top-of-Fabric where the links are used
        exclusively to flood topology information in multi-plane designs.
        <xref target="onastickexample"/> explains
        the resulting behavior based on one such example.
    </t>

</section>

</section>
<section title="Security" anchor="security-section">

<section title="Security Model" anchor="model">
    <t>
        An inherent property of any security and ZTP architecture is the
        resulting trade-off in regard to integrity verification of the
        information distributed through the fabric vs. necessary provisioning
        and auto-configuration.  At a minimum, in all approaches, the security of
        an established adjacency can be ensured. The stricter the security
        model the more provisioning must take over the role of ZTP.
    </t>

     <t>
        The most security conscious operators will want to have full control
        over which port on which router/switch is connected to the respective
         port on the "other side", which we will call the
        "port-association model" (PAM) achievable e.g. by configuring on each
         port pair a designated shared key or pair of private/public keys. In
        secure data center locations, operators may want to control which
        router/switch is connected to which other router/switch only or choose
        a "node-association model" (NAM) which allows, for example, simplified
        port sparing. In an even more relaxed environment, an operator may
        only be concerned that the router/switches share credentials ensuring
        that they belong to this particular data center network hence allowing the
        flexible sparing of whole routers/switches. We will define that case
        as the "fabric-association model" (FAM), equivalent to using a shared
        secret for the whole fabric.  Such flexibility may make sense for leaf
        nodes such as servers where the addition and swapping of servers is
        more frequent than the rest of the data center network.  Generally,
        leaves of the fabric tend to be less trusted than switches. The different
         models could
        be mixed throughout the fabric if the benefits outweigh the cost of
        increased complexity in provisioning. 
    </t>
           
    <t>
        In each of the above cases, some configuration mechanism is needed to
        allow the operator to specify which connections are allowed, and some
	mechanism is needed to:
         </t>
    <t>
	<list style="letters">
        <t>specify the according level in the fabric,</t>
	<t>discover and report missing connections, </t>
	<t>discover and report unexpected connections, and prevent such
        adjacencies from forming. </t>
</list>
    </t>	
       
    <t>
        On the more relaxed configuration side of the spectrum, operators
        might only configure the level of each switch, but don't explicitly
        configure which connections are allowed. In this case, RIFT will
        only allow adjacencies to come up between nodes are that in
        adjacent levels. The  operators with lowest security
        requirements may not use any configuration to specify which
        connections are allowed. Such fabrics could rely fully on ZTP for each
        router/switch to discover its level and would only allow adjacencies
	between adjacent levels to come up. 
	<xref target="security-model-dig"/> illustrates the tradeoffs inherent
        in
        the different security models.
    </t>
           
    <t>
        Ultimately, some level of verification of the link quality may be
        required before an adjacency is allowed to be used for forwarding. For
        example, an implementation may require that a BFD session comes up
        before advertising the adjacency.
    </t>
    <t>
        For the above outlined cases, RIFT has two approaches to enforce that
        a local port is connected to the correct port on the correct remote
	router/switch.  
	One approach is to piggy-back on RIFT's authentication mechanism. 
	Assuming the provisioning model (e.g. the YANG model) is
        flexible enough, operators can choose to provision a unique
	authentication key for: 
   </t>
    <t>
	<list style="letters">
	<t> each pair of ports in "port-association model" or </t>
	<t> each pair of switches in "node-association model" or </t>
	<t> each pair of levels or </t>
	<t> the entire fabric in "fabric-association model".  </t>
</list>
</t>
    <t>
       The other approach is to rely on the system-id, port-id and level
        fields in the LIE message to validate an adjacency against the
        configured expected cabling topology, and optionally introduce some
        new rules in the FSM to allow the adjacency to come up if the
        expectations are met.
       </t>
         <t>

                <figure align="center" anchor="security-model-dig"
                    title="Security Model">
                    <artwork align="center"><![CDATA[


         ^                 /\                  |
        /|\               /  \                 |
         |               /    \                |
         |              / PAM  \               |
     Increasing        /        \          Increasing
     Integrity        +----------+         Flexibility
         &           /    NAM     \            &
    Increasing      +--------------+         Less 
    Provisioning   /      FAM       \     Configuration
         |        +------------------+         |
         |       / Level Provisioning \        |
         |      +----------------------+      \|/
         |     /    Zero Configuration  \      v
              +--------------------------+
 
                    ]]>
                    </artwork>
                </figure>
	</t>   
            
</section>
    <section title="Security Mechanisms" anchor="security-mechanisms">


    <t>
        RIFT Security goals are to ensure authentication, message integrity and
        prevention of replay attacks.  Low processing overhead and efficient
        messaging are also a goal.  Message confidentiality is a non-goal.
    </t>
           
    <t>
        The model in the previous section allows a range of security key types
	    that are analogous to the various security association models.
    	PAM and NAM allow security
        associations at the port or node level using symmetric or asymmetric keys
        that are pre-installed.  FAM argues for security
        associations to be applied only at a group level or to be refined once
        the topology has been established.  RIFT does not specify how security
        keys are installed or updated it specifies how the key can be used to
        achieve goals. 
    </t>

        <t>The protocol has provisions for "weak" nonces to prevent replay attacks
            and includes authentication
            mechanisms comparable to <xref target="RFC5709"/> and
            <xref target="RFC7987"/>.
        </t>
 </section>

     <section title="Security Envelope" anchor="security-envelope">

     <t>
	RIFT MUST be carried in a mandatory secure envelope illustrated in
	<xref target="secenvelope"/>. Any value in the packet following a security fingerprint
         MUST be used only after the according fingerprint has been validated.
         </t>

         <t>Local
             configuration MAY allow to
             skip the checking of the envelope's integrity.
         </t>

     <t>
         <figure align="center" anchor="secenvelope" title="Security Envelope">

			    <artwork align="center"><![CDATA[
				   
    0                   1                   2                   3
    0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
  
   UDP Header:
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |           Source Port         |       RIFT destination port   |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |           UDP Length          |        UDP Checksum           |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

   Outer Security Envelope Header:
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |           RIFT MAGIC          |         Packet Number         |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |    Reserved   |  RIFT Major   | Outer Key ID  | Fingerprint   |
   |               |    Version    |               |    Length     |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   ~       Security Fingerprint covers all following content       ~
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   | Weak Nonce Local              | Weak Nonce Remote             |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |            Remaining TIE Lifetime (all 1s in case of LIE)     |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

   TIE Origin Security Envelope Header:
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |              TIE Origin Key ID                |  Fingerprint  |
   |                                               |    Length     |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   ~       Security Fingerprint covers all following content       ~
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

   Serialized RIFT Model Object
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
   |                                                               |
   ~                Serialized RIFT Model Object                   ~
   |                                                               |
   +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

        ]]>
                        </artwork>
		</figure>
	</t>
         <t>
             <list style='hanging'>

                 <t hangText="RIFT MAGIC:">16 bits. Constant value of 0xA1F7 that allows to
                     classify RIFT packets independent of used UDP port.
                 </t>

                 <t hangText="Packet Number:">16 bits. An optional, per packet type
                     monotonically growing number rolling over using
                     sequence number arithmetic defined in<xref target="arithmetic"/>. A node
                     SHOULD correctly set the number on subsequent
                     packets or otherwise MUST set the value to
                     `undefined_packet_number` as provided in the schema. This number can
                     be used to detect losses and misordering in flooding for either operational
                     purposes or in implementation to adjust flooding behavior to current
                     link or buffer quality. This number MUST NOT be used to discard or
                     validate the correctness of packets.
                 </t>

                 <t hangText="RIFT Major Version:">8 bits.
                     It allows to check whether
                     protocol versions are compatible, i.e. the serialized object can be decoded at
                     all.
                     An implementation MUST drop packets with unexpected value and MAY report a
                     problem. Must be same as in encoded model object, otherwise
                     packet is dropped.
                 </t>

                 <t hangText="Outer Key ID:">8 bits to allow key rollovers. This implies key type
                     and used
                     algorithm. Value 0 means that no valid fingerprint was computed. This key ID
                     scope is
                     local to the nodes on both ends of the adjacency.
                 </t>

                 <t hangText="TIE Origin Key ID:">24 bits. This implies key type
                     and used
                     algorithm. Value 0 means that no valid fingerprint was computed. This key ID
                     scope is
                     global to the RIFT instance since it implies the originator of the TIE so the
                     contained object does not have to be de-serialized to obtain it.
                 </t>

                 <t hangText="Length of Fingerprint:">8 bits. Length in 32-bit multiples of the
                     following
                     fingerprint not including lifetime or weak nonces.
                     It allows to navigate the structure when an unknown key type is present.
                     To clarify a common corner case when this value is set to 0 it signifies an
                     empty (0 bytes long) security fingerprint.
                 </t>

                 <t hangText="Security Fingerprint:">32 bits * Length of Fingerprint.
                     This is a signature that is computed over all data following
                     after it. If the significant bits of fingerprint are fewer than the 32 bits padded length
                     than the significant bits MUST be left
                     aligned and
                     remaining bits on the right padded with 0s.
                     When using PKI the Security fingerprint originating node uses its private key
                     to
                     create the
                     signature. The original packet can then be verified provided the public key is
                     shared
                     and current.
                 </t>
                 <t hangText="Remaining TIE Lifetime:">32 bits.
                     In case of anything but TIEs this field MUST be set to all ones and Origin
                     Security Envelope Header MUST NOT be present in the packet. For TIEs this field
                     represents the remaining lifetime of the TIE and Origin
                     Security Envelope Header MUST be present in the packet. The value in the
                     serialized model
                     object MUST be ignored.
                 </t>

                 <t hangText="Weak Nonce Local: ">16 bits. Local Weak Nonce of the adjacency as advertised in
                     LIEs.
                 </t>

                 <t hangText="Weak Nonce Remote: ">16 bits. Remote Weak Nonce of the adjacency as received in
                     LIEs.
                 </t>

                 <t hangText="TIE Origin Security Envelope Header:">It MUST be present if
                     and only if the Remaining TIE Lifetime
                     field is
                     NOT all ones. It carries through the originators key ID and according
                     fingerprint of the
                     object to protect TIE from modification during flooding. This ensures
                     origin validation and integrity
                     (but does not provide validation of a chain of trust).
                 </t>

             </list>
         </t>

         <t>Observe that due to the schema migration rules per <xref target="schema"/> the contained
             model can be always decoded if the major version matches and the envelope integrity
             has been validated. Consequently, description
             of the TIE is available to flood it properly including unknown TIE types.
         </t>
</section>

    <section title="Weak Nonces" anchor="nonces">
        <t>
            The protocol uses two 16 bit nonces to salt generated signatures. We use the
            term "nonce" a bit loosely since RIFT nonces are not being changed
            on every packet as common in cryptography. For efficiency purposes they
            are changed at a frequency high enough to dwarf
            replay attacks attempts for all practical purposes. Therefore,
            we call them "weak" nonces.

        </t>

            <t>Any implementation including RIFT security MUST generate and wrap around
            local nonces properly. When a nonce increment leads to `undefined_nonce` value
                the value SHOULD be incremented again immediately.
                All implementation MUST reflect the neighbor's nonces.
            An implementation SHOULD
            increment a chosen nonce on every LIE FSM transition that ends up in a different state
            from the previous and MUST increment its nonce at least
            every 5 minutes (such considerations
            allow for efficient implementations without opening a significant security risk).
            When flooding TIEs, the implementation MUST use recent (i.e. within allowed difference)
            nonces reflected in the LIE exchange.
            The schema specifies
            maximum allowable nonce value difference on a packet compared to reflected nonces in the
            LIEs. Any packet received with nonces deviating more than the allowed delta MUST be
            discarded without further computation of signatures to prevent computation load attacks.
        </t>
        <t>
            In case where a secure implementation does not receive signatures or
            receives undefined nonces from neighbor
            indicating that it does not support or verify signatures, it is a matter of local
            policy how such packets are treated. Any secure implementation MAY choose to either refuse
            forming an adjacency with an implementation not advertising signatures or valid
            nonces or simply keep on signing local packets while accepting neighbor's packets
            without further security verification.
        </t>
        <t>
            As a necessary exception, an implementation MUST advertise `undefined_nonce` for remote
            nonce value when the FSM is not in two-way or three-way state and accept an `undefined_nonce`
            for its local nonce value on packets in any other state than three-way.
        </t>
        <t>
            As optional optimization, an implementation MAY send one LIE with previously negotiated
            neighbor's nonce to try to
            speed up a neighbor's transition from three-way to one-way and MUST revert to sending
            `undefined_nonce` after that.
        </t>

    </section>

    <section title="Lifetime" anchor="sec-lifetime">
        <t>Protecting lifetime on flooding may lead to excessive number of security fingerprint
            computation and hence an application generating such fingerprints on TIEs MAY
            round the value down to the next `rounddown_lifetime_interval` defined in the schema
            when sending TIEs albeit such optimization in presence of security hashes over advancing
            weak nonces may not be feasible.
        </t>
    </section>

    <section title="Key Management" anchor="key-management">
    <t>
        As outlined in the Security Model a private shared key or a
        public/private key pair is used to
        Authenticate the adjacency.  The actual method of key distribution
        and key synchronization
	is assumed to be out of band from  RIFT's perspective. Both nodes in the adjacency
	must share the same keys  and configuration of key type and algorithm
        for a key ID.
	Mismatched keys will obviously not inter-operate due to unverifiable
        security envelope.

    </t>
           
    <t>
	Key roll-over while the adjacency is active
	is allowed and the technique is well known and described in e.g. <xref target="RFC6518"/>.
	Key distribution procedures are out of scope for RIFT.
    </t>
        
    </section>
       <section title=" Security Association Changes" anchor="security-association">
    <t>
        There in no mechanism to convert a security envelope for the same key ID
        from one algorithm  to
	another once the envelope is operational.  The recommended procedure to change to
	a new algorithm is
        to take the adjacency down and make the changes and then bring the
        adjacency up.  Obviously, an implementation MAY choose to stop verifying
        security envelope for the duration of key change to keep the adjacency up
        but since this introduces
        a security vulnerability window, such roll-over is not recommended.
    </t>
	
    </section>
</section>
</section>

<section title="Examples">

        <section title="Normal Operation">


            <t>This section describes RIFT deployment in the example topology
                without any node or link failures. We disregard flooding
                reduction for simplicity's sake.
                </t>

<t>As first step, the following bi-directional adjacencies will be created
    (and any other links that do not fulfill LIE rules in <xref target="LIE"></xref>
     disregarded):


    <list style="numbers">
        <t>ToF 21 (PoD 0) to Spine 111, Spine 112, Spine 121, and Spine 122</t>

        <t>ToF 22 (PoD 0) to Spine 111, Spine 112, Spine 121, and Spine 122</t>

        <t>Spine 111 to Leaf 111, Leaf 112</t>

        <t>Spine 112 to Leaf 111, Leaf 112</t>

        <t>Spine 121 to Leaf 121, Leaf 122</t>
        
        <t>Spine 122 to Leaf 121, Leaf 122</t>

        </list>
                </t>

<t>Consequently, North TIEs would be originated by Spine 111 and Spine 112 and
    each set would be sent to both ToF 21 and ToF 22.
    North TIEs also would be originated by Leaf 111 (w/ Prefix 111) and Leaf 112
    (w/ Prefix 112 and the multi-homed prefix)
    and each set would be sent to Spine 111 and Spine 112.
    Spine 111 and Spine 112 would then flood these North TIEs to ToF 21
    and ToF 22.
    </t>

<t>
    Similarly, North TIEs would be originated by Spine 121 and Spine 122 and
    each set would be sent to both ToF 21 and ToF 22.
    North TIEs also would be originated by Leaf 121 (w/ Prefix 121 and the
    multi-homed prefix) and Leaf 122
    (w/ Prefix 122) and each set would be sent to Spine 121 and Spine 122.
    Spine 121 and Spine 122 would then flood these North TIEs to ToF 21
    and ToF 22.
    </t>

<t>At this point both ToF 21 and ToF 22, as well as any controller to
    which they are connected, would have the complete network topology.
    At the same time, Spine 111/112/121/122 hold only the N-ties of
    level 0 of their respective PoD. leaves hold only their own North TIEs.
    </t>

<t>South TIEs with adjacencies and
    a default IP prefix would then be originated by ToF 21 and
    ToF 22 and each would be flooded to Spine 111, Spine 112, Spine 121, and
    Spine 122.  Spine 111, Spine 112, Spine 121, and Spine 122 would each
    send the South TIE from ToF 21 to ToF 22 and the South TIE from ToF 22 to
    ToF 21.  (South TIEs are reflected up to level from which they are received
    but they are NOT propagated southbound.)
    </t>

<t>A South TIE with a default IP prefix would be originated by Node
    111 and Spine 112 and each would be sent to Leaf 111 and Leaf 112.
    </t>

<t>Similarly, an South TIE with a default IP prefix would be originated by Node
    121 and Spine 122 and each would be sent to Leaf 121 and Leaf 122.

    At this point IP connectivity with maximum possible ECMP has been
    established between the leaves while constraining the amount of
    information held by each node
    to the minimum necessary for normal operation and dealing with failures.
    </t>


                   </section>

        <section title="Leaf Link Failure">

            <t>

                <figure align="center" anchor="pic-one-link-fail"
                    title="Single Leaf link failure">
                    <artwork align="center"><![CDATA[
.  |   |              |   |
.+-+---+-+          +-+---+-+
.|       |          |       |
.|Spin111|          |Spin112|
.+-+---+-+          ++----+-+
.  |   |             |    |
.  |   +---------------+  X
.  |                 | |  X Failure
.  |   +-------------+ |  X
.  |   |               |  |
.+-+---+-+          +--+--+-+
.|       |          |       |
.|Leaf111|          |Leaf112|
.+-------+          +-------+
.      +                  +
.     Prefix111     Prefix112
                    ]]>
                    </artwork>
                </figure>
            </t>


            <t>In case of a failing leaf link between spine 112 and leaf 112
                the link-state
                information will cause re-computation of the necessary SPF
                and the higher levels will
                stop forwarding towards prefix 112 through spine 112. Only
                spines 111 and 112, as well
                as both spines will see control traffic. Leaf 111 will
                receive a new South TIE
                from spine 112 and reflect back to spine 111.
                <!--
                 The link state information allows for maximum
                 convergence speed on failures and could be used to
                 provide sophisticated load balancing based on the available ECMP degree
                 in lower levels. Imagine ToF 21 sending a packet south to Leaf112 whereas
                 the link Spine 112->Leaf112 failed. To ensure saturation of the remaining
                 three links south, it could divide the traffic amongst Spine 112 and
                 Spine 111 in ratio 1:2.</t>
                 -->

                Spine 111 will de-aggregate prefix 111 and prefix 112 but
                we will not describe it further here
                since de-aggregation is emphasized
                in the next example. It is worth observing
                however
                in this example that if leaf 111 would keep on forwarding traffic towards
                prefix 112 using the advertised south-bound default of spine 112
                the traffic would end up on Top-of-Fabric 21 and ToF 22 and cross back
                into pod 1 using spine 111. This is arguably
                not as bad as black-holing
                present in the next example but clearly undesirable.
                Fortunately, de-aggregation prevents this type of behavior except
                for a transitory period of time.

            </t>
        </section>

        <section title="Partitioned Fabric" anchor="fabriccut">
            <t>

                <figure align="center" anchor="pic-part-fabric" title="Fabric partition">
                    <artwork align="center"><![CDATA[
.                +--------+          +--------+   South TIE of ToF 21
.                |        |          |        |   received by
.                |ToF   21|          |ToF   22|   south reflection of
.                ++-+--+-++          ++-+--+-++   spines 112 and 111
.                 | |  | |            | |  | |
.                 | |  | |            | |  | 0/0
.                 | |  | |            | |  | |
.                 | |  | |            | |  | |
.  +--------------+ |  +--- XXXXXX +  | |  | +---------------+
.  |                |    |         |  | |  |                 |
.  |    +-----------------------------+ |  |                 |
.  0/0  |           |    |         |    |  |                 |
.  |    0/0       0/0    +- XXXXXXXXXXXXXXXXXXXXXXXXX -+     |
.  |  1.1/16        |              |    |  |           |     |
.  |    |           +-+    +-0/0-----------+           |     |
.  |    |             |   1.1./16  |    |              |     |
.+-+----++          +-+-----+     ++-----0/0          ++----0/0
.|       |          |       |     |    1.1/16         |   1.1/16
.|Spin111|          |Spin112|     |Spin121|           |Spin122|
.+-+---+-+          ++----+-+     +-+---+-+           ++---+--+
.  |   |             |    |         |   |              |   |
.  |   +---------------+  |         |   +----------------+ |
.  |                 | |  |         |                  | | |
.  |   +-------------+ |  |         |   +--------------+ | |
.  |   |               |  |         |   |                | |
.+-+---+-+          +--+--+-+     +-+---+-+          +---+-+-+
.|       |          |       |     |       |          |       |
.|Leaf111|          |Leaf112|     |Leaf121|          |Leaf122|
.+-+-----+          ++------+     +-----+-+          +-+-----+
.  +                 +                  +              +
.  Prefix111    Prefix112             Prefix121     Prefix122
.                                       1.1/16
                    ]]>
                    </artwork>
                </figure>
            </t>

            <t>

                <xref target="pic-part-fabric"></xref> shows the arguably a more
                catastrophic but also a more interesting case. ToF 21 is
                completely severed from access to Prefix 121 (we use in the figure
                1.1/16 as example) by double link failure.
                However unlikely, if left
                unresolved, forwarding from leaf 111 and leaf 112 to prefix 121 would
                suffer 50% black-holing based on pure default route
                advertisements by ToF 21
                and ToF 22.
            </t>


            <t>
                The mechanism used to resolve this scenario is hinging on the
                distribution of southbound representation by Top-of-Fabric 21 that is
                reflected by spine 111 and spine 112 to ToF 22. ToF 22,
                having computed reachability to all prefixes in the network,
                advertises with the default route
                the ones that are reachable only via lower level
                neighbors that ToF 21 does not show an adjacency to. That
                results
                in spine 111 and spine 112 obtaining a longest-prefix match
                to prefix 121 which leads through ToF 22 and prevents black-holing
                through ToF 21 still advertising the 0/0 aggregate only.
            </t>

            <t>The prefix 121 advertised by Top-of-Fabric 22 does not have
                to be propagated further towards leaves since they do
                no benefit from this information. Hence the amount of flooding is
                restricted to ToF 21 reissuing its South TIEs
                and south reflection of those by spine 111 and spine 112. The resulting
                SPF in ToF 22 issues a new prefix South TIEs containing 1.1/16. None of
                the leaves become aware of the changes and the failure is
                constrained strictly to the level that became partitioned.

            </t>

            <t>To finish with an example of the resulting sets computed using notation
                    introduced in <xref target="disaggregate"/>,
                    Top-of-Fabric 22 constructs the following sets:
                    </t>
<t>
                <list>
                    <t>|R = Prefix 111, Prefix 112, Prefix 121, Prefix 122</t>

                    <t>|H (for r=Prefix 111) = Spine 111, Spine 112</t>

                    <t>|H (for r=Prefix 112) = Spine 111, Spine 112</t>

                   <t>|H (for r=Prefix 121) = Spine 121, Spine 122</t>

                    <t>|H (for r=Prefix 122) = Spine 121, Spine 122</t>

                    <t>|A (for ToF 21) = Spine 111, Spine 112</t>
</list>
</t>
                <t>With that and |H (for r=prefix 121) and |H (for r=prefix 122)
                    being disjoint from |A (for Top-of-Fabric 21), ToF 22 will
                    originate an South TIE with prefix 121 and prefix 122,
                    that is flooded to spines 112, 112, 121 and 122.
                </t>

        </section>

        <section title="Northbound Partitioned Router and Optional East-West Links"
            anchor="onastickexample">



            <t>
                </t>
            <t>
            <figure align="center" anchor="north-part-node"
                title="North Partitioned Router">

                <artwork align="center"><![CDATA[
.   +                  +                  +
.   X N1               | N2               | N3
.   X                  |                  |
.+--+----+          +--+----+          +--+-----+
.|       |0/0>  <0/0|       |0/0>  <0/0|        |
.|  A01  +----------+  A02  +----------+  A03   | Level 1
.++-+-+--+          ++--+--++          +---+-+-++
. | | |              |  |  |               | | |
. | | +----------------------------------+ | | |
. | |                |  |  |             | | | |
. | +-------------+  |  |  |  +--------------+ |
. |               |  |  |  |  |          | |   |
. | +----------------+  |  +-----------------+ |
. | |             |     |     |          | | | |
. | | +------------------------------------+ | |
. | | |           |     |     |          |   | |
.++-+-+--+        | +---+---+ |        +-+---+-++
.|       |        +-+       +-+        |        |
.|  L01  |          |  L02  |          |  L03   | Level 0
.+-------+          +-------+          +--------+
                ]]>
                </artwork>
            </figure>
</t>

            <t>
                <xref target="north-part-node"/> shows a part of a fabric where
                level 1 is horizontally connected and A01 lost its only northbound
                adjacency. Based on N-SPF rules in <xref target="nspf"/> A01 will
                compute northbound reachability by using the link A01 to A02 (whereas
                A02 will NOT use this link during N-SPF). Hence A01 will still
                advertise the default towards level 0 and route unidirectionally
                using the horizontal link.</t>

<t>
                As further consideration, the moment A02 looses link N2 the situation
                evolves again. A01 will have no more northbound reachability while
                still seeing A03 advertising northbound adjacencies in its
                south node tie. With that it will stop advertising a default
                route due to <xref target="defaultrouterules"/>.
                </t>

            </section>




</section>


<section title="Implementation and Operation: Further Details">

    <section title="Considerations for Leaf-Only Implementation">

        <t> RIFT can and is intended to be stretched  to the lowest level in the IP fabric to
            integrate ToRs or even servers. Since those entities would run as leaves
            only, it is worth to observe that a leaf only version is significantly
            simpler to implement and requires much less resources:
        </t>

        <t>
            <list style="numbers">
                <t>Under normal conditions, the leaf needs to support a multipath
                    default route only. In most catastrophic partitioning
                    case it has to be capable of accommodating
                    all the leaf routes in its own PoD to prevent black-holing.</t>
                <t>Leaf nodes hold only their own North TIEs and South TIEs of Level 1 nodes
                    they are connected to; so overall few in numbers.</t>
                <t>Leaf node does not have to support any type
                    of de-aggregation computation or propagation.</t>
                <t>Leaf nodes do not have to support overload bit normally.</t>
                <t>Unless optional leaf-2-leaf procedures are desired
                    default route origination and South TIE origination is
                    unnecessary.</t>
            </list>
        </t>
        

    </section>

    <section title="Considerations for Spine Implementation">

        <t>In case of spines, i.e. nodes that will never act as Top of Fabric
            a full implementation is not required, specifically the node
            does not need to perform any computation of negative disaggregation except
            respecting northbound disaggregation advertised from the north.


        </t>

    </section>


<section title="Adaptations to Other Proposed Data Center Topologies">

<t>
    <figure align="center" anchor="levelshortcuts" title="Level Shortcut">

        <artwork align="center"><![CDATA[
.  +-----+        +-----+
.  |     |        |     |
.+-+ S0  |        | S1  |
.| ++---++        ++---++
.|  |   |          |   |
.|  | +------------+   |
.|  | | +------------+ |
.|  | |              | |
.| ++-+--+        +--+-++
.| |     |        |     |
.| | A0  |        | A1  |
.| +-+--++        ++---++
.|   |  |          |   |
.|   |  +------------+ |
.|   | +-----------+ | |
.|   | |             | |
.| +-+-+-+        +--+-++
.+-+     |        |     |
.  | L0  |        | L1  |
.  +-----+        +-----+
        ]]>
        </artwork>
    </figure>
</t>


    <t>
        Strictly speaking, RIFT is not limited to Clos variations only. The protocol
        preconditions only a sense of 'compass rose direction' achieved by
        configuration (or derivation) of levels and other topologies are possible within this
        framework. So, conceptually,
    one could include leaf to leaf links and even shortcut between levels

        <!-- based on Hardwick review
        but
    certain requirements in <xref target="reqs"/> will not be met anymore.
    -->

    As an example, short cutting
    levels illustrated in <xref target="levelshortcuts"/> will lead either to suboptimal
    routing when L0 sends traffic to L1 (since using S0's default route will lead to
    the traffic being sent back to A0 or A1) or the leaves need each other's routes
    installed to understand that only A0 and A1 should be used to talk to each other.
    </t>

    <t>Whether such modifications of topology constraints make sense is dependent on many technology
        variables and the exhausting treatment of the topic is definitely outside the scope of
        this document.
        </t>

</section>

<section title="Originating Non-Default Route Southbound">

    <t>Obviously, an implementation MAY choose to originate southbound instead of a strict
        default route (as described in <xref target="defaultrouterules"/>) a shorter
        prefix P' but in such a scenario
        all addresses carried within the RIFT domain must be contained within P'.</t>

</section>

</section>

<section title="Security Considerations" anchor="security">
    <section title="General">
    <t>One can consider attack vectors where a router
        may reboot many times while changing its system ID and pollute
        the network with many stale TIEs or TIEs are sent with very long
        lifetimes and not cleaned up when the routes vanishes.
        Those attack vectors are not unique to RIFT.
        Given large memory footprints
        available today those attacks should be relatively benign.  Otherwise
        a node SHOULD implement a strategy of discarding contents of all TIEs
        that were not present in the SPF tree over a certain, configurable
        period of time. Since the protocol, like all modern link-state
        protocols, is self-stabilizing and will advertise the presence
        of such TIEs to its neighbors, they can be
        re-requested again if a computation finds that it sees an
        adjacency formed towards the system ID of the discarded
        TIEs.
    </t>
    </section>

    <section title="ZTP">
    <t><xref target="ZTP"/> presents many attack vectors in untrusted environments, starting
        with nodes that oscillate their level offers to the possibility of a node offering
        a three-way adjacency with the highest possible level value with a very long holdtime
        trying to put itself "on top of the lattice" and with
        that gaining access to the whole southbound topology.
        Session authentication mechanisms
        are necessary in
        environments where this is possible and RIFT provides the according security envelope
        to ensure this if desired.
    </t>
    </section>

        <section title="Lifetime">
            <t>Traditional
                IGP protocols are vulnerable to lifetime modification and replay attacks that can
                be somewhat mitigated by using techniques like <xref target="RFC7987"/>.
                RIFT removes this attack vector by protecting the lifetime behind a signature
                computed over it and additional nonce combination which makes even
                the replay attack window very small and for practical purposes irrelevant
                since lifetime cannot be artificially shortened by the attacker.
            </t>
        </section>

    <section title="Packet Number">
        <t>
            Optional packet number is carried in the security envelope without any encryption
            protection and is hence vulnerable to replay and modification attacks.
            Contrary to nonces this number must change on every packet and
            would present a very high cryptographic load if signed. The attack vector
            packet number present is
            relatively benign. Changing the packet number by a man-in-the-middle attack
            will only
            affect operational validation tools and possibly some performance
            optimizations on flooding. It is expected that an implementation detecting
            too many "fake losses" or "misorderings" due to the attack on the packet
            number would
            simply suppress its further processing.
        </t>

    </section>

    <section title="Outer Fingerprint Attacks">
        <t>
            A node can try to inject LIE packets observing a conversation on the wire by
            using the outer key ID albeit it cannot generate valid hashes in case it changes
            the integrity of the message so the only possible attack is DoS due to excessive
            LIE validation.
        </t>
        <t>
            A node can try to replay previous LIEs with changed state that it recorded
            but the attack is hard to replicate since the nonce combination must match
            the ongoing exchange and is then limited to a single flap only since both
            nodes will advance their nonces in case the adjacency state changed. Even in the
            most unlikely case the attack length is limited due to both sides periodically
            increasing their nonces.
        </t>
    </section>

    <section title="TIE Origin Fingerprint DoS Attacks">
        <t>
                A compromised node can attempt to generate "fake TIEs" using other nodes'
                TIE origin key identifiers. Albeit the ultimate validation of the origin fingerprint
                will fail in such scenarios and not progress further than immediately
                peering nodes, the resulting denial of service attack seems
                unavoidable since the TIE origin key id is only protected by the, here assumed
                to be compromised, node.
        </t>
    </section>

    <section title="Host Implementations">
        <t>
            It can be reasonably expected that with the proliferation of RotH
            servers, rather than dedicated networking devices,
            will constitute significant amount of  RIFT devices.
            Given their normally far wider software envelope and access granted to them, such
            servers are
            also far more likely to be compromised and present an attack vector on the protocol.
            Hijacking of prefixes to attract traffic is a trust problem and cannot be addressed
            within the protocol if the trust model is breached, i.e. the server presents valid
            credentials to form an adjacency and issue TIEs. However, in a move devious way,
            the servers can present
            DoS (or even DDos) vectors of issuing too many LIE packets, flood large amount of North TIEs
            and similar anomalies. A prudent implementation hosting leaves should implement
            thresholds and raise warnings when leaf is advertising
            number of TIEs in excess of those.
        </t>
    </section>
</section>



<?rfc needLines="8" ?>


<section anchor="IANA" title="IANA Considerations">

    <t>This specification requests multicast address assignments and standard port numbers. Additionally
        registries for the schema are requested and suggested values provided that reflect the numbers
        allocated in the given schema.
    </t>

    <section title="Requested Multicast and Port Numbers">
        <t>This document requests allocation in the 'IPv4 Multicast Address Space' registry the
            suggested value of 224.0.0.120 as 'ALL_V4_RIFT_ROUTERS' and in the 'IPv6 Multicast
            Address Space' registry the
            suggested value of FF02::A1F7 as 'ALL_V6_RIFT_ROUTERS'.
        </t>

        <t>
            This document requests allocation in the 'Service Name and Transport Protocol Port Number Registry'
            the allocation of a suggested value of 914 on udp for 'RIFT_LIES_PORT' and
        suggested value of 915 for
        'RIFT_TIES_PORT'.
        </t>
    </section>

    <section title="Requested Registries with Suggested Values">

        <t>
            This section requests registries that help govern the schema via usual IANA registry
            procedures. A top level 'RIFT' registry should hold the according registries requested in
            following sections with their pre-defined values.
            IANA is requested to store the schema version introducing the allocated value as well
            as, optionally, its description when present. This will allow to assign different values
            to an entry depending on schema version. Alternately, IANA is requested to consider a root
            RIFT/2 registry to store RIFT schema major version 2 values and may be requested in the
            future to create a RIFT/3 registry under that. In any case, IANA is requested to
            store the schema version in the entries since that will allow to distinguish between
            minor versions in the same major schema version. All values not suggested as to be
            considered `Unassigned`. The range of every registry is a 16-bit integer.
            Allocation of new values is always performed via `Expert Review` action.

        </t>

        <!-- generated request -->

        <section title="Registry RIFT/common/AddressFamilyType">

            <t>Address family type.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>Illegal</c>	<c>0</c>	<c>2.0</c>	<c></c>
                    <c>AddressFamilyMinValue</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>IPv4</c>	<c>2</c>	<c>2.0</c>	<c></c>
                    <c>IPv6</c>	<c>3</c>	<c>2.0</c>	<c></c>
                    <c>AddressFamilyMaxValue</c>	<c>4</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/HierarchyIndications">

            <t>Flags indicating node configuration in case of ZTP.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>leaf_only</c>	<c>0</c>	<c>2.0</c>	<c></c>
                    <c>leaf_only_and_leaf_2_leaf_procedures</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>top_of_fabric</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/IEEE802_1ASTimeStampType">

            <t>Timestamp per IEEE 802.1AS, all values MUST be interpreted in implementation as unsigned.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>AS_sec</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>AS_nsec</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/IPAddressType">

            <t>IP address type.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>ipv4address</c>	<c>1</c>	<c>2.0</c>	<c> Content is IPv4</c>
                    <c>ipv6address</c>	<c>2</c>	<c>2.0</c>	<c> Content is IPv6</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/IPPrefixType">

            <t>Prefix advertisement.</t>
            <t>@note: for interface
                addresses the protocol can propagate the address part beyond
                the subnet mask and on reachability computation that has to
                be normalized. The non-significant bits can be used for operational
                purposes.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>ipv4prefix</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>ipv6prefix</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/IPv4PrefixType">

            <t>IPv4 prefix type.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>address</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>prefixlen</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/IPv6PrefixType">

            <t>IPv6 prefix type.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>address</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>prefixlen</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/PrefixSequenceType">

            <t>Sequence of a prefix in case of move.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>timestamp</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>transactionid</c>	<c>2</c>	<c>2.0</c>	<c> Transaction ID set by client in e.g. in 6LoWPAN.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/RouteType">

            <t>RIFT route types.</t>
            <t>@note: route types which MUST be ordered on their preference
                PGP prefixes are most preferred attracting
                traffic north (towards spine) and then south
                normal prefixes are attracting traffic south (towards leaves),
                i.e. prefix in NORTH PREFIX TIE is preferred over SOUTH PREFIX TIE.</t>
            <t>@note: The only purpose of those values is to introduce an
                ordering whereas an implementation can choose internally
                any other values as long the ordering is preserved</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>Illegal</c>	<c>0</c>	<c>2.0</c>	<c></c>
                    <c>RouteTypeMinValue</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>Discard</c>	<c>2</c>	<c>2.0</c>	<c></c>
                    <c>LocalPrefix</c>	<c>3</c>	<c>2.0</c>	<c></c>
                    <c>SouthPGPPrefix</c>	<c>4</c>	<c>2.0</c>	<c></c>
                    <c>NorthPGPPrefix</c>	<c>5</c>	<c>2.0</c>	<c></c>
                    <c>NorthPrefix</c>	<c>6</c>	<c>2.0</c>	<c></c>
                    <c>NorthExternalPrefix</c>	<c>7</c>	<c>2.0</c>	<c></c>
                    <c>SouthPrefix</c>	<c>8</c>	<c>2.0</c>	<c></c>
                    <c>SouthExternalPrefix</c>	<c>9</c>	<c>2.0</c>	<c></c>
                    <c>NegativeSouthPrefix</c>	<c>10</c>	<c>2.0</c>	<c></c>
                    <c>RouteTypeMaxValue</c>	<c>11</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/TIETypeType">

            <t>Type of TIE.</t>
            <t>This enum indicates what TIE type the TIE is carrying.
                In case the value is not known to the receiver,
                the TIE MUST be re-flooded. This allows for
                future extensions of the protocol within the same major schema
                with types opaque to some nodes UNLESS the flooding scope is not
                the same as prefix TIE, then a major version revision MUST
                be performed.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>Illegal</c>	<c>0</c>	<c>2.0</c>	<c></c>
                    <c>TIETypeMinValue</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>NodeTIEType</c>	<c>2</c>	<c>2.0</c>	<c></c>
                    <c>PrefixTIEType</c>	<c>3</c>	<c>2.0</c>	<c></c>
                    <c>PositiveDisaggregationPrefixTIEType</c>	<c>4</c>	<c>2.0</c>	<c></c>
                    <c>NegativeDisaggregationPrefixTIEType</c>	<c>5</c>	<c>2.0</c>	<c></c>
                    <c>PGPrefixTIEType</c>	<c>6</c>	<c>2.0</c>	<c></c>
                    <c>KeyValueTIEType</c>	<c>7</c>	<c>2.0</c>	<c></c>
                    <c>ExternalPrefixTIEType</c>	<c>8</c>	<c>2.0</c>	<c></c>
                    <c>PositiveExternalDisaggregationPrefixTIEType</c>	<c>9</c>	<c>2.0</c>	<c></c>
                    <c>TIETypeMaxValue</c>	<c>10</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/common/TieDirectionType">

            <t>Direction of TIEs.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>Illegal</c>	<c>0</c>	<c>2.0</c>	<c></c>
                    <c>South</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>North</c>	<c>2</c>	<c>2.0</c>	<c></c>
                    <c>DirectionMaxValue</c>	<c>3</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/Community">

            <t>Prefix community.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>top</c>	<c>1</c>	<c>2.0</c>	<c> Higher order bits</c>
                    <c>bottom</c>	<c>2</c>	<c>2.0</c>	<c> Lower order bits</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/KeyValueTIEElement">

            <t>Generic key value pairs.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>keyvalues</c>	<c>1</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/LIEPacket">

            <t>RIFT LIE Packet.</t>
            <t>@note: this node's level is already included on the packet header</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>name</c>	<c>1</c>	<c>2.0</c>	<c> Node or adjacency name.</c>
                    <c>local_id</c>	<c>2</c>	<c>2.0</c>	<c> Local link ID.</c>
                    <c>flood_port</c>	<c>3</c>	<c>2.0</c>	<c> UDP port to which we can receive flooded TIEs.</c>
                    <c>link_mtu_size</c>	<c>4</c>	<c>2.0</c>	<c> Layer 3 MTU, used to discover to mismatch.</c>
                    <c>link_bandwidth</c>	<c>5</c>	<c>2.0</c>	<c> Local link bandwidth on the interface.</c>
                    <c>neighbor</c>	<c>6</c>	<c>2.0</c>	<c> Reflects the neighbor once received to provide
                    3-way connectivity.</c>
                    <c>pod</c>	<c>7</c>	<c>2.0</c>	<c> Node's PoD.</c>
                    <c>node_capabilities</c>	<c>10</c>	<c>2.0</c>	<c> Node capabilities shown in the LIE. The capabilities
                    MUST match the capabilities shown in the Node TIEs, otherwise
                    the behavior is unspecified. A node detecting the mismatch
                    SHOULD generate according error.</c>
                    <c>link_capabilities</c>	<c>11</c>	<c>2.0</c>	<c> Capabilities of this link.</c>
                    <c>holdtime</c>	<c>12</c>	<c>2.0</c>	<c> Required holdtime of the adjacency, i.e. how much time
                    MUST expire without LIE for the adjacency to drop.</c>
                    <c>label</c>	<c>13</c>	<c>2.0</c>	<c> Unsolicited, downstream assigned locally significant label
                    value for the adjacency.</c>
                    <c>not_a_ztp_offer</c>	<c>21</c>	<c>2.0</c>	<c> Indicates that the level on the LIE MUST NOT be used
                    to derive a ZTP level by the receiving node.</c>
                    <c>you_are_flood_repeater</c>	<c>22</c>	<c>2.0</c>	<c> Indicates to northbound neighbor that it should
                    be reflooding this node's N-TIEs to achieve flood reduction and
                    balancing for northbound flooding. To be ignored if received from a
                    northbound adjacency.</c>
                    <c>you_are_sending_too_quickly</c>	<c>23</c>	<c>2.0</c>	<c> Can be optionally set to indicate to neighbor that packet losses are seen on
                    reception based on packet numbers or the rate is too high. The receiver SHOULD
                    temporarily slow down flooding rates.</c>
                    <c>instance_name</c>	<c>24</c>	<c>2.0</c>	<c> Instance name in case multiple RIFT instances running on same interface.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/LinkCapabilities">

            <t>Link capabilities.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>bfd</c>	<c>1</c>	<c>2.0</c>	<c> Indicates that the link is supporting BFD.</c>
                    <c>v4_forwarding_capable</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/LinkIDPair">

            <t>LinkID pair describes one of parallel links between two nodes.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>local_id</c>	<c>1</c>	<c>2.0</c>	<c> Node-wide unique value for the local link.</c>
                    <c>remote_id</c>	<c>2</c>	<c>2.0</c>	<c> Received remote link ID for this link.</c>
                    <c>platform_interface_index</c>	<c>10</c>	<c>2.0</c>	<c> Describes the local interface index of the link.</c>
                    <c>platform_interface_name</c>	<c>11</c>	<c>2.0</c>	<c> Describes the local interface name.</c>
                    <c>trusted_outer_security_key</c>	<c>12</c>	<c>2.0</c>	<c> Indication whether the link is secured, i.e. protected by outer key, absence
                    of this element means no indication, undefined outer key means not secured.</c>
                    <c>bfd_up</c>	<c>13</c>	<c>2.0</c>	<c> Indication whether the link is protected by established BFD session.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/Neighbor">

            <t>Neighbor structure.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>originator</c>	<c>1</c>	<c>2.0</c>	<c> System ID of the originator.</c>
                    <c>remote_id</c>	<c>2</c>	<c>2.0</c>	<c> ID of remote side of the link.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/NodeCapabilities">

            <t>Capabilities the node supports.</t>
            <t>@note: The schema may add to this
                field future capabilities to indicate whether it will support
                interpretation of future schema extensions on the same major
                revision. Such fields MUST be optional and have an implicit or
                explicit false default value. If a future capability changes route
                selection or generates blackholes if some nodes are not supporting
                it then a major version increment is unavoidable.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>protocol_minor_version</c>	<c>1</c>	<c>2.0</c>	<c> Must advertise supported minor version dialect that way.</c>
                    <c>flood_reduction</c>	<c>2</c>	<c>2.0</c>	<c> Can this node participate in flood reduction.</c>
                    <c>hierarchy_indications</c>	<c>3</c>	<c>2.0</c>	<c> Does this node restrict itself to be top-of-fabric or
                    leaf only (in ZTP) and does it support leaf-2-leaf procedures.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/NodeFlags">

            <t>Indication flags of the node.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>overload</c>	<c>1</c>	<c>2.0</c>	<c> Indicates that node is in overload, do not transit traffic through it.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/NodeNeighborsTIEElement">

            <t>neighbor of a node</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>level</c>	<c>1</c>	<c>2.0</c>	<c> level of neighbor</c>
                    <c>cost</c>	<c>3</c>	<c>2.0</c>	<c></c>
                    <c>link_ids</c>	<c>4</c>	<c>2.0</c>	<c> can carry description of multiple parallel links in a TIE</c>
                    <c>bandwidth</c>	<c>5</c>	<c>2.0</c>	<c> total bandwidth to neighbor, this will be normally sum of the
                    bandwidths of all the parallel links.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/NodeTIEElement">

            <t>Description of a node.</t>
            <t>It may occur multiple times in different TIEs but if either
                <list>
                    <t>capabilities values do not match or</t>
                    <t>flags values do not match or</t>
                    <t>neighbors repeat with different values</t>
                </list></t>
            <t>the behavior is undefined and a warning SHOULD be generated.
                Neighbors can be distributed across multiple TIEs however if
                the sets are disjoint. Miscablings SHOULD be repeated in every
                node TIE, otherwise the behavior is undefined.</t>
            <t>@note: Observe that absence of fields implies defined defaults.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>level</c>	<c>1</c>	<c>2.0</c>	<c> Level of the node.</c>
                    <c>neighbors</c>	<c>2</c>	<c>2.0</c>	<c> Node's neighbors. If neighbor systemID repeats in other node TIEs of
                    same node the behavior is undefined.</c>
                    <c>capabilities</c>	<c>3</c>	<c>2.0</c>	<c> Capabilities of the node.</c>
                    <c>flags</c>	<c>4</c>	<c>2.0</c>	<c> Flags of the node.</c>
                    <c>name</c>	<c>5</c>	<c>2.0</c>	<c> Optional node name for easier operations.</c>
                    <c>pod</c>	<c>6</c>	<c>2.0</c>	<c> PoD to which the node belongs.</c>
                    <c>miscabled_links</c>	<c>10</c>	<c>2.0</c>	<c> If any local links are miscabled, the indication is flooded.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/PacketContent">

            <t>Content of a RIFT packet.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>lie</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>tide</c>	<c>2</c>	<c>2.0</c>	<c></c>
                    <c>tire</c>	<c>3</c>	<c>2.0</c>	<c></c>
                    <c>tie</c>	<c>4</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/PacketHeader">

            <t>Common RIFT packet header.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>major_version</c>	<c>1</c>	<c>2.0</c>	<c> Major version of protocol.</c>
                    <c>minor_version</c>	<c>2</c>	<c>2.0</c>	<c> Minor version of protocol.</c>
                    <c>sender</c>	<c>3</c>	<c>2.0</c>	<c> Node sending the packet, in case of LIE/TIRE/TIDE
                    also the originator of it.</c>
                    <c>level</c>	<c>4</c>	<c>2.0</c>	<c> Level of the node sending the packet, required on everything except
                    LIEs. Lack of presence on LIEs indicates UNDEFINED_LEVEL and is used
                    in ZTP procedures.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/PrefixAttributes">

            <t>Attributes of a prefix.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>metric</c>	<c>2</c>	<c>2.0</c>	<c> Distance of the prefix.</c>
                    <c>tags</c>	<c>3</c>	<c>2.0</c>	<c> Generic unordered set of route tags, can be redistributed to other protocols or use
                    within the context of real time analytics.</c>
                    <c>monotonic_clock</c>	<c>4</c>	<c>2.0</c>	<c> Monotonic clock for mobile addresses.</c>
                    <c>loopback</c>	<c>6</c>	<c>2.0</c>	<c> Indicates if the interface is a node loopback.</c>
                    <c>directly_attached</c>	<c>7</c>	<c>2.0</c>	<c> Indicates that the prefix is directly attached, i.e. should be routed to even if
                    the node is in overload. *</c>
                    <c>from_link</c>	<c>10</c>	<c>2.0</c>	<c> In case of locally originated prefixes, i.e. interface addresses this can
                    describe which link the address belongs to.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/PrefixTIEElement">

            <t>TIE carrying prefixes</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>prefixes</c>	<c>1</c>	<c>2.0</c>	<c> Prefixes with the associated attributes.
                    If the same prefix repeats in multiple TIEs of same node
                    behavior is unspecified.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/ProtocolPacket">

            <t>RIFT packet structure.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>header</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>content</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIDEPacket">

            <t>TIDE with sorted TIE headers, if headers are unsorted, behavior is undefined.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>start_range</c>	<c>1</c>	<c>2.0</c>	<c> First TIE header in the tide packet.</c>
                    <c>end_range</c>	<c>2</c>	<c>2.0</c>	<c> Last TIE header in the tide packet.</c>
                    <c>headers</c>	<c>3</c>	<c>2.0</c>	<c> _Sorted_ list of headers.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIEElement">

            <t>Single element in a TIE.</t>
            <t>Schema enum `common.TIETypeType`
                in TIEID indicates which elements MUST be present
                in the TIEElement. In case of mismatch the unexpected
                elements MUST be ignored. In case of lack of expected
                element the TIE an error MUST be reported and the TIE
                MUST be ignored.</t>
            <t>This type can be extended with new optional elements
                for new `common.TIETypeType` values without breaking
                the major but if it is necessary to understand whether
                all nodes support the new type a node capability must
                be added as well.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>node</c>	<c>1</c>	<c>2.0</c>	<c> Used in case of enum common.TIETypeType.NodeTIEType.</c>
                    <c>prefixes</c>	<c>2</c>	<c>2.0</c>	<c> Used in case of enum common.TIETypeType.PrefixTIEType.</c>
                    <c>positive_disaggregation_prefixes</c>	<c>3</c>	<c>2.0</c>	<c> Positive prefixes (always southbound).
                    It MUST NOT be advertised within a North TIE and ignored otherwise</c>
                    <c>negative_disaggregation_prefixes</c>	<c>5</c>	<c>2.0</c>	<c> Transitive, negative prefixes (always southbound) which
                    MUST be aggregated and propagated
                    according to the specification
                    southwards towards lower levels to heal
                    pathological upper level partitioning, otherwise
                    blackholes may occur in multiplane fabrics.
                    It MUST NOT be advertised within a North TIE.</c>
                    <c>external_prefixes</c>	<c>6</c>	<c>2.0</c>	<c> Externally reimported prefixes.</c>
                    <c>positive_external_disaggregation_prefixes</c>	<c>7</c>	<c>2.0</c>	<c> Positive external disaggregated prefixes (always southbound).
                    It MUST NOT be advertised within a North TIE and ignored otherwise.</c>
                    <c>keyvalues</c>	<c>9</c>	<c>2.0</c>	<c> Key-Value store elements.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIEHeader">

            <t>Header of a TIE.</t>
            <t>@note: TIEID space is a total order achieved by comparing the elements
                in sequence defined and comparing each value as an
                unsigned integer of according length.</t>
            <t>@note: After sequence number the lifetime received on the envelope
                must be used for comparison before further fields.</t>
            <t>@note: `origination_time` and `origination_lifetime` are disregarded
                for comparison purposes and carried purely for debugging/security
                purposes if present.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>tieid</c>	<c>2</c>	<c>2.0</c>	<c> ID of the tie.</c>
                    <c>seq_nr</c>	<c>3</c>	<c>2.0</c>	<c> Sequence number of the tie.</c>
                    <c>origination_time</c>	<c>10</c>	<c>2.0</c>	<c> Absolute timestamp when the TIE
                    was generated. This can be used on fabrics with
                    synchronized clock to prevent lifetime modification attacks.</c>
                    <c>origination_lifetime</c>	<c>12</c>	<c>2.0</c>	<c> Original lifetime when the TIE
                    was generated. This can be used on fabrics with
                    synchronized clock to prevent lifetime modification attacks.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIEHeaderWithLifeTime">

            <t>Header of a TIE as described in TIRE/TIDE.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>header</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>remaining_lifetime</c>	<c>2</c>	<c>2.0</c>	<c> Remaining lifetime that expires down to 0 just like in ISIS.
                    TIEs with lifetimes differing by less than `lifetime_diff2ignore` MUST
                    be considered EQUAL.</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIEID">

            <t>ID of a TIE.</t>
            <t>@note: TIEID space is a total order achieved by comparing the elements
                in sequence defined and comparing each value as an
                unsigned integer of according length.</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>direction</c>	<c>1</c>	<c>2.0</c>	<c> direction of TIE</c>
                    <c>originator</c>	<c>2</c>	<c>2.0</c>	<c> indicates originator of the TIE</c>
                    <c>tietype</c>	<c>3</c>	<c>2.0</c>	<c> type of the tie</c>
                    <c>tie_nr</c>	<c>4</c>	<c>2.0</c>	<c> number of the tie</c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIEPacket">

            <t>TIE packet</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>header</c>	<c>1</c>	<c>2.0</c>	<c></c>
                    <c>element</c>	<c>2</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>

        <section title="Registry RIFT/encoding/TIREPacket">

            <t>TIRE packet</t>

            <section title="Requested Entries">
                <texttable style="none">
                    <ttcol>Name</ttcol><ttcol align="right">Value</ttcol><ttcol align="right">Schema Version</ttcol><ttcol>Description</ttcol>
                    <c>headers</c>	<c>1</c>	<c>2.0</c>	<c></c>
                </texttable>

            </section>

        </section>



        <!-- end generate request -->

    </section>

</section>

<section anchor="Acknowledgments" title="Acknowledgments">

    <t>
        A new routing protocol in its complexity is not a product of a parent but of a
        village as the author list shows already. However, many more people provided input,
        fine-combed the specification based on their experience in design or implementation.
        This section will make an inadequate attempt in recording their contribution.
    </t>

    <t>Many thanks to Naiming Shen for some of the early
        discussions around
        the topic of using IGPs for routing in topologies related to Clos.
        Russ White to be especially acknowledged for the key
        conversation on epistemology that allowed to tie current
        asynchronous distributed systems theory results to a modern protocol
        design presented here.
        Adrian Farrel, Joel Halpern, Jeffrey Zhang, Krzysztof Szarkowicz,
        Nagendra Kumar, Melchior Aelmans
        provided thoughtful comments that improved the
        readability of the document and found good amount of
        corners where the light failed to shine. Kris Price was
        first to mention single router, single arm default considerations.
        Jeff Tantsura helped out with some initial thoughts on BFD
        interactions while Jeff Haas corrected several misconceptions
        about BFD's finer points. Artur Makutunowicz pointed out
        many possible improvements and acted as sounding
        board in regard to modern protocol implementation techniques
        RIFT is exploring. Barak Gafni formalized first time clearly
        the problem of partitioned spine and fallen leaves on a
        (clean) napkin in Singapore that led to the very important
        part of the specification centered around multiple Top-of-Fabric planes
        and negative disaggregation. Igor Gashinsky and others shared many thoughts on
        problems encountered in design and operation of large-scale data
        center fabrics. Xu Benchong found a delicate error in the flooding procedures while
        implementing.
    </t>
</section>


    </middle>


    <back>

<references title="Normative References">


    <reference anchor="ISO10589">
        <front>
            <title> Intermediate system to Intermediate system
                intra-domain
                routeing information exchange protocol for use
                in conjunction with
                the protocol for providing the connectionless-mode
                Network Service
                (ISO 8473), ISO/IEC 10589:2002, Second Edition.</title>

            <author>
                <organization>ISO &quot;International Organization for
                    Standardization&quot;</organization>
            </author>
            <date month="Nov" year="2002"/>
        </front>
    </reference>

    <reference anchor="thrift" target="https://thrift.apache.org/docs/idl">
        <front>
            <title>
Thrift Interface Description Language
            </title>
            <author>
                <organization>
Apache Software Foundation
                </organization>
            </author>
            <date/>
        </front>
    </reference>

    &RFC1982;
    &RFC2328;
    &RFC2365;
    &RFC4271;
    &RFC4291;
    &RFC5082;
    &RFC5120;
    &RFC5303;
    &RFC5549;
    &RFC5709;
    &RFC5881;
    &RFC5905;
    &RFC7752;
    &RFC7987;
    &RFC8174;
    &RFC8200;
    &RFC8202;
    &RFC8505;
    <!-- SR completely removed

    &RFC8402;

    -->


<reference anchor="EUI64"
    target ="http://standards.ieee.org/develop/regauth/tut/eui.pdf">
    <front>
        <title>Guidelines
            for
            Use
            of
            Extended
            Unique
            Identifier
            (EUI),
            Organizationally
            Unique
            Identifier
            (OUI),
            and
            Company
            ID
            (CID)</title>
        <author>
            <organization>IEEE</organization>
        </author>
        <date/>
    </front>
    <seriesInfo name="IEEE" value="EUI"/>
</reference>

</references>

<references title="Informative References">
    <!-- Here we use entities that we defined at the beginning. -->
    &RFC0826;
    <!-- &RFC7855; -->
    &RFC2131;
    &RFC8415;
    &RFC3626;
    <!--
    &RFC4655;
    -->
    &RFC4861;
    &RFC4862;
    &RFC6518;
    &RFC7938;

    <reference anchor="IEEEstd1588"
        target ="https://ieeexplore.ieee.org/document/4579760/">
        <front>
            <title>IEEE Standard for a Precision Clock Synchronization Protocol for
                Networked Measurement and Control Systems</title>
            <author>
                <organization>IEEE</organization>
            </author>
            <date/>
        </front>
        <seriesInfo name="IEEE" value="Standard 1588"/>
    </reference>

    <reference anchor="IEEEstd8021AS"
        target ="https://ieeexplore.ieee.org/document/5741898/">
        <front>
            <title>IEEE Standard for Local and Metropolitan Area Networks - Timing
                and Synchronization for Time-Sensitive Applications in Bridged Local
                Area Networks</title>
            <author>
                <organization>IEEE</organization>
            </author>
            <date/>
        </front>
        <seriesInfo name="IEEE" value="Standard 802.1AS"/>
    </reference>

    <reference anchor="CLOS">
        <front>
            <title>On Nonblocking Folded-Clos Networks in
                Computer Communication Environments</title>
            <author initials="X." surname="Yuan">
                <organization>IEEE International Parallel &amp;
                    Distributed Processing Symposium</organization>
            </author>
            <date  year="2011"/>
        </front>
        <seriesInfo name="IEEE" value="International Parallel &amp;
        Distributed Processing Symposium"/>
    </reference>

    <reference anchor="DIJKSTRA">
        <front>
            <title>A Note on Two Problems in Connexion with Graphs</title>
            <author initials="E. W." surname="Dijkstra">
                <organization></organization>
            </author>

            <date  year="1959"/>
        </front>
        <seriesInfo name="Journal Numer. Math." value=""/>
    </reference>

    <reference anchor="DYNAMO">
        <front>
            <title>Dynamo: amazon's highly available key-value store</title>
            <author  initials="G." surname="De Candia et al.">
                <organization></organization>
            </author><date  year="2007"/>
        </front>
        <seriesInfo name="ACM" value="SIGOPS symposium on Operating systems principles (SOSP '07)"/>
    </reference>

    <reference anchor="EPPSTEIN">
        <front>
            <title>Finding the k-Shortest Paths</title>
            <author initials="D" surname="Eppstein">
                <organization>USC
                </organization>
            </author>

            <date  year="1997"/>
        </front>

    </reference>

    <reference anchor="FATTREE">
        <front>
            <title>Fat-Trees: Universal Networks for Hardware-Efficient
                Supercomputing</title>
            <author initials="C. E." surname="Leiserson">
                <organization>IEEE Transactions on Computers</organization>
            </author>
            <date  year="1985"/>
        </front>

    </reference>

    <reference anchor="VAHDAT08">
        <front>
            <title>A Scalable, Commodity Data Center Network
                Architecture</title>
            <author initials="M." surname="Al-Fares">
                <organization>USC</organization>
            </author>

            <author initials="A." surname="Loukissas">
                <organization>USC</organization>
            </author>

            <author initials="A." surname="Vahdat">
                <organization>USC</organization>
            </author>
            <date  year="2008"/>
        </front>
        <seriesInfo name="SIGCOMM" value=""/>
    </reference>

<reference anchor="DOT">
    <front>

<title>Graphviz: open source graph drawing tools</title>
        <author initials="J." surname="Ellson"></author>
        <author initials="L." surname="Koutsofios"></author>
        <date  year="2001"/>
    </front>
    <seriesInfo name="Springer-Verlag" value=""/>

</reference>

<!--
    <reference anchor="MAKSIC2013">
        <front>
            <title>Improving Utilization of Data Center Networks</title>
            <author  initials="N." surname="Maksic et al.">
                <organization></organization>
            </author><date  month="Nov" year="2013"/>
        </front>
        <seriesInfo name="IEEE" value="Communications Magazine"/>
    </reference>
-->

    <reference anchor="ISO10589-Second-Edition">

        <front>
            <title>Intermediate system to Intermediate system intra-domain
                routeing information exchange protocol for use in
                conjunction with the protocol for providing the
                connectionless-mode Network Service (ISO 8473)</title>

            <author>
                <organization>International Organization for Standardization</organization>
            </author>
            <date  month="Nov" year="2002"/>
        </front>
    </reference>


<!--    <reference anchor="QUIC">-->
<!--        <front>-->
<!--            <title>QUIC: A UDP-Based Multiplexed and Secure Transport</title>-->
<!--            <author initials="J." surname="Iyengar et al.">-->
<!--                <organization>IETF</organization></author>-->
<!--            <date year="2016"/>-->
<!--        </front>-->
<!--    </reference>-->

<!--    <reference anchor="PROTOBUF">-->
<!--        <front>-->
<!--            <title>Protocol Buffers,-->
<!--                https://developers.google.com/protocol-buffers</title>-->
<!--            <author>-->
<!--                <organization>Google, Inc.</organization>-->
<!--            </author>-->
<!--            <date/>-->
<!--        </front>-->
<!--    </reference>-->

    <reference anchor="Wikipedia">
        <front>
            <title>https://en.wikipedia.org/wiki/Serial_number_arithmetic</title>
            <author>
                <organization>Wikipedia</organization>
            </author>
            <date year="2016"></date>
        </front>
    </reference>

</references>




        <section anchor="arithmetic" title="Sequence Number Binary Arithmetic">
            <t>The only reasonably reference to a cleaner than <xref target="RFC1982"></xref>
                sequence number solution is given in
                <xref target="Wikipedia"></xref>. It basically converts the problem
                into two complement's arithmetic.
                Assuming a straight two complement's subtractions on the bit-width
                of the sequence number
                the according &gt;: and =:
                relations
                are defined as:
            </t>

            <t>
                <figure align="center">
                    <artwork align="left"><![CDATA[
    U_1, U_2 are 12-bits aligned unsigned version number

    D_f is  ( U_1 - U_2 ) interpreted as two complement signed 12-bits
    D_b is  ( U_2 - U_1 ) interpreted as two complement signed 12-bits

    U_1 >: U_2 IIF D_f > 0 AND D_b < 0
    U_1 =: U_2 IIF D_f = 0
]]></artwork>
                </figure>
            </t>
            <t>
                The &gt;: relationship is anti-symmetric but not transitive.
                Observe that this leaves >: of the numbers having maximum two complement
                distance, e.g. ( 0 and 0x800 ) undefined in our 12-bits case since D_f and D_b are
                both -0x7ff.

            </t>
            <t>A simple example of the relationship in case of 3-bit arithmetic
                follows as table indicating D_f/D_b values and then the relationship of
                U_1 to U_2:
            </t>
            <t>
                <figure align="center">
                    <artwork align="left"><![CDATA[
        U2 / U1   0    1    2    3    4    5    6    7
        0        +/+  +/-  +/-  +/-  -/-  -/+  -/+  -/+
        1        -/+  +/+  +/-  +/-  +/-  -/-  -/+  -/+
        2        -/+  -/+  +/+  +/-  +/-  +/-  -/-  -/+
        3        -/+  -/+  -/+  +/+  +/-  +/-  +/-  -/-
        4        -/-  -/+  -/+  -/+  +/+  +/-  +/-  +/-
        5        +/-  -/-  -/+  -/+  -/+  +/+  +/-  +/-
        6        +/-  +/-  -/-  -/+  -/+  -/+  +/+  +/-
        7        +/-  +/-  +/-  -/-  -/+  -/+  -/+  +/+
    ]]></artwork>
                </figure>
            </t>

            <t>
                <figure align="center">
                    <artwork align="left"><![CDATA[
       U2 / U1   0    1    2    3    4    5    6    7
       0         =    >    >    >    ?    <    <    <
       1         <    =    >    >    >    ?    <    <
       2         <    <    =    >    >    >    ?    <
       3         <    <    <    =    >    >    >    ?
       4         ?    <    <    <    =    >    >    >
       5         >    ?    <    <    <    =    >    >
       6         >    >    ?    <    <    <    =    >
       7         >    >    >    ?    <    <    <    =
        ]]></artwork>
                </figure>
            </t>

        </section>

        <section title="Information Elements Schema" anchor="schema">

            <t>This section introduces the schema for information elements.
                The IDL is <xref target="thrift">Thrift</xref>.
            </t>
            <t>On schema changes that</t>
                <t>
                    <list style="numbers">
                        <t>change  field numbers or</t>
                        <t>add new *required* fields or</t>
                        <t>remove  any fields or</t>
                        <t>change  lists into sets, unions into structures or</t>
                        <t>change  multiplicity of fields or</t>
                        <t>changes name of any field or type or</t>
                        <t>change  data types of any field or</t>
                        <t>adds, changes or removes a default value of any *existing* field or</t>
                        <t>removes or changes any defined constant or constant value or</t>
                        <t>changes any enumeration type except extending `common.TIEType`
                            (use of enumeration types is generally discouraged)</t>
                        </list>
                    </t>
                <t>major version of the schema MUST increase.
                    All other changes MUST
                increase minor version within the same major.</t>

<t>Observe however that introducing an optional field
    does not cause a major version increase even if the fields inside the
    structure are optional with defaults.</t>

<t>All signed integer as forced by <xref target="thrift">Thrift</xref> support must be cast for internal
    purposes to equivalent unsigned values without discarding the signedness bit.
    An implementation SHOULD try to avoid using the signedness bit when
    generating values.</t>

<t>The schema is normative.</t>


<section title="common.thrift">
    <t>
<figure><artwork><![CDATA[

/**
    Thrift file with common definitions for RIFT
*/

namespace py common
namespace rs models

/** @note MUST be interpreted in implementation as unsigned 64 bits.
 *        The implementation SHOULD NOT use the MSB.
 */
typedef i64      SystemIDType
typedef i32      IPv4Address
/** this has to be long enough to accomodate prefix */
typedef binary   IPv6Address
/** @note MUST be interpreted in implementation as unsigned */
typedef i16      UDPPortType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      TIENrType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      MTUSizeType
/** @note MUST be interpreted in implementation as unsigned
    rolling over number */
typedef i16      SeqNrType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      LifeTimeInSecType
/** @note MUST be interpreted in implementation as unsigned */
typedef i8       LevelType
/** optional, recommended monotonically increasing number
    _per packet type per adjacency_
    that can be used to detect losses/misordering/restarts.
    @note MUST be interpreted in implementation as unsigned
          rolling over number */
typedef i16      PacketNumberType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      PodType
/** @note MUST be interpreted in implementation as unsigned.
          This is carried in the
          security envelope and MUST fit into 8 bits. */
typedef i8       VersionType
/** @note MUST be interpreted in implementation as unsigned */
typedef i16      MinorVersionType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      MetricType
/** @note MUST be interpreted in implementation as unsigned
          and unstructured */
typedef i64      RouteTagType
/** @note MUST be interpreted in implementation as unstructured
          label value */
typedef i32      LabelType
/** @note MUST be interpreted in implementation as unsigned */
typedef i32      BandwithInMegaBitsType
/** @note Key Value key ID type */
typedef string   KeyIDType
/** node local, unique identification for a link (interface/tunnel
  * etc. Basically anything RIFT runs on). This is kept
  * at 32 bits so it aligns with BFD [RFC5880] discriminator size.
  */
typedef i32    LinkIDType
typedef string KeyNameType
typedef i8     PrefixLenType
/** timestamp in seconds since the epoch */
typedef i64    TimestampInSecsType
/** security nonce.
    @note MUST be interpreted in implementation as rolling
          over unsigned value */
typedef i16    NonceType
/** LIE FSM holdtime type */
typedef i16    TimeIntervalInSecType
/** Transaction ID type for prefix mobility as specified by RFC6550,
    value MUST be interpreted in implementation as unsigned  */
typedef i8     PrefixTransactionIDType
/** Timestamp per IEEE 802.1AS, all values MUST be interpreted in
    implementation as unsigned.  */
struct IEEE802_1ASTimeStampType {
    1: required     i64     AS_sec;
    2: optional     i32     AS_nsec;
}
/** generic counter type */
typedef i64 CounterType
/** Platform Interface Index type, i.e. index of interface on hardware,
    can be used e.g. with RFC5837 */
typedef i32 PlatformInterfaceIndex

/** Flags indicating node configuration in case of ZTP.
 */
enum HierarchyIndications {
    /** forces level to `leaf_level` and enables according procedures */
    leaf_only                            = 0,
    /** forces level to `leaf_level` and enables according procedures */
    leaf_only_and_leaf_2_leaf_procedures = 1,
    /** forces level to `top_of_fabric` and enables according
        procedures */
    top_of_fabric                        = 2,
}

const PacketNumberType  undefined_packet_number    = 0
/** This MUST be used when node is configured as top of fabric in ZTP.
    This is kept reasonably low to alow for fast ZTP convergence on
    failures. */
const LevelType   top_of_fabric_level              = 24
/** default bandwidth on a link */
const BandwithInMegaBitsType  default_bandwidth    = 100
/** fixed leaf level when ZTP is not used */
const LevelType   leaf_level                  = 0
const LevelType   default_level               = leaf_level
const PodType     default_pod                 = 0
const LinkIDType  undefined_linkid            = 0

/** default distance used */
const MetricType  default_distance         = 1
/** any distance larger than this will be considered infinity */
const MetricType  infinite_distance       = 0x7FFFFFFF
/** represents invalid distance */
const MetricType  invalid_distance        = 0
const bool overload_default               = false
const bool flood_reduction_default        = true
/** default LIE FSM holddown time */
const TimeIntervalInSecType   default_lie_holdtime  = 3
/** default ZTP FSM holddown time */
const TimeIntervalInSecType   default_ztp_holdtime  = 1
/** by default LIE levels are ZTP offers */
const bool default_not_a_ztp_offer        = false
/** by default everyone is repeating flooding */
const bool default_you_are_flood_repeater = true
/** 0 is illegal for SystemID */
const SystemIDType IllegalSystemID        = 0
/** empty set of nodes */
const set<SystemIDType> empty_set_of_nodeids = {}
/** default lifetime of TIE is one week */
const LifeTimeInSecType default_lifetime      = 604800
/** default lifetime when TIEs are purged is 5 minutes */
const LifeTimeInSecType purge_lifetime        = 300
/** round down interval when TIEs are sent with security hashes
    to prevent excessive computation. **/
const LifeTimeInSecType rounddown_lifetime_interval = 60
/** any `TieHeader` that has a smaller lifetime difference
    than this constant is equal (if other fields equal). This
    constant MUST be larger than `purge_lifetime` to avoid
    retransmissions */
const LifeTimeInSecType lifetime_diff2ignore  = 400

/** default UDP port to run LIEs on */
const UDPPortType     default_lie_udp_port       =  914
/** default UDP port to receive TIEs on, that can be peer specific */
const UDPPortType     default_tie_udp_flood_port =  915

/** default MTU link size to use */
const MTUSizeType     default_mtu_size           = 1400
/** default link being BFD capable */
const bool            bfd_default                = true

/** undefined nonce, equivalent to missing nonce */
const NonceType       undefined_nonce            = 0;
/** outer security key id, MUST be interpreted as in implementation
    as unsigned */
typedef i8            OuterSecurityKeyID
/** security key id, MUST be interpreted as in implementation
    as unsigned */
typedef i32           TIESecurityKeyID
/** undefined key */
const TIESecurityKeyID undefined_securitykey_id   = 0;
/** Maximum delta (negative or positive) that a mirrored nonce can
    deviate from local value to be considered valid. If nonces are
    changed every minute on both sides this opens statistically
    a `maximum_valid_nonce_delta` minutes window of identical LIEs,
    TIE, TI(x)E replays.
    The interval cannot be too small since LIE FSM may change
    states fairly quickly during ZTP without sending LIEs*/
const i16             maximum_valid_nonce_delta  = 5;

/** Direction of TIEs. */
enum TieDirectionType {
    Illegal           = 0,
    South             = 1,
    North             = 2,
    DirectionMaxValue = 3,
}

/** Address family type. */
enum AddressFamilyType {
   Illegal                = 0,
   AddressFamilyMinValue  = 1,
   IPv4     = 2,
   IPv6     = 3,
   AddressFamilyMaxValue  = 4,
}

/** IPv4 prefix type. */
struct IPv4PrefixType {
    1: required IPv4Address    address;
    2: required PrefixLenType  prefixlen;
}

/** IPv6 prefix type. */
struct IPv6PrefixType {
    1: required IPv6Address    address;
    2: required PrefixLenType  prefixlen;
}

/** IP address type. */
union IPAddressType {
    /** Content is IPv4 */
    1: optional IPv4Address   ipv4address;
    /** Content is IPv6 */
    2: optional IPv6Address   ipv6address;
}

/** Prefix advertisement.

    @note: for interface
        addresses the protocol can propagate the address part beyond
        the subnet mask and on reachability computation that has to
        be normalized. The non-significant bits can be used
        for operational purposes.
*/
union IPPrefixType {
    1: optional IPv4PrefixType   ipv4prefix;
    2: optional IPv6PrefixType   ipv6prefix;
}

/** Sequence of a prefix in case of move.
 */
struct PrefixSequenceType {
    1: required IEEE802_1ASTimeStampType  timestamp;
    /** Transaction ID set by client in e.g. in 6LoWPAN. */
    2: optional PrefixTransactionIDType   transactionid;
}

/** Type of TIE.

    This enum indicates what TIE type the TIE is carrying.
    In case the value is not known to the receiver,
    the TIE MUST be re-flooded. This allows for
    future extensions of the protocol within the same major schema
    with types opaque to some nodes UNLESS the flooding scope is not
    the same as prefix TIE, then a major version revision MUST
    be performed.
*/
enum TIETypeType {
    Illegal                                     = 0,
    TIETypeMinValue                             = 1,
    /** first legal value */
    NodeTIEType                                 = 2,
    PrefixTIEType                               = 3,
    PositiveDisaggregationPrefixTIEType         = 4,
    NegativeDisaggregationPrefixTIEType         = 5,
    PGPrefixTIEType                             = 6,
    KeyValueTIEType                             = 7,
    ExternalPrefixTIEType                       = 8,
    PositiveExternalDisaggregationPrefixTIEType = 9,
    TIETypeMaxValue                             = 10,
}

/** RIFT route types.

    @note: route types which MUST be ordered on their preference
           PGP prefixes are most preferred attracting
           traffic north (towards spine) and then south
           normal prefixes are attracting traffic south
           (towards leaves), i.e. prefix in NORTH PREFIX TIE
           is preferred over SOUTH PREFIX TIE.

    @note: The only purpose of those values is to introduce an
           ordering whereas an implementation can choose internally
           any other values as long the ordering is preserved
 */
enum RouteType {
    Illegal               =  0,
    RouteTypeMinValue     =  1,
    /** First legal value. */
    /** Discard routes are most preferred */
    Discard               =  2,

    /** Local prefixes are directly attached prefixes on the
     *  system such as e.g. interface routes.
     */
    LocalPrefix           =  3,
    /** Advertised in S-TIEs */
    SouthPGPPrefix        =  4,
    /** Advertised in N-TIEs */
    NorthPGPPrefix        =  5,
    /** Advertised in N-TIEs */
    NorthPrefix           =  6,
    /** Externally imported north */
    NorthExternalPrefix   =  7,
    /** Advertised in S-TIEs, either normal prefix or positive
        disaggregation */
    SouthPrefix           =  8,
    /** Externally imported south */
    SouthExternalPrefix   =  9,
    /** Negative, transitive prefixes are least preferred */
    NegativeSouthPrefix   = 10,
    RouteTypeMaxValue     = 11,
}

]]></artwork></figure>
</t>

</section>

<section title="encoding.thrift">
    <t>
    <figure><artwork><![CDATA[

/**
    Thrift file for packet encodings for RIFT
*/

include "common.thrift"


/** Represents protocol encoding schema major version */
const common.VersionType protocol_major_version = 2
/** Represents protocol encoding schema minor version */
const common.MinorVersionType protocol_minor_version =  0

/** Common RIFT packet header. */
struct PacketHeader {
    /** Major version of protocol. */
    1: required common.VersionType major_version =
            protocol_major_version;
    /** Minor version of protocol. */
    2: required common.VersionType minor_version =
            protocol_minor_version;
    /** Node sending the packet, in case of LIE/TIRE/TIDE
        also the originator of it. */
    3: required common.SystemIDType  sender;
    /** Level of the node sending the packet, required on everything
        except LIEs. Lack of presence on LIEs indicates UNDEFINED_LEVEL
        and is used in ZTP procedures.
     */
    4: optional common.LevelType            level;
}

/** Prefix community. */
struct Community {
    /** Higher order bits */
    1: required i32          top;
    /** Lower order bits */
    2: required i32          bottom;
}

/** Neighbor structure.  */
struct Neighbor {
    /** System ID of the originator. */
    1: required common.SystemIDType        originator;
    /** ID of remote side of the link. */
    2: required common.LinkIDType          remote_id;
}

/** Capabilities the node supports.

    @note: The schema may add to this
    field future capabilities to indicate whether it will support
    interpretation of future schema extensions on the same major
    revision. Such fields MUST be optional and have an implicit or
    explicit false default value. If a future capability changes route
    selection or generates blackholes if some nodes are not supporting
    it then a major version increment is unavoidable.
*/
struct NodeCapabilities {
    /** Must advertise supported minor version dialect that way. */
    1: required common.MinorVersionType        protocol_minor_version =
            protocol_minor_version;
    /** Can this node participate in flood reduction. */
    2: optional bool                           flood_reduction =
            common.flood_reduction_default;
    /** Does this node restrict itself to be top-of-fabric or
        leaf only (in ZTP) and does it support leaf-2-leaf
        procedures. */
    3: optional common.HierarchyIndications    hierarchy_indications;
}

/** Link capabilities. */
struct LinkCapabilities {
    /** Indicates that the link is supporting BFD. */
    1: optional bool                           bfd =
            common.bfd_default;
    /** Indicates whether the interface will support v4 forwarding.

        @note: This MUST be set to true when LIEs from a v4 address are
               sent and MAY be set to true in LIEs on v6 address. If v4
               and v6 LIEs indicate contradicting information the
               behavior is unspecified. */
    2: optional bool                           v4_forwarding_capable =
            true;
}

/** RIFT LIE Packet.

    @note: this node's level is already included on the packet header
*/
struct LIEPacket {
    /** Node or adjacency name. */
    1: optional string                    name;
    /** Local link ID. */
    2: required common.LinkIDType         local_id;
    /** UDP port to which we can receive flooded TIEs. */
    3: required common.UDPPortType        flood_port =
            common.default_tie_udp_flood_port;
    /** Layer 3 MTU, used to discover to mismatch. */
    4: optional common.MTUSizeType        link_mtu_size =
            common.default_mtu_size;
    /** Local link bandwidth on the interface. */
    5: optional common.BandwithInMegaBitsType
            link_bandwidth = common.default_bandwidth;
    /** Reflects the neighbor once received to provide
        3-way connectivity. */
    6: optional Neighbor                  neighbor;
    /** Node's PoD. */
    7: optional common.PodType            pod =
            common.default_pod;
    /** Node capabilities shown in LIE. The capabilities
        MUST match the capabilities shown in the Node TIEs, otherwise
        the behavior is unspecified. A node detecting the mismatch
        SHOULD generate according error. */
   10: required NodeCapabilities          node_capabilities;
   /** Capabilities of this link. */
   11: optional LinkCapabilities          link_capabilities;
   /** Required holdtime of the adjacency, i.e. how much time
       MUST expire without LIE for the adjacency to drop. */
   12: required common.TimeIntervalInSecType
            holdtime = common.default_lie_holdtime;
   /** Unsolicited, downstream assigned locally significant label
       value for the adjacency. */
   13: optional common.LabelType          label;
    /** Indicates that the level on the LIE MUST NOT be used
        to derive a ZTP level by the receiving node. */
   21: optional bool                      not_a_ztp_offer =
            common.default_not_a_ztp_offer;
   /** Indicates to northbound neighbor that it should
       be reflooding this node's N-TIEs to achieve flood reduction and
       balancing for northbound flooding. To be ignored if received
       from a northbound adjacency. */
   22: optional bool                      you_are_flood_repeater =
             common.default_you_are_flood_repeater;
   /** Can be optionally set to indicate to neighbor that packet losses
       are seen on reception based on packet numbers or the rate is
       too high. The receiver SHOULD temporarily slow down
       flooding rates.
    */
   23: optional bool                      you_are_sending_too_quickly =
             false;
   /** Instance name in case multiple RIFT instances running on same
       interface. */
   24: optional string                    instance_name;
}

/** LinkID pair describes one of parallel links between two nodes. */
struct LinkIDPair {
    /** Node-wide unique value for the local link. */
    1: required common.LinkIDType      local_id;
    /** Received remote link ID for this link. */
    2: required common.LinkIDType      remote_id;

    /** Describes the local interface index of the link. */
   10: optional common.PlatformInterfaceIndex platform_interface_index;
   /** Describes the local interface name. */
   11: optional string                        platform_interface_name;
   /** Indication whether the link is secured, i.e. protected by
       outer key, absence of this element means no indication,
       undefined outer key means not secured. */
   12: optional common.OuterSecurityKeyID
                trusted_outer_security_key;
   /** Indication whether the link is protected by established
       BFD session. */
   13: optional bool                          bfd_up;
}

/** ID of a TIE.

    @note: TIEID space is a total order achieved by comparing
           the elements in sequence defined and comparing each
           value as an unsigned integer of according length.
*/
struct TIEID {
    /** direction of TIE */
    1: required common.TieDirectionType    direction;
    /** indicates originator of the TIE */
    2: required common.SystemIDType        originator;
    /** type of the tie */
    3: required common.TIETypeType         tietype;
    /** number of the tie */
    4: required common.TIENrType           tie_nr;
}

/** Header of a TIE.

   @note: TIEID space is a total order achieved by comparing
          the elements in sequence defined and comparing each
          value as an unsigned integer of according length.

   @note: After sequence number the lifetime received on the envelope
          must be used for comparison before further fields.

   @note: `origination_time` and `origination_lifetime` are disregarded
          for comparison purposes and carried purely for
          debugging/security purposes if present.
*/
struct TIEHeader {
    /** ID of the tie. */
    2: required TIEID                             tieid;
    /** Sequence number of the tie. */
    3: required common.SeqNrType                  seq_nr;

    /** Absolute timestamp when the TIE
        was generated. This can be used on fabrics with
        synchronized clock to prevent lifetime modification attacks. */
   10: optional common.IEEE802_1ASTimeStampType   origination_time;
   /** Original lifetime when the TIE
       was generated. This can be used on fabrics with
       synchronized clock to prevent lifetime modification attacks. */
   12: optional common.LifeTimeInSecType          origination_lifetime;
}

/** Header of a TIE as described in TIRE/TIDE.
*/
struct TIEHeaderWithLifeTime {
    1: required     TIEHeader                       header;
    /** Remaining lifetime that expires down to 0 just like in ISIS.
        TIEs with lifetimes differing by less than
        `lifetime_diff2ignore` MUST be considered EQUAL. */
    2: required     common.LifeTimeInSecType        remaining_lifetime;
}

/** TIDE with sorted TIE headers, if headers are unsorted, behavior
    is undefined. */
struct TIDEPacket {
    /** First TIE header in the tide packet. */
    1: required TIEID                       start_range;
    /** Last TIE header in the tide packet. */
    2: required TIEID                       end_range;
    /** _Sorted_ list of headers. */
    3: required list<TIEHeaderWithLifeTime> headers;
}

/** TIRE packet */
struct TIREPacket {
    1: required set<TIEHeaderWithLifeTime>  headers;
}

/** neighbor of a node */
struct NodeNeighborsTIEElement {
    /** level of neighbor */
    1: required common.LevelType                level;
    /**  Cost to neighbor.

         @note: All parallel links to same node
         incur same cost, in case the neighbor has multiple
         parallel links at different cost, the largest distance
         (highest numerical value) MUST be advertised.

         @note: any neighbor with cost <= 0 MUST be ignored
                in computations */
    3: optional common.MetricType               cost
                = common.default_distance;
    /** can carry description of multiple parallel links in a TIE */
    4: optional set<LinkIDPair>                 link_ids;

    /** total bandwith to neighbor, this will be normally sum of the
        bandwidths of all the parallel links. */
    5: optional common.BandwithInMegaBitsType
                bandwidth = common.default_bandwidth;
}

/** Indication flags of the node. */
struct NodeFlags {
    /** Indicates that node is in overload, do not transit traffic
        through it. */
    1: optional bool         overload = common.overload_default;
}

/** Description of a node.

    It may occur multiple times in different TIEs but if either
        <list>
         <t>capabilities values do not match or</t>
        <t>flags values do not match or</t>
        <t>neighbors repeat with different values</t>
        </list>

    the behavior is undefined and a warning SHOULD be generated.
    Neighbors can be distributed across multiple TIEs however if
    the sets are disjoint. Miscablings SHOULD be repeated in every
    node TIE, otherwise the behavior is undefined.

    @note: Observe that absence of fields implies defined defaults.
*/
struct NodeTIEElement {
    /** Level of the node. */
    1: required common.LevelType            level;
    /** Node's neighbors. If neighbor systemID repeats in other
        node TIEs of same node the behavior is undefined. */
    2: required map<common.SystemIDType,
                NodeNeighborsTIEElement>    neighbors;
    /** Capabilities of the node. */
    3: required NodeCapabilities            capabilities;
    /** Flags of the node. */
    4: optional NodeFlags                   flags;
    /** Optional node name for easier operations. */
    5: optional string                      name;
    /** PoD to which the node belongs. */
    6: optional common.PodType              pod;

    /** If any local links are miscabled, the indication is flooded. */
   10: optional set<common.LinkIDType>      miscabled_links;

}

/** Attributes of a prefix. */
struct PrefixAttributes {
    /** Distance of the prefix. */
    2: required common.MetricType            metric
            = common.default_distance;
    /** Generic unordered set of route tags, can be redistributed
        to other protocols or use within the context of real time
        analytics. */
    3: optional set<common.RouteTagType>     tags;
    /** Monotonic clock for mobile addresses. */
    4: optional common.PrefixSequenceType    monotonic_clock;
    /** Indicates if the interface is a node loopback. */
    6: optional bool                         loopback = false;
    /** Indicates that the prefix is directly attached, i.e. should be
        routed to even if the node is in overload. */
    7: optional bool                         directly_attached = true;

    /** In case of locally originated prefixes, i.e. interface
        addresses this can describe which link the address
        belongs to. */
   10: optional common.LinkIDType            from_link;
}

/** TIE carrying prefixes */
struct PrefixTIEElement {
    /** Prefixes with the associated attributes.
        If the same prefix repeats in multiple TIEs of same node
        behavior is unspecified. */
    1: required map<common.IPPrefixType, PrefixAttributes> prefixes;
}

/** Generic key value pairs. */
struct KeyValueTIEElement {
    /** @note: if the same key repeats in multiple TIEs of same node
        or with different values, behavior is unspecified */
    1: required map<common.KeyIDType,string>    keyvalues;
}

/** Single element in a TIE.

    Schema enum `common.TIETypeType`
    in TIEID indicates which elements MUST be present
    in the TIEElement. In case of mismatch the unexpected
    elements MUST be ignored. In case of lack of expected
    element the TIE an error MUST be reported and the TIE
    MUST be ignored.

    This type can be extended with new optional elements
    for new `common.TIETypeType` values without breaking
    the major but if it is necessary to understand whether
    all nodes support the new type a node capability must
    be added as well.
 */
union TIEElement {
    /** Used in case of enum common.TIETypeType.NodeTIEType. */
    1: optional NodeTIEElement     node;
    /** Used in case of enum common.TIETypeType.PrefixTIEType. */
    2: optional PrefixTIEElement          prefixes;
    /** Positive prefixes (always southbound).
        It MUST NOT be advertised within a North TIE and
        ignored otherwise.
    */
    3: optional PrefixTIEElement   positive_disaggregation_prefixes;
    /** Transitive, negative prefixes (always southbound) which
        MUST be aggregated and propagated
        according to the specification
        southwards towards lower levels to heal
        pathological upper level partitioning, otherwise
        blackholes may occur in multiplane fabrics.
        It MUST NOT be advertised within a North TIE.
    */
    5: optional PrefixTIEElement   negative_disaggregation_prefixes;
    /** Externally reimported prefixes. */
    6: optional PrefixTIEElement          external_prefixes;
    /** Positive external disaggregated prefixes (always southbound).
        It MUST NOT be advertised within a North TIE and
        ignored otherwise.
    */
    7: optional PrefixTIEElement
            positive_external_disaggregation_prefixes;
    /** Key-Value store elements. */
    9: optional KeyValueTIEElement keyvalues;
}

/** TIE packet */
struct TIEPacket {
    1: required TIEHeader  header;
    2: required TIEElement element;
}

/** Content of a RIFT packet. */
union PacketContent {
    1: optional LIEPacket     lie;
    2: optional TIDEPacket    tide;
    3: optional TIREPacket    tire;
    4: optional TIEPacket     tie;
}

/** RIFT packet structure. */
struct ProtocolPacket {
    1: required PacketHeader  header;
    2: required PacketContent content;
}
    ]]></artwork></figure>
    </t>
</section>


            </section>




        <section title="Constants">

            <section title="Configurable Protocol Constants" anchor="constants">

                <t>This section gathers constants that are provided in the
                     schema files and in the document.</t>

                <texttable anchor="addresses"
                    title="all_constants"
                    style="all">

                    <ttcol></ttcol><ttcol>Type</ttcol><ttcol>Value</ttcol>

                    <c>LIE IPv4 Multicast Address</c>
                    <c>Default Value, Configurable</c>
                    <c>224.0.0.120 or all-rift-routers to be assigned in IPv4
                        Multicast Address Space Registry in Local Network Control Block</c>

                    <c>LIE IPv6 Multicast Address</c>
                    <c>Default Value, Configurable</c>
                    <c>FF02::A1F7 or all-rift-routers to be assigned in IPv6 Multicast
                        Address Assignments</c>

                    <c>LIE Destination Port</c>
                    <c>Default Value, Configurable</c>
                    <c>914</c>

                    <c>Level value for TOP_OF_FABRIC flag</c>
                    <c>Constant</c><c>24</c>

                    <c>Default LIE Holdtime</c>
                    <c>Default Value, Configurable</c>
                    <c>3 seconds</c>

                    <c>TIE Retransmission Interval</c>
                    <c>Default Value</c>
                    <c>1 second</c>

                    <c>TIDE Generation Interval</c>
                    <c>Default Value, Configurable</c>
                    <c>5 seconds</c>

                    <c>MIN_TIEID signifies start of TIDEs</c>
                    <c>Constant</c>
                    <c>TIE Key with minimal values: TIEID(originator=0, tietype=TIETypeMinValue,
                        tie_nr=0, direction=South)</c>

                    <c>MAX_TIEID signifies end of TIDEs</c>
                    <c>Constant</c>
                    <c>TIE Key with maximal values: TIEID(originator=MAX_UINT64,
                        tietype=TIETypeMaxValue, tie_nr=MAX_UINT64, direction=North)</c>

                </texttable>
            </section>


        </section>


        <!-- References split into informative and normative -->

        <!-- There are 2 ways to insert reference entries from the citation libraries:
         1. define an ENTITY at the top, and use "ampersand character"RFC2629; here (as shown)
         2. simply use a PI "less than character"?rfc include="reference.RFC.2119.xml"?> here
         (for I-Ds: include="reference.I-D.narten-iana-considerations-rfc2434bis.xml")

         Both are cited textually in the same manner: by using xref elements.
         If you use the PI option, xml2rfc will, by default, try to find included files in the same
         directory as the including file. You can also define the XML_LIBRARY environment variable
         with a value containing a set of directories to search.  These can be either in the local
         filing system or remote ones accessed by http (http://domain/dir/... ).-->



    </back>
</rfc>

